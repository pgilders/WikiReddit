{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get page ranges\n",
    "\n",
    "#  round datetime to day, get Â± 7 days inclusive\n",
    "\n",
    "#  merge any date ranges that overlaps\n",
    "\n",
    "#  reorganise data into dict where key is date range and value is a list of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import unquote\n",
    "import wikitoolkit as wt\n",
    "import pickle\n",
    "import sqlite3\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = 'wikireddit <p.gildersleve@exeter.ac.uk>'\n",
    "conn = sqlite3.connect('data/wikireddit.db')\n",
    "\n",
    "ranges_df = pd.read_hdf('data/ranges_df.h5')\n",
    "\n",
    "with open('data/langpagemaps.pkl', 'rb') as f:\n",
    "    pagemapsdict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges_df_consol = ranges_df.groupby(['lang', 'start_date', 'end_date'])['title'].apply(list)\n",
    "date_article_dict = {}\n",
    "# convert ranges_df_consol to a dict of dicts\n",
    "for lang, start_date, end_date in ranges_df_consol.index:\n",
    "    if lang not in date_article_dict:\n",
    "        date_article_dict[lang] = {}\n",
    "    date_article_dict[lang][(start_date, end_date)] = ranges_df_consol[(lang, start_date, end_date)]\n",
    "\n",
    "starts_df_consol = ranges_df.groupby(['lang', 'start_date'])['title'].apply(list)\n",
    "startdate_article_dict = {}\n",
    "# convert starts_df_consol to a dict of dicts\n",
    "for lang, start_date in starts_df_consol.index:\n",
    "    if lang not in startdate_article_dict:\n",
    "        startdate_article_dict[lang] = {}\n",
    "    startdate_article_dict[lang][start_date] = starts_df_consol[(lang, start_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get revisions in range [from, to] (inclusive), as well as revisions at start of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "aa 0.0\n",
      "ab 0.0\n",
      "ace 0.0\n",
      "ady 0.0\n",
      "af 0.0\n",
      "als 0.0\n",
      "am 0.0\n",
      "an 0.0\n",
      "ang 0.0\n",
      "ar 0.94899169632265726\n",
      "arc 0.0\n",
      "ary 0.0\n",
      "arz 0.0\n",
      "as 0.0\n",
      "ast 0.0\n",
      "atj 0.0\n",
      "avk 0.0\n",
      "ay 0.0\n",
      "az 0.90497737556561093\n",
      "azb 0.0\n",
      "ba 0.0\n",
      "ban 0.0\n",
      "bar 0.7633587786259542\n",
      "bat-smg 0.0\n",
      "bcl 0.0\n",
      "be 0.0\n",
      "be-tarask 0.0\n",
      "bg 0.96830985915492966\n",
      "bh 0.0\n",
      "bi 0.0\n",
      "bjn 0.0\n",
      "bn 0.0\n",
      "bo 0.0\n",
      "bpy 0.0\n",
      "br 0.0\n",
      "bs 0.73529411764705894\n",
      "bug 0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/Documents/R4R/.venv/lib/python3.12/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'bat-smg'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/patrick/Documents/R4R/.venv/lib/python3.12/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'be-tarask'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bxr 0.0\n",
      "ca 0.89418777943368115\n",
      "cbk-zam 0.0\n",
      "cdo 0.0\n",
      "ce 0.0\n",
      "ceb 0.0\n",
      "chr 0.0\n",
      "chy 0.0\n",
      "ckb 0.0\n",
      "co 0.0\n",
      "cr 0.0\n",
      "crh 0.0\n",
      "cs 0.993227990970654634\n",
      "csb 0.0\n",
      "cu 0.0\n",
      "cv 0.0\n",
      "cy 0.9259259259259259\n",
      "da 0.24896265560165975\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7c5b83731dc0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7c5b76562120>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7c5acc5729f0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7c5b8ff85280>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da 0.99585062240663913\n",
      "de 0.345881132184239364\r"
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/done_langdateranges_rev.pkl'):\n",
    "    with open('data/done_langdateranges_rev.pkl', 'rb') as f:\n",
    "        done_langdateranges = pickle.load(f)\n",
    "else:\n",
    "    done_langdateranges = {}\n",
    "\n",
    "maxgroupsize = 10000\n",
    "for lang, da_dict in date_article_dict.items():\n",
    "    print()\n",
    "    try:\n",
    "        l_revisions_df = [pd.read_hdf('data/revisions.h5', key=f'/{lang}')]\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        l_revisions_df = []\n",
    "    if lang not in done_langdateranges:\n",
    "        done_langdateranges[lang] = set()\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    for n, (daterange, articles) in enumerate(da_dict.items()):\n",
    "        if n % 100 == 0:\n",
    "            print(lang, n/len(da_dict), end='\\r')\n",
    "        if daterange in done_langdateranges[lang]:\n",
    "            continue\n",
    "\n",
    "        grouprevs = {}\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                revisions = await wt.get_revisions(wtsession, articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                                                start=daterange[0].isoformat(), stop=daterange[1].isoformat())\n",
    "                grouprevs.update(revisions)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "            except ValueError as e:\n",
    "                print(e, 'Reducing group size to', groupsize // 2)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "\n",
    "        revisions = pd.concat({k: pd.DataFrame(v) for k, v in grouprevs.items()}).reset_index(\n",
    "            level=1, drop=True).reset_index().rename(columns={'index': 'title'})\n",
    "        done_langdateranges[lang].add(daterange)\n",
    "        if len(revisions) == 0:\n",
    "            continue\n",
    "        revisions['lang'] = lang\n",
    "        revisions = revisions[['lang', 'title', 'revid', 'parentid', 'timestamp']]\n",
    "        l_revisions_df.append(revisions)\n",
    "\n",
    "        if (n%5000 == 0)&(n > 0): # save every 5000 for very large langs for safety\n",
    "            if len(l_revisions_df) > 0:\n",
    "                l_revisions_df_i = pd.concat(l_revisions_df, ignore_index=True)\n",
    "                l_revisions_df_i.to_hdf('data/revisions.h5', key=f'/{lang}', mode='a')\n",
    "                del l_revisions_df_i\n",
    "                with open('data/done_langdateranges_rev.pkl', 'wb') as f:\n",
    "                    pickle.dump(done_langdateranges, f)\n",
    "        \n",
    "    if len(l_revisions_df) > 0:\n",
    "        l_revisions_df = pd.concat(l_revisions_df, ignore_index=True)\n",
    "        l_revisions_df.to_hdf('data/revisions.h5', key=f'/{lang}', mode='a')\n",
    "        with open('data/done_langdateranges_rev.pkl', 'wb') as f:\n",
    "            pickle.dump(done_langdateranges, f)\n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentids = set(pd.concat(l_revisions_df, ignore_index=True)['parentid'].astype(int).unique())\n",
    "revids = set(pd.concat(l_revisions_df, ignore_index=True)['revid'].astype(int).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32720, 32723, 1635)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parentids), len(revids), len(parentids - revids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get starting revids from parentid column - this won't work for everything\n",
    "for lang in date_article_dict:\n",
    "    l_revisions_df = pd.read_hdf('data/revisions.h5', key=f'/{lang}')\n",
    "    parentids = set(l_revisions_df['parentid'].unique())\n",
    "    revids = set(l_revisions_df['revid'].unique())\n",
    "\n",
    "    start_revids = parentids - revids\n",
    "    start_revids = list(start_revids - set([0]))\n",
    "    while len(start_revids) > 0:\n",
    "        try:\n",
    "            revisions = await \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ace\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ady\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "als\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ang\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x746814761ac0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x746895582ea0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7468880a8a10>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7468207e9b50>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x746814763e00>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7468207e8800>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8561643835616438\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arc\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ary\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arz\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/Documents/R4R/.venv/lib/python3.12/site-packages/tables/path.py:137: NaturalNameWarning: object name is a Python keyword: 'as'; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ast\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atj\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avk\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ay\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "az\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azb\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ba\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ban\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat-smg\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/Documents/R4R/.venv/lib/python3.12/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'bat-smg'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcl\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be-tarask\n",
      "0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/Documents/R4R/.venv/lib/python3.12/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'be-tarask'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/tmp/ipykernel_5084/2352896156.py:34: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed-integer,key->block0_values] [items->Index(['lang', 'title', 'revid', 'parentid', 'timestamp'], dtype='object')]\n",
      "\n",
      "  l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bg\n",
      "0.0\r"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(articles) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m         revisions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m wt\u001b[38;5;241m.\u001b[39mget_revision(wtsession, titles\u001b[38;5;241m=\u001b[39marticles[:groupsize], pagemaps\u001b[38;5;241m=\u001b[39mpagemapsdict[lang],\n\u001b[1;32m     16\u001b[0m                                          date\u001b[38;5;241m=\u001b[39mdate\u001b[38;5;241m.\u001b[39misoformat())\n\u001b[1;32m     17\u001b[0m         grouprevs\u001b[38;5;241m.\u001b[39mupdate(revisions)\n\u001b[1;32m     18\u001b[0m         articles \u001b[38;5;241m=\u001b[39m articles[groupsize:]\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/wikitoolkit/revisions.py:60\u001b[0m, in \u001b[0;36mget_revision\u001b[0;34m(session, titles, pageids, date, pagemaps, props, return_props)\u001b[0m\n\u001b[1;32m     56\u001b[0m query_args_list, key, ix \u001b[38;5;241m=\u001b[39m querylister(titles, pageids, generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m             pagemaps\u001b[38;5;241m=\u001b[39mpagemaps, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Execute the API query and parse the revision data\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m iterate_async_query(session\u001b[38;5;241m.\u001b[39mmw_session, query_args_list, function\u001b[38;5;241m=\u001b[39mparse_revision, continuation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Organize the revision data based on titles or pageids\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m titles:\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/wikitoolkit/api.py:136\u001b[0m, in \u001b[0;36miterate_async_query\u001b[0;34m(session, query_args_list, function, f_args, continuation, debug, httpmethod, posturl)\u001b[0m\n\u001b[1;32m    133\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [query_async(session, query_args, continuation, debug, httpmethod, posturl) \u001b[38;5;28;01mfor\u001b[39;00m query_args \u001b[38;5;129;01min\u001b[39;00m query_args_list]\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Execute the tasks asynchronously and gather the results\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/wikitoolkit/revisions.py:17\u001b[0m, in \u001b[0;36mparse_revision\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse single revision data from the API.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    dict: revision data.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m rev_info \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m data:\n\u001b[1;32m     18\u001b[0m     rev_info[(page[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpageid\u001b[39m\u001b[38;5;124m'\u001b[39m], page[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m])] \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevisions\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28;01mNone\u001b[39;00m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rev_info\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/wikitoolkit/api.py:76\u001b[0m, in \u001b[0;36mquery_async\u001b[0;34m(session, query_args, continuation, debug, httpmethod, posturl)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Perform the initial query\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m httpmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m     continued \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(session\u001b[38;5;241m.\u001b[39mget(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     77\u001b[0m                                                     continuation\u001b[38;5;241m=\u001b[39mcontinuation,\n\u001b[1;32m     78\u001b[0m                                                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mquery_args))\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m httpmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39mposturl, json\u001b[38;5;241m=\u001b[39mquery_args) \u001b[38;5;28;01mas\u001b[39;00m response:\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/mwapi/async_session.py:185\u001b[0m, in \u001b[0;36mAsyncSession.get\u001b[0;34m(self, query_continue, auth, continuation, **params)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_continue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, continuation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    164\u001b[0m               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Makes an API request with the GET method\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    :Parameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m        :class:`mwapi.errors.APIError` : if the API responds with an error\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, params\u001b[38;5;241m=\u001b[39mparams, auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    186\u001b[0m                               query_continue\u001b[38;5;241m=\u001b[39mquery_continue,\n\u001b[1;32m    187\u001b[0m                               continuation\u001b[38;5;241m=\u001b[39mcontinuation)\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/mwapi/async_session.py:149\u001b[0m, in \u001b[0;36mAsyncSession.request\u001b[0;34m(self, method, params, query_continue, auth, continuation)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_continuation(method, params\u001b[38;5;241m=\u001b[39mnormal_params, auth\u001b[38;5;241m=\u001b[39mauth)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(method, params\u001b[38;5;241m=\u001b[39mnormal_params, auth\u001b[38;5;241m=\u001b[39mauth)\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/mwapi/async_session.py:81\u001b[0m, in \u001b[0;36mAsyncSession._request\u001b[0;34m(self, method, params, auth)\u001b[0m\n\u001b[1;32m     78\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_url,\n\u001b[1;32m     82\u001b[0m                                     params\u001b[38;5;241m=\u001b[39mparams, data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     83\u001b[0m                                     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m     84\u001b[0m                                     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m     85\u001b[0m                                     verify_ssl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     86\u001b[0m                                     auth\u001b[38;5;241m=\u001b[39mauth) \u001b[38;5;28;01mas\u001b[39;00m resp:\n\u001b[1;32m     88\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m doc:\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/aiohttp/client.py:1423\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[0;32m-> 1423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp: _RetType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/aiohttp/client.py:728\u001b[0m, in \u001b[0;36mClientSession._request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001b[0m\n\u001b[1;32m    726\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m req\u001b[38;5;241m.\u001b[39msend(conn)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 728\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstart(conn)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     resp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/aiohttp/client_reqrep.py:1055\u001b[0m, in \u001b[0;36mClientResponse.start\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1054\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol\n\u001b[0;32m-> 1055\u001b[0m     message, payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m http\u001b[38;5;241m.\u001b[39mHttpProcessingError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1062\u001b[0m         headers\u001b[38;5;241m=\u001b[39mexc\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1063\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/aiohttp/streams.py:668\u001b[0m, in \u001b[0;36mDataQueue.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop\u001b[38;5;241m.\u001b[39mcreate_future()\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio\u001b[38;5;241m.\u001b[39mCancelledError, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maxgroupsize = 10000\n",
    "for lang, da_dict in startdate_article_dict.items():\n",
    "    print(lang)\n",
    "    l_revisions_df = []\n",
    "    if lang not in pagemapsdict:\n",
    "        pagemapsdict[lang] = wt.PageMaps()\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    for n, (date, articles) in enumerate(da_dict.items()):\n",
    "        if n % 1000 == 0:\n",
    "            print(n/len(da_dict), end='\\r')\n",
    "        grouprevs = {}\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                revisions = await wt.get_revision(wtsession, titles=articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                                                 date=date.isoformat())\n",
    "                grouprevs.update(revisions)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "            except ValueError as e:\n",
    "                print(e, 'Reducing group size to', groupsize // 2)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "\n",
    "        date_revisions = pd.DataFrame({k: v for k, v in grouprevs.items() if v}).T.reset_index().rename(columns={'index': 'title'})\n",
    "        if len(date_revisions) == 0:\n",
    "            continue\n",
    "        date_revisions['lang'] = lang\n",
    "        date_revisions = date_revisions[['lang', 'title', 'revid', 'parentid', 'timestamp']]\n",
    "        l_revisions_df.append(date_revisions)\n",
    "\n",
    "    if len(l_revisions_df) > 0:\n",
    "        l_revisions_df = pd.concat(l_revisions_df, ignore_index=True)\n",
    "        l_revisions_df.to_hdf('data/date_revisions.h5', key=f'/{lang}', mode='a')\n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar: 1336 articles remaining. 1000 articles processed.\r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lang_title_dict = ranges_df[['lang', 'title']].drop_duplicates().groupby('lang')['title'].apply(list).to_dict()\n",
    "\n",
    "maxgroupsize = 1000\n",
    "l_topics_df = []\n",
    "for lang, articles in lang_title_dict.items():\n",
    "\n",
    "    if lang not in pagemapsdict:\n",
    "        pagemapsdict[lang] = wt.PageMaps()\n",
    "\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "\n",
    "    grouptopics = {}\n",
    "    groupsize = maxgroupsize\n",
    "    while len(articles) > 0:\n",
    "        print(f'{lang}: {len(articles)} articles remaining. {len(grouptopics)} articles processed.', end='\\r')\n",
    "        try:\n",
    "            # a_topics = await wt.get_articles_topics(wtsession, articles[:groupsize],\n",
    "            #                                      lang=lang, tf_args={'threshold': 0},\n",
    "            #                                      pagemaps=pagemapsdict[lang])\n",
    "            a_topics = wt.get_articles_topics_sync(wtsession, articles[:groupsize],\n",
    "                                        lang=lang, tf_args={'threshold': 0},\n",
    "                                        pagemaps=pagemapsdict[lang])\n",
    "            grouptopics.update(a_topics)\n",
    "            articles = articles[groupsize:]\n",
    "            groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "        except ValueError as e:\n",
    "            print(e, 'Reducing group size to', groupsize // 2)\n",
    "            time.sleep(0.1)\n",
    "            groupsize = groupsize // 2\n",
    "\n",
    "    grouptopics = pd.concat({k: pd.Series(v) for k, v in grouptopics.items()}).reset_index().rename(\n",
    "                    columns={'level_0': 'article', 'level_1': 'topic', 0: 'score'})\n",
    "    grouptopics = grouptopics.pivot(index='article', columns='topic', values='score').reset_index().rename_axis(None, axis=1)\n",
    "    grouptopics['lang'] = lang\n",
    "    grouptopics = grouptopics.set_index(['lang', 'article'])\n",
    "    l_topics_df.append(grouptopics)\n",
    "    \n",
    "    await wtsession.close()\n",
    "\n",
    "l_topics_df = pd.concat(l_topics_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa successful read\n",
      "aa 0.0\n",
      "ab successful read\n",
      "ab 0.0\n",
      "ace successful read\n",
      "ace 0.0\n",
      "ady successful read\n",
      "ady 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af successful read\n",
      "af 0.0\n",
      "ERROR while fetching and parsing ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/af.wikipedia/all-access/all-agents/Melianthus/daily/2021120200/2021122300']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/patrick/Documents/R4R/.venv/lib/python3.12/site-packages/mwviews/api/pageviews.py\", line 146, in article_views\n",
      "    raise Exception(\n",
      "Exception: The pageview API returned nothing useful at: ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/af.wikipedia/all-access/all-agents/Melianthus/daily/2021120200/2021122300']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "als successful read\n",
      "als 0.0\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/done_langdateranges_pv.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     66\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(done_langdateranges, f)        \n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m wtsession\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/wikitoolkit/api.py:353\u001b[0m, in \u001b[0;36mWTSession.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Close the session objects.\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmw_session\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlw_session\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/aiohttp/client.py:1262\u001b[0m, in \u001b[0;36mClientSession.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector_owner:\n\u001b[0;32m-> 1262\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/R4R/.venv/lib/python3.12/site-packages/aiohttp/helpers.py:119\u001b[0m, in \u001b[0;36mnoop.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__await__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists('data/done_langdateranges_pv.pkl'):\n",
    "    with open('data/done_langdateranges_pv.pkl', 'rb') as f:\n",
    "        done_langdateranges = pickle.load(f)\n",
    "else:\n",
    "    done_langdateranges = {}\n",
    "\n",
    "maxgroupsize = 10000\n",
    "for lang, da_dict in date_article_dict.items():\n",
    "    if lang not in pagemapsdict:\n",
    "        pagemapsdict[lang] = wt.PageMaps()\n",
    "    if lang not in done_langdateranges:\n",
    "        done_langdateranges[lang] = set()\n",
    "    try:\n",
    "        l_pvdf = [pd.read_hdf('data/date_pageviews.h5', key=f'/{lang}')]\n",
    "        print(lang, 'successful read')\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        l_pvdf = []\n",
    "\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    \n",
    "    for n, (daterange, articles) in enumerate(da_dict.items()):\n",
    "        if n % 100 == 0:\n",
    "            print(lang, n/len(da_dict))\n",
    "        if daterange in done_langdateranges[lang]:\n",
    "            continue\n",
    "        grouppvs = []\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                try:\n",
    "                    pageviews = wt.api_article_views(wtsession, f'{lang}.wikipedia', articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                            start=daterange[0].strftime('%Y%m%d'), end=(daterange[1] - pd.Timedelta(days=1)).strftime('%Y%m%d'))\n",
    "                except Exception as e:\n",
    "                    if e.args[0][:44] == 'The pageview API returned nothing useful at:':\n",
    "                        pageviews = {d.to_pydatetime(): {x: 0 for x in articles}\n",
    "                                for d in pd.date_range(start=daterange[0], end=daterange[1] - pd.Timedelta(days=1))}\n",
    "                    else:\n",
    "                        raise e\n",
    "                    \n",
    "                pvdf = pd.DataFrame(pageviews).T.reset_index().rename(columns={'index': 'date'})\n",
    "                pvdf = pvdf.melt(id_vars='date', var_name='title', value_name='pageviews')\n",
    "                pvdf['lang'] = lang\n",
    "                pvdf = pvdf[['lang', 'title', 'date', 'pageviews']]\n",
    "                grouppvs.append(pvdf)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "            except ValueError as e:\n",
    "                print(e, 'Reducing group size to', groupsize // 2)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "        gpvdf = pd.concat(grouppvs, ignore_index=True)\n",
    "        l_pvdf.append(gpvdf)\n",
    "        done_langdateranges[lang].add(daterange)\n",
    "\n",
    "        if (len(l_pvdf) > 0) & (n % 1000 == 0) & (n > 0):\n",
    "            print('writing intermediate')\n",
    "            l_pvdfw = pd.concat(l_pvdf, ignore_index=True)\n",
    "            l_pvdfw.to_hdf('data/date_pageviews.h5', key=f'/{lang}', mode='a')\n",
    "            with open('data/done_langdateranges_pv.pkl', 'wb') as f:\n",
    "                pickle.dump(done_langdateranges, f)\n",
    "\n",
    "    if len(l_pvdf) > 0:\n",
    "        l_pvdf = pd.concat(l_pvdf, ignore_index=True)\n",
    "        l_pvdf.to_hdf('data/date_pageviews.h5', key=f'/{lang}', mode='a')\n",
    "        with open('data/done_langdateranges_pv.pkl', 'wb') as f:\n",
    "            pickle.dump(done_langdateranges, f)        \n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikidata IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
