{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get page ranges\n",
    "\n",
    "#  round datetime to day, get Â± 7 days inclusive\n",
    "\n",
    "#  merge any date ranges that overlaps\n",
    "\n",
    "#  reorganise data into dict where key is date range and value is a list of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import unquote\n",
    "import wikitoolkit as wt\n",
    "import pickle\n",
    "import sqlite3\n",
    "import time\n",
    "import os\n",
    "from urllib3.exceptions import MaxRetryError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = 'wikireddit <p.gildersleve@exeter.ac.uk>'\n",
    "conn = sqlite3.connect('wikireddit.db')\n",
    "\n",
    "ranges_df = pd.read_hdf('data/ranges_df.h5')\n",
    "raw_ranges_df = pd.read_hdf('data/raw_ranges_df.h5')\n",
    "\n",
    "with open('data/langpagemaps.pkl', 'rb') as f:\n",
    "    pagemapsdict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ranges for pageviews and edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges_df_consol = ranges_df.groupby(['lang', 'start_date', 'end_date'])['title'].apply(list)\n",
    "date_article_dict = {}\n",
    "# convert ranges_df_consol to a dict of dicts\n",
    "for lang, start_date, end_date in ranges_df_consol.index:\n",
    "    if lang not in date_article_dict:\n",
    "        date_article_dict[lang] = {}\n",
    "    date_article_dict[lang][(start_date, end_date)] = ranges_df_consol[(lang, start_date, end_date)]\n",
    "\n",
    "raw_ranges_df_consol = raw_ranges_df.groupby(['lang', 'start_date', 'end_date'])['title'].apply(list)\n",
    "raw_date_article_dict = {}\n",
    "# convert raw_ranges_df_consol to a dict of dicts\n",
    "for lang, start_date, end_date in raw_ranges_df_consol.index:\n",
    "    if lang not in raw_date_article_dict:\n",
    "        raw_date_article_dict[lang] = {}\n",
    "    raw_date_article_dict[lang][(start_date, end_date)] = raw_ranges_df_consol[(lang, start_date, end_date)]\n",
    "\n",
    "starts_df_consol = ranges_df.groupby(['lang', 'start_date'])['title'].apply(list)\n",
    "startdate_article_dict = {}\n",
    "# convert starts_df_consol to a dict of dicts\n",
    "for lang, start_date in starts_df_consol.index:\n",
    "    if lang not in startdate_article_dict:\n",
    "        startdate_article_dict[lang] = {}\n",
    "    startdate_article_dict[lang][start_date] = starts_df_consol[(lang, start_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get revisions in range [from, to] (inclusive), as well as revisions at start of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/done_langdateranges_rev.pkl'):\n",
    "    with open('data/done_langdateranges_rev.pkl', 'rb') as f:\n",
    "        done_langdateranges = pickle.load(f)\n",
    "else:\n",
    "    done_langdateranges = {}\n",
    "\n",
    "maxgroupsize = 10000\n",
    "for lang, da_dict in date_article_dict.items():\n",
    "    print()\n",
    "    try:\n",
    "        l_revisions_df = [pd.read_hdf('data/revisions.h5', key=f'/{lang}')]\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        l_revisions_df = []\n",
    "    if lang not in done_langdateranges:\n",
    "        done_langdateranges[lang] = set()\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    for n, (daterange, articles) in enumerate(da_dict.items()):\n",
    "        if n % 100 == 0:\n",
    "            print(lang, n/len(da_dict), end='\\r')\n",
    "        if daterange in done_langdateranges[lang]:\n",
    "            continue\n",
    "\n",
    "        grouprevs = {}\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                revisions = await wt.get_revisions(wtsession, articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                                                start=daterange[0].isoformat(), stop=daterange[1].isoformat())\n",
    "                grouprevs.update(revisions)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "            except ValueError as e:\n",
    "                print(e, 'Reducing group size to', groupsize // 2)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "\n",
    "        revisions = pd.concat({k: pd.DataFrame(v) for k, v in grouprevs.items()}).reset_index(\n",
    "            level=1, drop=True).reset_index().rename(columns={'index': 'title'})\n",
    "        done_langdateranges[lang].add(daterange)\n",
    "        if len(revisions) == 0:\n",
    "            continue\n",
    "        revisions['lang'] = lang\n",
    "        revisions = revisions[['lang', 'title', 'revid', 'parentid', 'timestamp']]\n",
    "        l_revisions_df.append(revisions)\n",
    "\n",
    "        if (n%5000 == 0)&(n > 0): # save every 5000 for very large langs for safety\n",
    "            if len(l_revisions_df) > 0:\n",
    "                l_revisions_df_i = pd.concat(l_revisions_df, ignore_index=True)\n",
    "                l_revisions_df_i.to_hdf('data/revisions.h5', key=f'/{lang}', mode='a')\n",
    "                del l_revisions_df_i\n",
    "                with open('data/done_langdateranges_rev.pkl', 'wb') as f:\n",
    "                    pickle.dump(done_langdateranges, f)\n",
    "        \n",
    "    if len(l_revisions_df) > 0:\n",
    "        l_revisions_df = pd.concat(l_revisions_df, ignore_index=True)\n",
    "        l_revisions_df.to_hdf('data/revisions.h5', key=f'/{lang}', mode='a')\n",
    "        with open('data/done_langdateranges_rev.pkl', 'wb') as f:\n",
    "            pickle.dump(done_langdateranges, f)\n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get revisions at start of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/done_langdatestarts_rev.pkl'):\n",
    "    with open('data/done_langdatestarts_rev.pkl', 'rb') as f:\n",
    "        done_langdatestarts = pickle.load(f)\n",
    "else:\n",
    "    done_langdatestarts = {}\n",
    "\n",
    "maxgroupsize = 1000\n",
    "for lang, da_dict in startdate_article_dict.items():\n",
    "    print()\n",
    "    try:\n",
    "        l_revisions_df = [pd.read_hdf('data/revisions.h5', key=f'/{lang}')]\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        l_revisions_df = []\n",
    "    if lang not in done_langdatestarts:\n",
    "        done_langdatestarts[lang] = set()\n",
    "\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    for n, (date, articles) in enumerate(da_dict.items()):\n",
    "        if n % 10 == 0:\n",
    "            print(lang, n/len(da_dict), end='\\r')\n",
    "        if date in done_langdatestarts[lang]:\n",
    "            continue\n",
    "        grouprevs = {}\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                revisions = await wt.get_revision(wtsession, titles=articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                                                 date=date.isoformat())\n",
    "                grouprevs.update(revisions)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "            except ValueError as e:\n",
    "                print('\\n', n, e, 'Reducing group size to', groupsize // 2)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "\n",
    "        date_revisions = pd.DataFrame({k: v for k, v in grouprevs.items() if v}).T.reset_index().rename(columns={'index': 'title'})\n",
    "        done_langdatestarts[lang].add(date)\n",
    "        if len(date_revisions) == 0:\n",
    "            continue\n",
    "        date_revisions['lang'] = lang\n",
    "        date_revisions = date_revisions[['lang', 'title', 'revid', 'parentid', 'timestamp']]\n",
    "        l_revisions_df.append(date_revisions)\n",
    "\n",
    "        if (n%100 == 0)&(n > 0):\n",
    "            if len(l_revisions_df) > 0:\n",
    "                l_revisions_df_i = pd.concat(l_revisions_df, ignore_index=True)\n",
    "                l_revisions_df_i = l_revisions_df_i.astype({'revid': int, 'parentid': int}).drop_duplicates(subset=['revid'])\n",
    "                l_revisions_df_i.to_hdf('data/revisions.h5', key=f'/{lang}', mode='a')\n",
    "                del l_revisions_df_i\n",
    "                with open('data/done_langdatestarts_rev.pkl', 'wb') as f:\n",
    "                    pickle.dump(done_langdatestarts, f)\n",
    "\n",
    "    if len(l_revisions_df) > 0:\n",
    "        l_revisions_df = pd.concat(l_revisions_df, ignore_index=True)\n",
    "        l_revisions_df = l_revisions_df.astype({'revid': int, 'parentid': int}).drop_duplicates(subset=['revid'])\n",
    "        l_revisions_df.to_hdf('data/revisions.h5', key=f'/{lang}', mode='a')\n",
    "        with open('data/done_langdatestarts_rev.pkl', 'wb') as f:\n",
    "            pickle.dump(done_langdatestarts, f)\n",
    "\n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get pageviews for canonical and raw titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/done_langdateranges_raw_pv.pkl'):\n",
    "    with open('data/done_langdateranges_raw_pv.pkl', 'rb') as f:\n",
    "        done_langdateranges = pickle.load(f)\n",
    "else:\n",
    "    done_langdateranges = {}\n",
    "\n",
    "errors = []\n",
    "maxgroupsize = 10000\n",
    "for lang, da_dict in raw_date_article_dict.items():\n",
    "\n",
    "    if lang not in done_langdateranges:\n",
    "        done_langdateranges[lang] = set()\n",
    "    try:\n",
    "        l_pvdf = [pd.read_hdf('data/pageviews_raw.h5', key=f'/{lang}')]\n",
    "        print(lang, 'successful read')\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        l_pvdf = []\n",
    "\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    \n",
    "    for n, (daterange, articles) in enumerate(da_dict.items()):\n",
    "        if n % 100 == 0:\n",
    "            print(lang, n/len(da_dict), end='\\r')\n",
    "        if daterange in done_langdateranges[lang]:\n",
    "            continue\n",
    "        grouppvs = []\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                try:\n",
    "                    pageviews = await wt.api_article_views(wtsession, f'{lang}.wikipedia', articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                            start=daterange[0].strftime('%Y%m%d'), end=(daterange[1] - pd.Timedelta(days=1)).strftime('%Y%m%d'),\n",
    "                            agent='user', redirects=False, process=False)\n",
    "                except Exception as e:\n",
    "                    if e.args[0][:44] == 'The pageview API returned nothing useful at:':\n",
    "                        pageviews = {d.to_pydatetime(): {x: 0 for x in articles}\n",
    "                                for d in pd.date_range(start=daterange[0], end=daterange[1] - pd.Timedelta(days=1))}\n",
    "                    else:\n",
    "                        raise e\n",
    "                    \n",
    "                pvdf = pd.DataFrame(pageviews).T.reset_index().rename(columns={'index': 'date'})\n",
    "                pvdf = pvdf.melt(id_vars='date', var_name='title', value_name='pageviews')\n",
    "                pvdf['lang'] = lang\n",
    "                pvdf = pvdf[['lang', 'title', 'date', 'pageviews']]\n",
    "                grouppvs.append(pvdf)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "\n",
    "            except (ValueError, TypeError, MaxRetryError, IndexError) as e:\n",
    "                print(n/len(da_dict), 'Reducing group size to', groupsize // 2, end='\\r')\n",
    "                errors.append(e)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "\n",
    "        gpvdf = pd.concat(grouppvs, ignore_index=True)\n",
    "        l_pvdf.append(gpvdf)\n",
    "        done_langdateranges[lang].add(daterange)\n",
    "\n",
    "        if (len(l_pvdf) > 0) & (n % 5000 == 0) & (n > 0):\n",
    "            print('writing intermediate')\n",
    "            l_pvdfw = pd.concat(l_pvdf, ignore_index=True)\n",
    "            l_pvdfw.to_hdf('data/pageviews_raw.h5', key=f'/{lang}', mode='a')\n",
    "            with open('data/done_langdateranges_raw_pv.pkl', 'wb') as f:\n",
    "                pickle.dump(done_langdateranges, f)\n",
    "\n",
    "    if len(l_pvdf) > 0:\n",
    "        l_pvdf = pd.concat(l_pvdf, ignore_index=True)\n",
    "        l_pvdf.to_hdf('data/pageviews_raw.h5', key=f'/{lang}', mode='a')\n",
    "        with open('data/done_langdateranges_raw_pv.pkl', 'wb') as f:\n",
    "            pickle.dump(done_langdateranges, f)  \n",
    "                  \n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/done_langdateranges_pv.pkl'):\n",
    "    with open('data/done_langdateranges_pv.pkl', 'rb') as f:\n",
    "        done_langdateranges = pickle.load(f)\n",
    "else:\n",
    "    done_langdateranges = {}\n",
    "\n",
    "errors = []\n",
    "maxgroupsize = 10000\n",
    "for lang, da_dict in date_article_dict.items():\n",
    "\n",
    "    if lang not in done_langdateranges:\n",
    "        done_langdateranges[lang] = set()\n",
    "    try:\n",
    "        l_pvdf = [pd.read_hdf('data/pageviews.h5', key=f'/{lang}')]\n",
    "        print(lang, 'successful read')\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        l_pvdf = []\n",
    "\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    \n",
    "    for n, (daterange, articles) in enumerate(da_dict.items()):\n",
    "        if n % 100 == 0:\n",
    "            print(lang, n/len(da_dict), end='\\r')\n",
    "        if daterange in done_langdateranges[lang]:\n",
    "            continue\n",
    "        grouppvs = []\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                try:\n",
    "                    pageviews = await wt.api_article_views(wtsession, f'{lang}.wikipedia', articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                            start=daterange[0].strftime('%Y%m%d'), end=(daterange[1] - pd.Timedelta(days=1)).strftime('%Y%m%d'),\n",
    "                            agent='user', redirects=False)\n",
    "                except Exception as e:\n",
    "                    if e.args[0][:44] == 'The pageview API returned nothing useful at:':\n",
    "                        pageviews = {d.to_pydatetime(): {x: 0 for x in articles}\n",
    "                                for d in pd.date_range(start=daterange[0], end=daterange[1] - pd.Timedelta(days=1))}\n",
    "                    else:\n",
    "                        raise e\n",
    "                    \n",
    "                pvdf = pd.DataFrame(pageviews).T.reset_index().rename(columns={'index': 'date'})\n",
    "                pvdf = pvdf.melt(id_vars='date', var_name='title', value_name='pageviews')\n",
    "                pvdf['lang'] = lang\n",
    "                pvdf = pvdf[['lang', 'title', 'date', 'pageviews']]\n",
    "                grouppvs.append(pvdf)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "\n",
    "            except (ValueError, TypeError, MaxRetryError, IndexError) as e:\n",
    "                print(n/len(da_dict), 'Reducing group size to', groupsize // 2, end='\\r')\n",
    "                errors.append(e)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "\n",
    "        gpvdf = pd.concat(grouppvs, ignore_index=True)\n",
    "        l_pvdf.append(gpvdf)\n",
    "        done_langdateranges[lang].add(daterange)\n",
    "\n",
    "        if (len(l_pvdf) > 0) & (n % 5000 == 0) & (n > 0):\n",
    "            print('writing intermediate')\n",
    "            l_pvdfw = pd.concat(l_pvdf, ignore_index=True)\n",
    "            l_pvdfw.to_hdf('data/pageviews.h5', key=f'/{lang}', mode='a')\n",
    "            with open('data/done_langdateranges_pv.pkl', 'wb') as f:\n",
    "                pickle.dump(done_langdateranges, f)\n",
    "\n",
    "    if len(l_pvdf) > 0:\n",
    "        l_pvdf = pd.concat(l_pvdf, ignore_index=True)\n",
    "        l_pvdf.to_hdf('data/pageviews.h5', key=f'/{lang}', mode='a')\n",
    "        with open('data/done_langdateranges_pv.pkl', 'wb') as f:\n",
    "            pickle.dump(done_langdateranges, f)  \n",
    "                  \n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to sql db\n",
    "\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "def save_to_sql_with_retry(df, conn, table_name, retries=5, delay=1):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "            break\n",
    "        except sqlite3.OperationalError as e:\n",
    "            if 'database is locked' in str(e):\n",
    "                print(f\"Database is locked, retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "# erase any existing pageviews table\n",
    "\n",
    "conn.execute('DROP TABLE IF EXISTS pageviews;')\n",
    "\n",
    "# /write pageviews to sql\n",
    "\n",
    "for lang in date_article_dict:\n",
    "    print(lang)\n",
    "    pvdf = pd.read_hdf('data/pageviews.h5', key=f'/{lang}')\n",
    "    try:\n",
    "        pvrdf = pd.read_hdf('data/pageviews_raw.h5', key=f'/{lang}')\n",
    "        final_pvdf = pd.concat([pvdf, pvrdf], ignore_index=True).drop_duplicates(subset=['lang', 'title', 'date'])\n",
    "    except KeyError:\n",
    "        final_pvdf = pvdf.drop_duplicates(subset=['lang', 'title', 'date'])\n",
    "\n",
    "    save_to_sql_with_retry(final_pvdf, conn, 'pageviews')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get storage size of database\n",
    "import os\n",
    "mb_size = os.path.getsize('wikireddit.db') / 1024 / 1024\n",
    "print(f\"Database size: {mb_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_title_dict = ranges_df[['lang', 'title']].drop_duplicates().groupby('lang')['title'].apply(list).to_dict()\n",
    "\n",
    "maxgroupsize = 1000\n",
    "l_topics_df = []\n",
    "xx = None\n",
    "for lang, articles in lang_title_dict.items():\n",
    "\n",
    "    if lang not in pagemapsdict:\n",
    "        pagemapsdict[lang] = wt.PageMaps()\n",
    "\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "\n",
    "    grouptopics = {}\n",
    "    groupsize = maxgroupsize\n",
    "    counter = 0\n",
    "    la = None\n",
    "    while len(articles) > 0:\n",
    "        print(f'{lang}: {len(articles)} articles remaining. {len(grouptopics)} articles processed.', end='\\r')\n",
    "        try:\n",
    "\n",
    "            a_topics = wt.get_articles_topics_sync(wtsession, articles[:groupsize],\n",
    "                                        lang=lang, tf_args={'threshold': 0},\n",
    "                                        pagemaps=pagemapsdict[lang])\n",
    "            grouptopics.update(a_topics)\n",
    "            articles = articles[groupsize:]\n",
    "            groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            if len(articles) == la:\n",
    "                counter += 1\n",
    "                time.sleep(10)\n",
    "                if counter > 10:\n",
    "                    print(f'{lang}: {len(articles)} articles remaining. {len(grouptopics)} articles processed.')\n",
    "                    print(e, 'emergency_save')\n",
    "                    l_topics_df = pd.concat(l_topics_df, ignore_index=True)\n",
    "                    l_topics_df.to_hdf('data/topics_df_exit.h5', key='df')\n",
    "                    with open('data/grouptopics.pkl_exit', 'wb') as f:\n",
    "                        pickle.dump(grouptopics, f)\n",
    "\n",
    "            print(e, 'Reducing group size to', groupsize // 2)\n",
    "            time.sleep(0.1)\n",
    "            groupsize = groupsize // 2\n",
    "\n",
    "    grouptopics = pd.concat({k: pd.Series(v) for k, v in grouptopics.items()}).reset_index().rename(\n",
    "                    columns={'level_0': 'article', 'level_1': 'topic', 0: 'score'})\n",
    "    grouptopics = grouptopics.pivot(index='article', columns='topic', values='score').reset_index().rename_axis(None, axis=1)\n",
    "    grouptopics['lang'] = lang\n",
    "    grouptopics = grouptopics.set_index(['lang', 'article'])\n",
    "    l_topics_df.append(grouptopics)\n",
    "    \n",
    "    await wtsession.close()\n",
    "\n",
    "l_topics_df = pd.concat(l_topics_df, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
