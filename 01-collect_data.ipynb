{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import syft as sy\n",
    "import time\n",
    "import os\n",
    "import dotenv\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into <reddit-for-researchers-internal: High side Datasite> as <prgildersleve@gmail.com>\n"
     ]
    }
   ],
   "source": [
    "# we recommend that you source these values using environment variables\n",
    "dotenv.load_dotenv()\n",
    "URL = \"https://reddit-for-researchers.snooguts.net\"\n",
    "EMAIL = os.getenv(\"EMAIL\")\n",
    "PASSWORD = os.getenv(\"PASSWORD\")\n",
    "\n",
    "# you can provide a \"password\" keyword argument, but if you don't...\n",
    "# the browser will prompt you for input\n",
    "client = sy.login(\n",
    "    url=URL,\n",
    "    email=EMAIL,\n",
    "    password=PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_to_id_dict(client):\n",
    "    # TODO handle case with duplicate names?\n",
    "    return {request.code.service_func_name: str(request.id) for request in client.requests}\n",
    "\n",
    "\n",
    "def query_r4r(function_name, name_to_id_dict, client):\n",
    "\n",
    "    while True:   \n",
    "        try:\n",
    "            request = client.api.services.request.get_by_uid(uid=sy.UID(name_to_id_dict[function_name]))\n",
    "            job = request.code(blocking=False)\n",
    "            results = job.wait()\n",
    "            df = results.get()\n",
    "            return df\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            time.sleep(10)\n",
    "            client = sy.login(\n",
    "                url=URL,\n",
    "                email=EMAIL,\n",
    "                password=PASSWORD\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcdict = name_to_id_dict(client)\n",
    "funcdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = client.api.services.request.get_by_uid(uid=sy.UID(funcdict[\"avg_comment_score_pg\"]))\n",
    "job = request.code(blocking=False)\n",
    "results = job.wait()\n",
    "results.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posts\n",
    "\n",
    "posts_dict = {}\n",
    "for year in range(2020, 2024):\n",
    "    key = f\"pg_wiki_{year}\"\n",
    "    print(key)\n",
    "    try:\n",
    "        posts_dict[key] = pd.read_hdf(f\"data/{key}.h5\")\n",
    "    except:\n",
    "        posts_dict[key] = query_r4r(key, funcdict, client)\n",
    "        for c in ['score', 'gildings', 'num_comments']:\n",
    "            posts_dict[key][c] = posts_dict[key][c].astype(np.int64)\n",
    "        for c in ['nsfw', 'self', 'video', 'locked', 'spoiler', 'sticky']:\n",
    "            posts_dict[key][c] = posts_dict[key][c].astype(np.bool_)\n",
    "        for c in ['created_at', 'updated_at']:\n",
    "            posts_dict[key][c] = pd.to_datetime(posts_dict[key][c]).astype('datetime64[ns]')\n",
    "\n",
    "        posts_dict[key].to_hdf(f\"data/{key}.h5\", key='df', mode='w')\n",
    "\n",
    "posts_df = pd.concat(posts_dict.values())\n",
    "posts_df.to_hdf(\"data/posts.h5\", key='df', mode='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_hdf(\"data/posts.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments\n",
    "\n",
    "comments_dict = {}\n",
    "for year in range(2020, 2024):\n",
    "    for month in range(1, 13):\n",
    "        key = f\"pg_wiki_comments_{year}{month:02d}\"\n",
    "        print(key)\n",
    "        try:\n",
    "            comments_dict[key] = pd.read_hdf(f\"data/{key}.h5\")\n",
    "        except:\n",
    "            comments_dict[key] = query_r4r(key, funcdict, client)\n",
    "            for c in ['score']:\n",
    "                comments_dict[key][c] = comments_dict[key][c].astype(np.int64)\n",
    "            for c in ['gilded']:\n",
    "                comments_dict[key][c] = comments_dict[key][c].astype(np.bool_)\n",
    "            for c in ['created_at', 'last_modified_at']:\n",
    "                comments_dict[key][c] = pd.to_datetime(comments_dict[key][c]).astype('datetime64[ns]')\n",
    "            comments_dict[key].to_hdf(f\"data/{key}.h5\", key='df', mode='w')\n",
    "\n",
    "comments_df = pd.concat(comments_dict.values())\n",
    "os.remove(\"data/comments_1.h5\")\n",
    "comments_df.iloc[:len(comments_df)//4].to_hdf(\"data/comments_1.h5\", key='df', mode='w')\n",
    "os.remove(\"data/comments_2.h5\")\n",
    "comments_df.iloc[len(comments_df)//4:len(comments_df)//2].to_hdf(\"data/comments_2.h5\", key='df', mode='w')\n",
    "os.remove(\"data/comments_3.h5\")\n",
    "comments_df.iloc[len(comments_df)//2:len(comments_df)//4*3].to_hdf(\"data/comments_3.h5\", key='df', mode='w')\n",
    "os.remove(\"data/comments_4.h5\")\n",
    "comments_df.iloc[len(comments_df)//4*3:].to_hdf(\"data/comments_4.h5\", key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  replies\n",
    "\n",
    "replies_df = query_r4r('pg_wiki_replies', funcdict, client)\n",
    "for c in ['score']:\n",
    "    replies_df[c] = replies_df[c].astype(np.int64)\n",
    "for c in ['gilded']:\n",
    "    replies_df[c] = replies_df[c].astype(np.bool_)\n",
    "for c in ['created_at', 'last_modified_at']:\n",
    "    replies_df[c] = pd.to_datetime(replies_df[c]).astype('datetime64[ns]')\n",
    "replies_df.to_hdf(f\"data/replies.h5\", key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replies\n",
    "\n",
    "replies_dict = {}\n",
    "for year in range(2020, 2024):\n",
    "    key = f\"pg_wiki_replies_{year}\"\n",
    "    print(key)\n",
    "    try:\n",
    "        replies_dict[key] = pd.read_hdf(f\"data/{key}.h5\")\n",
    "    except:\n",
    "        replies_dict[key] = query_r4r(key, funcdict, client)\n",
    "        for c in ['score']:\n",
    "            replies_dict[key][c] = replies_dict[key][c].astype(np.int64)\n",
    "        for c in ['gilded']:\n",
    "            replies_dict[key][c] = replies_dict[key][c].astype(np.bool_)\n",
    "        for c in ['created_at', 'last_modified_at']:\n",
    "            replies_dict[key][c] = pd.to_datetime(replies_dict[key][c]).astype('datetime64[ns]')\n",
    "\n",
    "        replies_dict[key].to_hdf(f\"data/{key}.h5\", key='df', mode='w')\n",
    "\n",
    "replies_df = pd.concat(replies_dict.values())\n",
    "os.remove(\"data/replies.h5\")\n",
    "replies_df.to_hdf(\"data/replies.h5\", key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_dict = {}\n",
    "for year in range(2020, 2024):\n",
    "    for month in range(1, 13):\n",
    "        key = f\"pg_wiki_replies_{year}\"\n",
    "        print(key)\n",
    "        try:\n",
    "            replies_dict[key] = pd.read_hdf(f\"data/{key}.h5\")\n",
    "        except:\n",
    "            replies_dict[key] = query_r4r(key, funcdict, client)\n",
    "            for c in ['score']:\n",
    "                replies_dict[key][c] = replies_dict[key][c].astype(np.int64)\n",
    "            for c in ['gilded']:\n",
    "                replies_dict[key][c] = replies_dict[key][c].astype(np.bool_)\n",
    "            for c in ['created_at', 'last_modified_at']:\n",
    "                replies_dict[key][c] = pd.to_datetime(replies_dict[key][c]).astype('datetime64[ns]')\n",
    "\n",
    "            replies_dict[key].to_hdf(f\"data/{key}.h5\", key='df', mode='w')\n",
    "\n",
    "replies_df = pd.concat(replies_dict.values())\n",
    "os.remove(\"data/replies.h5\")\n",
    "replies_df.to_hdf(\"data/replies.h5\", key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_hdf(\"data/posts.h5\")\n",
    "comments_df = pd.concat([pd.read_hdf(f\"data/comments_{i}.h5\") for i in range(1, 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_132352/3707721486.py:4: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'subreddit_id', 'title', 'body', 'url', 'author_id',\n",
      "       'distinguished', 'flair_text', 'language_code', 'thumbnail',\n",
      "       'crosspost_parent_id', 'permalink'],\n",
      "      dtype='object')]\n",
      "\n",
      "  posts_df.to_hdf(\"data/posts.h5\", key='df', mode='w')\n",
      "/tmp/ipykernel_132352/3707721486.py:7: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block4_values] [items->Index(['id', 'post_id', 'parent_id', 'body', 'author_id', 'subreddit_id',\n",
      "       'permalink'],\n",
      "      dtype='object')]\n",
      "\n",
      "  comments_df.iloc[len(comments_df)//4*(i-1):len(comments_df)//4*i].to_hdf(f\"data/comments_{i}.h5\", key='df', mode='w')\n",
      "/tmp/ipykernel_132352/3707721486.py:7: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block4_values] [items->Index(['id', 'post_id', 'parent_id', 'body', 'author_id', 'subreddit_id',\n",
      "       'permalink'],\n",
      "      dtype='object')]\n",
      "\n",
      "  comments_df.iloc[len(comments_df)//4*(i-1):len(comments_df)//4*i].to_hdf(f\"data/comments_{i}.h5\", key='df', mode='w')\n",
      "/tmp/ipykernel_132352/3707721486.py:7: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block4_values] [items->Index(['id', 'post_id', 'parent_id', 'body', 'author_id', 'subreddit_id',\n",
      "       'permalink'],\n",
      "      dtype='object')]\n",
      "\n",
      "  comments_df.iloc[len(comments_df)//4*(i-1):len(comments_df)//4*i].to_hdf(f\"data/comments_{i}.h5\", key='df', mode='w')\n",
      "/tmp/ipykernel_132352/3707721486.py:7: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block4_values] [items->Index(['id', 'post_id', 'parent_id', 'body', 'author_id', 'subreddit_id',\n",
      "       'permalink'],\n",
      "      dtype='object')]\n",
      "\n",
      "  comments_df.iloc[len(comments_df)//4*(i-1):len(comments_df)//4*i].to_hdf(f\"data/comments_{i}.h5\", key='df', mode='w')\n"
     ]
    }
   ],
   "source": [
    "posts_df.loc[posts_df['updated_at'] == pd.Timestamp('1970-01-01 00:00:00'), 'updated_at'] = pd.NaT\n",
    "comments_df.loc[comments_df['last_modified_at'] == pd.Timestamp('1970-01-01 00:00:00'), 'last_modified_at'] = pd.NaT\n",
    "\n",
    "posts_df.to_hdf(\"data/posts.h5\", key='df', mode='w')\n",
    "for i in range(1, 5):\n",
    "    os.remove(f\"data/comments_{i}.h5\")\n",
    "    comments_df.iloc[len(comments_df)//4*(i-1):len(comments_df)//4*i].to_hdf(f\"data/comments_{i}.h5\", key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: updated_at, dtype: datetime64[ns])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: last_modified_at, dtype: datetime64[ns])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(posts_df['updated_at'][(posts_df['updated_at'] == pd.Timestamp('1970-01-01 00:00:00'))])\n",
    "display(comments_df['last_modified_at'][(comments_df['last_modified_at'] == pd.Timestamp('1970-01-01 00:00:00'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.loc[posts_df['updated_at'] == pd.Timestamp('1970-01-01 00:00:00'), 'updated_at'] = pd.NaT\n",
    "comments_df.loc[comments_df['last_modified_at'] == pd.Timestamp('1970-01-01 00:00:00'), 'last_modified_at'] = pd.NaT\n",
    "ccols = ['subreddit_id', 'post_id', 'parent_id', 'id', 'created_at',\n",
    "         'last_modified_at', 'score', 'upvote_ratio', 'gilded']\n",
    "cw = comments_df[ccols]\n",
    "pcols = ['subreddit_id', 'crosspost_parent_id', 'id', 'created_at', 'updated_at',\n",
    "         'language_code', 'score', 'upvote_ratio', 'gildings', 'num_comments']\n",
    "pw = posts_df[pcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10264340"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  create sqlite database\n",
    "conn = sqlite3.connect('wikireddit.db')\n",
    "posts_df[pcols].to_sql('posts', conn, index=False, if_exists='replace')\n",
    "comments_df[ccols].to_sql('comments', conn, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test read some rows from each table\n",
    "conn = sqlite3.connect('wikireddit.db')\n",
    "display(pd.read_sql('SELECT * FROM posts LIMIT 5', conn))\n",
    "display(pd.read_sql('SELECT * FROM comments LIMIT 5', conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database size: 1191.16 MB\n"
     ]
    }
   ],
   "source": [
    "# get storage size of database\n",
    "import os\n",
    "mb_size = os.path.getsize('wikireddit.db') / 1024 / 1024\n",
    "print(f\"Database size: {mb_size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
