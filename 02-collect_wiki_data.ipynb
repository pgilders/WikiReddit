{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import unquote\n",
    "import wikitoolkit as wt\n",
    "import pickle\n",
    "import sqlite3\n",
    "import time\n",
    "import os\n",
    "from urllib3.exceptions import MaxRetryError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = 'wikireddit <p.gildersleve@exeter.ac.uk>'\n",
    "conn = sqlite3.connect('wikireddit.db')\n",
    "\n",
    "ranges_df = pd.read_hdf('data/ranges_df.h5')\n",
    "raw_ranges_df = pd.read_hdf('data/raw_ranges_df.h5')\n",
    "\n",
    "with open('data/langpagemaps.pkl', 'rb') as f:\n",
    "    pagemapsdict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ranges for pageviews and edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges_df_consol = ranges_df.groupby(['lang', 'start_date', 'end_date'])['title'].apply(list)\n",
    "date_article_dict = {}\n",
    "# convert ranges_df_consol to a dict of dicts\n",
    "for lang, start_date, end_date in ranges_df_consol.index:\n",
    "    if lang not in date_article_dict:\n",
    "        date_article_dict[lang] = {}\n",
    "    date_article_dict[lang][(start_date, end_date)] = ranges_df_consol[(lang, start_date, end_date)]\n",
    "\n",
    "raw_ranges_df_consol = raw_ranges_df.groupby(['lang', 'start_date', 'end_date'])['title'].apply(list)\n",
    "raw_date_article_dict = {}\n",
    "# convert raw_ranges_df_consol to a dict of dicts\n",
    "for lang, start_date, end_date in raw_ranges_df_consol.index:\n",
    "    if lang not in raw_date_article_dict:\n",
    "        raw_date_article_dict[lang] = {}\n",
    "    raw_date_article_dict[lang][(start_date, end_date)] = raw_ranges_df_consol[(lang, start_date, end_date)]\n",
    "\n",
    "starts_df_consol = ranges_df.groupby(['lang', 'start_date'])['title'].apply(list)\n",
    "startdate_article_dict = {}\n",
    "# convert starts_df_consol to a dict of dicts\n",
    "for lang, start_date in starts_df_consol.index:\n",
    "    if lang not in startdate_article_dict:\n",
    "        startdate_article_dict[lang] = {}\n",
    "    startdate_article_dict[lang][start_date] = starts_df_consol[(lang, start_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get revisions in range [from, to] (inclusive), as well as revisions at start of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/done_langdateranges_rev.pkl'):\n",
    "    with open('data/done_langdateranges_rev.pkl', 'rb') as f:\n",
    "        done_langdateranges = pickle.load(f)\n",
    "else:\n",
    "    done_langdateranges = {}\n",
    "\n",
    "maxgroupsize = 10000\n",
    "for lang, da_dict in date_article_dict.items():\n",
    "    print()\n",
    "    try:\n",
    "        l_revisions_df = [pd.read_hdf('data/revisions.h5', key=f'/{lang}')]\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        l_revisions_df = []\n",
    "    if lang not in done_langdateranges:\n",
    "        done_langdateranges[lang] = set()\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    for n, (daterange, articles) in enumerate(da_dict.items()):\n",
    "        if n % 100 == 0:\n",
    "            print(lang, n/len(da_dict), end='\\r')\n",
    "        if daterange in done_langdateranges[lang]:\n",
    "            continue\n",
    "\n",
    "        grouprevs = {}\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                revisions = await wt.get_revisions(wtsession, articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                                                start=daterange[0].isoformat(), stop=daterange[1].isoformat())\n",
    "                grouprevs.update(revisions)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "            except ValueError as e:\n",
    "                print(e, 'Reducing group size to', groupsize // 2)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "\n",
    "        revisions = pd.concat({k: pd.DataFrame(v) for k, v in grouprevs.items()}).reset_index(\n",
    "            level=1, drop=True).reset_index().rename(columns={'index': 'title'})\n",
    "        done_langdateranges[lang].add(daterange)\n",
    "        if len(revisions) == 0:\n",
    "            continue\n",
    "        revisions['lang'] = lang\n",
    "        revisions = revisions[['lang', 'title', 'revid', 'parentid', 'timestamp']]\n",
    "        l_revisions_df.append(revisions)\n",
    "\n",
    "        if (n%5000 == 0)&(n > 0): # save every 5000 for very large langs for safety\n",
    "            if len(l_revisions_df) > 0:\n",
    "                l_revisions_df_i = pd.concat(l_revisions_df, ignore_index=True)\n",
    "                l_revisions_df_i.to_hdf('data/revisions.h5', key=f'/{lang}', mode='a')\n",
    "                del l_revisions_df_i\n",
    "                with open('data/done_langdateranges_rev.pkl', 'wb') as f:\n",
    "                    pickle.dump(done_langdateranges, f)\n",
    "        \n",
    "    if len(l_revisions_df) > 0:\n",
    "        l_revisions_df = pd.concat(l_revisions_df, ignore_index=True)\n",
    "        l_revisions_df.to_hdf('data/revisions.h5', key=f'/{lang}', mode='a')\n",
    "        with open('data/done_langdateranges_rev.pkl', 'wb') as f:\n",
    "            pickle.dump(done_langdateranges, f)\n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get revisions at start of range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/done_langdatestarts_rev.pkl'):\n",
    "    with open('data/done_langdatestarts_rev.pkl', 'rb') as f:\n",
    "        done_langdatestarts = pickle.load(f)\n",
    "else:\n",
    "    done_langdatestarts = {}\n",
    "\n",
    "maxgroupsize = 1000\n",
    "for lang, da_dict in startdate_article_dict.items():\n",
    "    print()\n",
    "    try:\n",
    "        l_revisions_df = [pd.read_hdf('data/revisions.h5', key=f'/{lang}')]\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        l_revisions_df = []\n",
    "    if lang not in done_langdatestarts:\n",
    "        done_langdatestarts[lang] = set()\n",
    "\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    for n, (date, articles) in enumerate(da_dict.items()):\n",
    "        if n % 10 == 0:\n",
    "            print(lang, n/len(da_dict), end='\\r')\n",
    "        if date in done_langdatestarts[lang]:\n",
    "            continue\n",
    "        grouprevs = {}\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                revisions = await wt.get_revision(wtsession, titles=articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                                                 date=date.isoformat())\n",
    "                grouprevs.update(revisions)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "            except ValueError as e:\n",
    "                print('\\n', n, e, 'Reducing group size to', groupsize // 2)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "\n",
    "        date_revisions = pd.DataFrame({k: v for k, v in grouprevs.items() if v}).T.reset_index().rename(columns={'index': 'title'})\n",
    "        done_langdatestarts[lang].add(date)\n",
    "        if len(date_revisions) == 0:\n",
    "            continue\n",
    "        date_revisions['lang'] = lang\n",
    "        date_revisions = date_revisions[['lang', 'title', 'revid', 'parentid', 'timestamp']]\n",
    "        l_revisions_df.append(date_revisions)\n",
    "\n",
    "        if (n%100 == 0)&(n > 0):\n",
    "            if len(l_revisions_df) > 0:\n",
    "                l_revisions_df_i = pd.concat(l_revisions_df, ignore_index=True)\n",
    "                l_revisions_df_i = l_revisions_df_i.astype({'revid': int, 'parentid': int}).drop_duplicates(subset=['revid'])\n",
    "                l_revisions_df_i.to_hdf('data/revisions.h5', key=f'/{lang}', mode='a')\n",
    "                del l_revisions_df_i\n",
    "                with open('data/done_langdatestarts_rev.pkl', 'wb') as f:\n",
    "                    pickle.dump(done_langdatestarts, f)\n",
    "\n",
    "    if len(l_revisions_df) > 0:\n",
    "        l_revisions_df = pd.concat(l_revisions_df, ignore_index=True)\n",
    "        l_revisions_df = l_revisions_df.astype({'revid': int, 'parentid': int}).drop_duplicates(subset=['revid'])\n",
    "        l_revisions_df.to_hdf('data/revisions.h5', key=f'/{lang}', mode='a')\n",
    "        with open('data/done_langdatestarts_rev.pkl', 'wb') as f:\n",
    "            pickle.dump(done_langdatestarts, f)\n",
    "\n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save to sql db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save revisions to sql\n",
    "conn.execute('DROP TABLE IF EXISTS revisions;')\n",
    "\n",
    "for lang in date_article_dict.keys():\n",
    "    try:\n",
    "        revisions_df = pd.read_hdf('data/revisions.h5', key=f'/{lang}').drop_duplicates(subset=['revid'])\n",
    "    except KeyError:\n",
    "        print(lang, 'no revisions')\n",
    "        continue\n",
    "    revisions_df.to_sql('revisions', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database size: 13051.60 MB\n"
     ]
    }
   ],
   "source": [
    "# get storage size of database\n",
    "import os\n",
    "mb_size = os.path.getsize('wikireddit.db') / 1024 / 1024\n",
    "print(f\"Database size: {mb_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get pageviews for canonical and raw titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/done_langdateranges_raw_pv.pkl'):\n",
    "    with open('data/done_langdateranges_raw_pv.pkl', 'rb') as f:\n",
    "        done_langdateranges = pickle.load(f)\n",
    "else:\n",
    "    done_langdateranges = {}\n",
    "\n",
    "errors = []\n",
    "maxgroupsize = 10000\n",
    "for lang, da_dict in raw_date_article_dict.items():\n",
    "\n",
    "    if lang not in done_langdateranges:\n",
    "        done_langdateranges[lang] = set()\n",
    "    try:\n",
    "        l_pvdf = [pd.read_hdf('data/pageviews_raw.h5', key=f'/{lang}')]\n",
    "        print(lang, 'successful read')\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        l_pvdf = []\n",
    "\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    \n",
    "    for n, (daterange, articles) in enumerate(da_dict.items()):\n",
    "        if n % 100 == 0:\n",
    "            print(lang, n/len(da_dict), end='\\r')\n",
    "        if daterange in done_langdateranges[lang]:\n",
    "            continue\n",
    "        grouppvs = []\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                try:\n",
    "                    pageviews = await wt.api_article_views(wtsession, f'{lang}.wikipedia', articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                            start=daterange[0].strftime('%Y%m%d'), end=(daterange[1] - pd.Timedelta(days=1)).strftime('%Y%m%d'),\n",
    "                            agent='user', redirects=False, process=False)\n",
    "                except Exception as e:\n",
    "                    if e.args[0][:44] == 'The pageview API returned nothing useful at:':\n",
    "                        pageviews = {d.to_pydatetime(): {x: 0 for x in articles}\n",
    "                                for d in pd.date_range(start=daterange[0], end=daterange[1] - pd.Timedelta(days=1))}\n",
    "                    else:\n",
    "                        raise e\n",
    "                    \n",
    "                pvdf = pd.DataFrame(pageviews).T.reset_index().rename(columns={'index': 'date'})\n",
    "                pvdf = pvdf.melt(id_vars='date', var_name='title', value_name='pageviews')\n",
    "                pvdf['lang'] = lang\n",
    "                pvdf = pvdf[['lang', 'title', 'date', 'pageviews']]\n",
    "                grouppvs.append(pvdf)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "\n",
    "            except (ValueError, TypeError, MaxRetryError, IndexError) as e:\n",
    "                print(n/len(da_dict), 'Reducing group size to', groupsize // 2, end='\\r')\n",
    "                errors.append(e)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "\n",
    "        gpvdf = pd.concat(grouppvs, ignore_index=True)\n",
    "        l_pvdf.append(gpvdf)\n",
    "        done_langdateranges[lang].add(daterange)\n",
    "\n",
    "        if (len(l_pvdf) > 0) & (n % 5000 == 0) & (n > 0):\n",
    "            print('writing intermediate')\n",
    "            l_pvdfw = pd.concat(l_pvdf, ignore_index=True)\n",
    "            l_pvdfw.to_hdf('data/pageviews_raw.h5', key=f'/{lang}', mode='a')\n",
    "            with open('data/done_langdateranges_raw_pv.pkl', 'wb') as f:\n",
    "                pickle.dump(done_langdateranges, f)\n",
    "\n",
    "    if len(l_pvdf) > 0:\n",
    "        l_pvdf = pd.concat(l_pvdf, ignore_index=True)\n",
    "        l_pvdf.to_hdf('data/pageviews_raw.h5', key=f'/{lang}', mode='a')\n",
    "        with open('data/done_langdateranges_raw_pv.pkl', 'wb') as f:\n",
    "            pickle.dump(done_langdateranges, f)  \n",
    "                  \n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/done_langdateranges_pv.pkl'):\n",
    "    with open('data/done_langdateranges_pv.pkl', 'rb') as f:\n",
    "        done_langdateranges = pickle.load(f)\n",
    "else:\n",
    "    done_langdateranges = {}\n",
    "\n",
    "errors = []\n",
    "maxgroupsize = 10000\n",
    "for lang, da_dict in date_article_dict.items():\n",
    "\n",
    "    if lang not in done_langdateranges:\n",
    "        done_langdateranges[lang] = set()\n",
    "    try:\n",
    "        l_pvdf = [pd.read_hdf('data/pageviews.h5', key=f'/{lang}')]\n",
    "        print(lang, 'successful read')\n",
    "    except (KeyError, FileNotFoundError):\n",
    "        l_pvdf = []\n",
    "\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    \n",
    "    for n, (daterange, articles) in enumerate(da_dict.items()):\n",
    "        if n % 100 == 0:\n",
    "            print(lang, n/len(da_dict), end='\\r')\n",
    "        if daterange in done_langdateranges[lang]:\n",
    "            continue\n",
    "        grouppvs = []\n",
    "        groupsize = maxgroupsize\n",
    "        while len(articles) > 0:\n",
    "            try:\n",
    "                try:\n",
    "                    pageviews = await wt.api_article_views(wtsession, f'{lang}.wikipedia', articles[:groupsize], pagemaps=pagemapsdict[lang],\n",
    "                            start=daterange[0].strftime('%Y%m%d'), end=(daterange[1] - pd.Timedelta(days=1)).strftime('%Y%m%d'),\n",
    "                            agent='user', redirects=False)\n",
    "                except Exception as e:\n",
    "                    if e.args[0][:44] == 'The pageview API returned nothing useful at:':\n",
    "                        pageviews = {d.to_pydatetime(): {x: 0 for x in articles}\n",
    "                                for d in pd.date_range(start=daterange[0], end=daterange[1] - pd.Timedelta(days=1))}\n",
    "                    else:\n",
    "                        raise e\n",
    "                    \n",
    "                pvdf = pd.DataFrame(pageviews).T.reset_index().rename(columns={'index': 'date'})\n",
    "                pvdf = pvdf.melt(id_vars='date', var_name='title', value_name='pageviews')\n",
    "                pvdf['lang'] = lang\n",
    "                pvdf = pvdf[['lang', 'title', 'date', 'pageviews']]\n",
    "                grouppvs.append(pvdf)\n",
    "                articles = articles[groupsize:]\n",
    "                groupsize = min(int(round(groupsize * (2**0.6), 0)), maxgroupsize)\n",
    "\n",
    "            except (ValueError, TypeError, MaxRetryError, IndexError) as e:\n",
    "                print(n/len(da_dict), 'Reducing group size to', groupsize // 2, end='\\r')\n",
    "                errors.append(e)\n",
    "                time.sleep(0.1)\n",
    "                groupsize = groupsize // 2\n",
    "\n",
    "        gpvdf = pd.concat(grouppvs, ignore_index=True)\n",
    "        l_pvdf.append(gpvdf)\n",
    "        done_langdateranges[lang].add(daterange)\n",
    "\n",
    "        if (len(l_pvdf) > 0) & (n % 5000 == 0) & (n > 0):\n",
    "            print('writing intermediate')\n",
    "            l_pvdfw = pd.concat(l_pvdf, ignore_index=True)\n",
    "            l_pvdfw.to_hdf('data/pageviews.h5', key=f'/{lang}', mode='a')\n",
    "            with open('data/done_langdateranges_pv.pkl', 'wb') as f:\n",
    "                pickle.dump(done_langdateranges, f)\n",
    "\n",
    "    if len(l_pvdf) > 0:\n",
    "        l_pvdf = pd.concat(l_pvdf, ignore_index=True)\n",
    "        l_pvdf.to_hdf('data/pageviews.h5', key=f'/{lang}', mode='a')\n",
    "        with open('data/done_langdateranges_pv.pkl', 'wb') as f:\n",
    "            pickle.dump(done_langdateranges, f)  \n",
    "                  \n",
    "    await wtsession.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "ab\n",
      "ace\n",
      "ady\n",
      "af\n",
      "als\n",
      "am\n",
      "an\n",
      "ang\n",
      "ar\n",
      "arc\n",
      "ary\n",
      "arz\n",
      "as\n",
      "ast\n",
      "atj\n",
      "avk\n",
      "ay\n",
      "az\n",
      "azb\n",
      "ba\n",
      "ban\n",
      "bar\n",
      "bat-smg\n",
      "bcl\n",
      "be\n",
      "be-tarask\n",
      "bg\n",
      "bh\n",
      "bi\n",
      "bjn\n",
      "bn\n",
      "bo\n",
      "bpy\n",
      "br\n",
      "bs\n",
      "bug\n",
      "bxr\n",
      "ca\n",
      "cbk-zam\n",
      "cdo\n",
      "ce\n",
      "ceb\n",
      "chr\n",
      "chy\n",
      "ckb\n",
      "co\n",
      "cr\n",
      "crh\n",
      "cs\n",
      "csb\n",
      "cu\n",
      "cv\n",
      "cy\n",
      "da\n",
      "de\n",
      "diq\n",
      "donate\n",
      "dsb\n",
      "dv\n",
      "el\n",
      "eml\n",
      "en\n",
      "eo\n",
      "es\n",
      "et\n",
      "eu\n",
      "ext\n",
      "fa\n",
      "ff\n",
      "fi\n",
      "fiu-vro\n",
      "fo\n",
      "fr\n",
      "frp\n",
      "frr\n",
      "fur\n",
      "fy\n",
      "ga\n",
      "gag\n",
      "gan\n",
      "gcr\n",
      "gd\n",
      "gl\n",
      "glk\n",
      "gn\n",
      "gom\n",
      "got\n",
      "gu\n",
      "gur\n",
      "gv\n",
      "hak\n",
      "haw\n",
      "he\n",
      "hi\n",
      "hif\n",
      "hr\n",
      "hsb\n",
      "ht\n",
      "hu\n",
      "hy\n",
      "hyw\n",
      "ia\n",
      "id\n",
      "ie\n",
      "io\n",
      "is\n",
      "it\n",
      "ja\n",
      "jam\n",
      "jbo\n",
      "jv\n",
      "ka\n",
      "kaa\n",
      "kab\n",
      "kk\n",
      "kl\n",
      "km\n",
      "kn\n",
      "ko\n",
      "koi\n",
      "krc\n",
      "ks\n",
      "ksh\n",
      "ku\n",
      "kv\n",
      "kw\n",
      "ky\n",
      "la\n",
      "lad\n",
      "lb\n",
      "lbe\n",
      "lez\n",
      "lfn\n",
      "li\n",
      "lij\n",
      "lld\n",
      "lmo\n",
      "lo\n",
      "lt\n",
      "ltg\n",
      "lv\n",
      "mai\n",
      "mdf\n",
      "mg\n",
      "mhr\n",
      "mi\n",
      "min\n",
      "mk\n",
      "ml\n",
      "mn\n",
      "mni\n",
      "mr\n",
      "mrj\n",
      "ms\n",
      "mt\n",
      "mus\n",
      "mwl\n",
      "my\n",
      "myv\n",
      "nah\n",
      "nap\n",
      "nds\n",
      "nds-nl\n",
      "ne\n",
      "nl\n",
      "nn\n",
      "no\n",
      "nostalgia\n",
      "nov\n",
      "nqo\n",
      "nrm\n",
      "nv\n",
      "oc\n",
      "olo\n",
      "om\n",
      "or\n",
      "os\n",
      "pa\n",
      "pam\n",
      "pap\n",
      "pcd\n",
      "pcm\n",
      "pdc\n",
      "pfl\n",
      "pi\n",
      "pih\n",
      "pl\n",
      "pms\n",
      "pnb\n",
      "pnt\n",
      "ps\n",
      "pt\n",
      "qu\n",
      "rm\n",
      "rmy\n",
      "ro\n",
      "roa-rup\n",
      "roa-tara\n",
      "ru\n",
      "rue\n",
      "rw\n",
      "sa\n",
      "sah\n",
      "sat\n",
      "sc\n",
      "scn\n",
      "sco\n",
      "sd\n",
      "se\n",
      "sh\n",
      "shi\n",
      "si\n",
      "simple\n",
      "sk\n",
      "skr\n",
      "sl\n",
      "sm\n",
      "smn\n",
      "sn\n",
      "so\n",
      "sq\n",
      "sr\n",
      "ss\n",
      "stq\n",
      "su\n",
      "sv\n",
      "sw\n",
      "szl\n",
      "szy\n",
      "ta\n",
      "tay\n",
      "te\n",
      "test\n",
      "tg\n",
      "th\n",
      "thankyou\n",
      "tk\n",
      "tl\n",
      "tn\n",
      "to\n",
      "tpi\n",
      "tr\n",
      "trv\n",
      "tt\n",
      "ty\n",
      "tyv\n",
      "udm\n",
      "ug\n",
      "uk\n",
      "ur\n",
      "uz\n",
      "vec\n",
      "vep\n",
      "vi\n",
      "vls\n",
      "vo\n",
      "wa\n",
      "war\n",
      "wo\n",
      "wuu\n",
      "xh\n",
      "xmf\n",
      "yi\n",
      "yo\n",
      "za\n",
      "zea\n",
      "zh\n",
      "zh-classical\n",
      "zh-min-nan\n",
      "zh-yue\n",
      "zu\n"
     ]
    }
   ],
   "source": [
    "# save to sql db\n",
    "import time\n",
    "\n",
    "def save_to_sql_with_retry(df, conn, table_name, retries=5, delay=1):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "            break\n",
    "        except sqlite3.OperationalError as e:\n",
    "            if 'database is locked' in str(e):\n",
    "                print(f\"Database is locked, retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "# erase any existing pageviews table\n",
    "\n",
    "conn.execute('DROP TABLE IF EXISTS pageviews;')\n",
    "\n",
    "# write pageviews to sql\n",
    "\n",
    "for lang in date_article_dict:\n",
    "    print(lang)\n",
    "    pvdf = pd.read_hdf('data/pageviews.h5', key=f'/{lang}')\n",
    "    try:\n",
    "        pvrdf = pd.read_hdf('data/pageviews_raw.h5', key=f'/{lang}')\n",
    "        final_pvdf = pd.concat([pvdf, pvrdf], ignore_index=True).drop_duplicates(subset=['lang', 'title', 'date'])\n",
    "    except KeyError:\n",
    "        final_pvdf = pvdf.drop_duplicates(subset=['lang', 'title', 'date'])\n",
    "\n",
    "    save_to_sql_with_retry(final_pvdf, conn, 'pageviews')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database size: 13051.60 MB\n"
     ]
    }
   ],
   "source": [
    "# get storage size of database\n",
    "import os\n",
    "mb_size = os.path.getsize('wikireddit.db') / 1024 / 1024\n",
    "print(f\"Database size: {mb_size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
