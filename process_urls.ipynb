{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import unquote\n",
    "import wikitoolkit as wt\n",
    "import string\n",
    "import pickle\n",
    "import sqlite3\n",
    "\n",
    "my_agent = 'wikireddit <p.gildersleve@exeter.ac.uk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create full link tables\n",
    "\n",
    "bodylinks = pd.read_hdf('data/bodylinks.h5', 'df')\n",
    "titlelinks = pd.read_hdf('data/titlelinks.h5', 'df')\n",
    "bodylinks['in_title'] = False\n",
    "titlelinks['in_title'] = True\n",
    "commentlinks = pd.read_hdf('data/commentlinks.h5', 'df')\n",
    "\n",
    "posts = pd.read_hdf('data/posts.h5', 'df')\n",
    "comments = pd.concat([pd.read_hdf(f'data/comments_{x}.h5') for x in range(1,5)]).reset_index(drop=True)\n",
    "\n",
    "postlinks = pd.concat([bodylinks, titlelinks], ignore_index=True)\n",
    "postlinks = posts.merge(postlinks, on='id', how='left').dropna(subset=['final_url'])\n",
    "commentlinks = comments.merge(commentlinks, on='id', how='left').dropna(subset=['final_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>last_modified_at</th>\n",
       "      <th>body</th>\n",
       "      <th>author_id</th>\n",
       "      <th>gilded</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>processed_url_3</th>\n",
       "      <th>end_processed_valid</th>\n",
       "      <th>end_processed_url</th>\n",
       "      <th>valid_rd</th>\n",
       "      <th>status_rd</th>\n",
       "      <th>redirected_url</th>\n",
       "      <th>final_valid</th>\n",
       "      <th>final_status</th>\n",
       "      <th>final_url</th>\n",
       "      <th>created_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t1_fffb0wn</td>\n",
       "      <td>t3_etay7w</td>\n",
       "      <td>t1_fff9hw3</td>\n",
       "      <td>2020-01-24 15:33:19.870</td>\n",
       "      <td>2020-01-26 13:20:38.671925</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Intelligence_q...</td>\n",
       "      <td>d2e6550007d314004ed01148c5f12c450fa7969b42466d...</td>\n",
       "      <td>False</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Intelligence_q...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Intelligence_q...</td>\n",
       "      <td>2020-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t1_ffy2irk</td>\n",
       "      <td>t3_evtkjd</td>\n",
       "      <td>t1_ffxzb0u</td>\n",
       "      <td>2020-01-29 22:35:26.247</td>\n",
       "      <td>2020-01-30 18:39:07.158948</td>\n",
       "      <td>\\[serious\\]\\n\\nThe [Lockheed D-21 Supersonic R...</td>\n",
       "      <td>410d99f20e4f77b865c34f36d55b7f684abeef964927b6...</td>\n",
       "      <td>False</td>\n",
       "      <td>69</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lockheed_D-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lockheed_D-21</td>\n",
       "      <td>2020-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t1_fcx0nes</td>\n",
       "      <td>t3_ejacx2</td>\n",
       "      <td>t1_fcwy5m3</td>\n",
       "      <td>2020-01-03 10:05:22.663</td>\n",
       "      <td>2020-01-04 17:11:59.126507</td>\n",
       "      <td>Nel 2002 gli USA condussero un'esercitazione m...</td>\n",
       "      <td>12bf92ea903745a1ec2091399501de5280135141a81a0e...</td>\n",
       "      <td>False</td>\n",
       "      <td>31</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Millennium_Cha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Millennium_Cha...</td>\n",
       "      <td>2020-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t1_fcs6j6m</td>\n",
       "      <td>t3_eijy3m</td>\n",
       "      <td>t1_fcruhz4</td>\n",
       "      <td>2020-01-01 20:55:31.722</td>\n",
       "      <td>2020-05-01 00:59:11.366739</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Richard_Ayoade\\n...</td>\n",
       "      <td>b9d218408473cd1d94a93a15bce8d1042108a81e0ef2f9...</td>\n",
       "      <td>False</td>\n",
       "      <td>88</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Richard_Ayoade</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Richard_Ayoade</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t1_fd2xgkl</td>\n",
       "      <td>t3_ejubhh</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-01-04 15:44:27.762</td>\n",
       "      <td>2020-05-12 23:57:33.294889</td>\n",
       "      <td>There is actually something called \"Paris Synd...</td>\n",
       "      <td>7cc6e6e4030d944303d6cc187427a8ac723eecf07ba17a...</td>\n",
       "      <td>False</td>\n",
       "      <td>1112</td>\n",
       "      <td>0.980969</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Paris_syndrome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Paris_syndrome</td>\n",
       "      <td>2020-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12372841</th>\n",
       "      <td>t1_ke76826</td>\n",
       "      <td>t3_18mvu96</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-12-20 16:43:03.018</td>\n",
       "      <td>2023-12-20 19:38:30.945054</td>\n",
       "      <td>Folks really don't know [Surplus Value](https:...</td>\n",
       "      <td>580d4c4c8711f0d818d0e860ebaa45deb799e36b22cc4e...</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Surplus_value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Surplus_value</td>\n",
       "      <td>2023-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12372843</th>\n",
       "      <td>t1_kfiaxie</td>\n",
       "      <td>t3_18tt17t</td>\n",
       "      <td>t1_kfhqh0p</td>\n",
       "      <td>2023-12-30 03:24:54.664</td>\n",
       "      <td>2023-12-30 22:31:17.607799</td>\n",
       "      <td>K, then give them 56% of your country. That's ...</td>\n",
       "      <td>b0e78294f1f9963d57277916e223ce487dd48025cf6b05...</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Romani_Holocaust</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Romani_Holocaust</td>\n",
       "      <td>2023-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12372844</th>\n",
       "      <td>t1_kehafjk</td>\n",
       "      <td>t3_18o44v0</td>\n",
       "      <td>t1_keg5h57</td>\n",
       "      <td>2023-12-22 16:20:19.633</td>\n",
       "      <td>2023-12-23 15:42:32.254796</td>\n",
       "      <td>US Bombed civilian targets in Bangkok during W...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Bombing_of_Ban...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Bombing_of_Ban...</td>\n",
       "      <td>2023-12-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12372845</th>\n",
       "      <td>t1_kecuqt1</td>\n",
       "      <td>t3_18nh7z6</td>\n",
       "      <td>t1_kecrk7a</td>\n",
       "      <td>2023-12-21 18:35:42.405</td>\n",
       "      <td>2023-12-21 23:52:26.808479</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Historical_Jesus</td>\n",
       "      <td>92f38a9e1f2ff20cc4161e66592edf3cfc208425338003...</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Historical_Jesus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Historical_Jesus</td>\n",
       "      <td>2023-12-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12372847</th>\n",
       "      <td>t1_kfhfnnz</td>\n",
       "      <td>t3_18tshhd</td>\n",
       "      <td>t1_kfh2m8q</td>\n",
       "      <td>2023-12-29 23:45:50.663</td>\n",
       "      <td>2023-12-30 12:02:24.360235</td>\n",
       "      <td>I thought they were developing rolling landing...</td>\n",
       "      <td>72f63ba184fbca371e7756732a893c4c9838722c4ecdc4...</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Shipborne_roll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>200.0</td>\n",
       "      <td>https://en.m.wikipedia.org/wiki/Shipborne_roll...</td>\n",
       "      <td>2023-12-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093372 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     post_id   parent_id              created_at  \\\n",
       "0         t1_fffb0wn   t3_etay7w  t1_fff9hw3 2020-01-24 15:33:19.870   \n",
       "2         t1_ffy2irk   t3_evtkjd  t1_ffxzb0u 2020-01-29 22:35:26.247   \n",
       "3         t1_fcx0nes   t3_ejacx2  t1_fcwy5m3 2020-01-03 10:05:22.663   \n",
       "4         t1_fcs6j6m   t3_eijy3m  t1_fcruhz4 2020-01-01 20:55:31.722   \n",
       "5         t1_fd2xgkl   t3_ejubhh        None 2020-01-04 15:44:27.762   \n",
       "...              ...         ...         ...                     ...   \n",
       "12372841  t1_ke76826  t3_18mvu96        None 2023-12-20 16:43:03.018   \n",
       "12372843  t1_kfiaxie  t3_18tt17t  t1_kfhqh0p 2023-12-30 03:24:54.664   \n",
       "12372844  t1_kehafjk  t3_18o44v0  t1_keg5h57 2023-12-22 16:20:19.633   \n",
       "12372845  t1_kecuqt1  t3_18nh7z6  t1_kecrk7a 2023-12-21 18:35:42.405   \n",
       "12372847  t1_kfhfnnz  t3_18tshhd  t1_kfh2m8q 2023-12-29 23:45:50.663   \n",
       "\n",
       "                   last_modified_at  \\\n",
       "0        2020-01-26 13:20:38.671925   \n",
       "2        2020-01-30 18:39:07.158948   \n",
       "3        2020-01-04 17:11:59.126507   \n",
       "4        2020-05-01 00:59:11.366739   \n",
       "5        2020-05-12 23:57:33.294889   \n",
       "...                             ...   \n",
       "12372841 2023-12-20 19:38:30.945054   \n",
       "12372843 2023-12-30 22:31:17.607799   \n",
       "12372844 2023-12-23 15:42:32.254796   \n",
       "12372845 2023-12-21 23:52:26.808479   \n",
       "12372847 2023-12-30 12:02:24.360235   \n",
       "\n",
       "                                                       body  \\\n",
       "0         https://en.m.wikipedia.org/wiki/Intelligence_q...   \n",
       "2         \\[serious\\]\\n\\nThe [Lockheed D-21 Supersonic R...   \n",
       "3         Nel 2002 gli USA condussero un'esercitazione m...   \n",
       "4         https://en.wikipedia.org/wiki/Richard_Ayoade\\n...   \n",
       "5         There is actually something called \"Paris Synd...   \n",
       "...                                                     ...   \n",
       "12372841  Folks really don't know [Surplus Value](https:...   \n",
       "12372843  K, then give them 56% of your country. That's ...   \n",
       "12372844  US Bombed civilian targets in Bangkok during W...   \n",
       "12372845   https://en.m.wikipedia.org/wiki/Historical_Jesus   \n",
       "12372847  I thought they were developing rolling landing...   \n",
       "\n",
       "                                                  author_id  gilded  score  \\\n",
       "0         d2e6550007d314004ed01148c5f12c450fa7969b42466d...   False     -3   \n",
       "2         410d99f20e4f77b865c34f36d55b7f684abeef964927b6...   False     69   \n",
       "3         12bf92ea903745a1ec2091399501de5280135141a81a0e...   False     31   \n",
       "4         b9d218408473cd1d94a93a15bce8d1042108a81e0ef2f9...   False     88   \n",
       "5         7cc6e6e4030d944303d6cc187427a8ac723eecf07ba17a...   False   1112   \n",
       "...                                                     ...     ...    ...   \n",
       "12372841  580d4c4c8711f0d818d0e860ebaa45deb799e36b22cc4e...   False     -1   \n",
       "12372843  b0e78294f1f9963d57277916e223ce487dd48025cf6b05...   False     -1   \n",
       "12372844                                               None   False     -1   \n",
       "12372845  92f38a9e1f2ff20cc4161e66592edf3cfc208425338003...   False     -1   \n",
       "12372847  72f63ba184fbca371e7756732a893c4c9838722c4ecdc4...   False     -1   \n",
       "\n",
       "          upvote_ratio  ... processed_url_3 end_processed_valid  \\\n",
       "0             0.363636  ...             NaN                True   \n",
       "2             1.000000  ...             NaN                True   \n",
       "3             0.969697  ...             NaN                True   \n",
       "4             0.988889  ...             NaN                True   \n",
       "5             0.980969  ...             NaN                True   \n",
       "...                ...  ...             ...                 ...   \n",
       "12372841      0.333333  ...            None                True   \n",
       "12372843      0.333333  ...            None                True   \n",
       "12372844      0.444444  ...            None                True   \n",
       "12372845      0.333333  ...            None                True   \n",
       "12372847      0.333333  ...            None                True   \n",
       "\n",
       "                                          end_processed_url valid_rd  \\\n",
       "0         https://en.m.wikipedia.org/wiki/Intelligence_q...      NaN   \n",
       "2               https://en.wikipedia.org/wiki/Lockheed_D-21      NaN   \n",
       "3         https://en.m.wikipedia.org/wiki/Millennium_Cha...      NaN   \n",
       "4              https://en.wikipedia.org/wiki/Richard_Ayoade      NaN   \n",
       "5            https://en.m.wikipedia.org/wiki/Paris_syndrome      NaN   \n",
       "...                                                     ...      ...   \n",
       "12372841        https://en.wikipedia.org/wiki/Surplus_value      NaN   \n",
       "12372843     https://en.wikipedia.org/wiki/Romani_Holocaust      NaN   \n",
       "12372844  https://en.m.wikipedia.org/wiki/Bombing_of_Ban...      NaN   \n",
       "12372845   https://en.m.wikipedia.org/wiki/Historical_Jesus      NaN   \n",
       "12372847  https://en.m.wikipedia.org/wiki/Shipborne_roll...      NaN   \n",
       "\n",
       "          status_rd redirected_url final_valid final_status  \\\n",
       "0               NaN            NaN        True        200.0   \n",
       "2               NaN            NaN        True        200.0   \n",
       "3               NaN            NaN        True        200.0   \n",
       "4               NaN            NaN        True        200.0   \n",
       "5               NaN            NaN        True        200.0   \n",
       "...             ...            ...         ...          ...   \n",
       "12372841        NaN            NaN        True        200.0   \n",
       "12372843        NaN            NaN        True        200.0   \n",
       "12372844        NaN            NaN        True        200.0   \n",
       "12372845        NaN            NaN        True        200.0   \n",
       "12372847        NaN            NaN        True        200.0   \n",
       "\n",
       "                                                  final_url created_date  \n",
       "0         https://en.m.wikipedia.org/wiki/Intelligence_q...   2020-01-24  \n",
       "2               https://en.wikipedia.org/wiki/Lockheed_D-21   2020-01-29  \n",
       "3         https://en.m.wikipedia.org/wiki/Millennium_Cha...   2020-01-03  \n",
       "4              https://en.wikipedia.org/wiki/Richard_Ayoade   2020-01-01  \n",
       "5            https://en.m.wikipedia.org/wiki/Paris_syndrome   2020-01-04  \n",
       "...                                                     ...          ...  \n",
       "12372841        https://en.wikipedia.org/wiki/Surplus_value   2023-12-20  \n",
       "12372843     https://en.wikipedia.org/wiki/Romani_Holocaust   2023-12-30  \n",
       "12372844  https://en.m.wikipedia.org/wiki/Bombing_of_Ban...   2023-12-22  \n",
       "12372845   https://en.m.wikipedia.org/wiki/Historical_Jesus   2023-12-21  \n",
       "12372847  https://en.m.wikipedia.org/wiki/Shipborne_roll...   2023-12-29  \n",
       "\n",
       "[9093372 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commentlinks['created_date'] = pd.to_datetime(commentlinks['created_at']).dt.date\n",
    "commentlinks['final_url'] = commentlinks['final_url'].apply(unquote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "postlinks['created_date'] = pd.to_datetime(postlinks['created_at']).dt.date\n",
    "postlinks['final_url'] = postlinks['final_url'].apply(unquote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "po = postlinks[['id', 'end_processed_valid', 'end_processed_url', 'final_valid', 'final_status', 'final_url',\n",
    "       'in_title']].copy()\n",
    "po['redirected'] = po['final_url'] != po['end_processed_url']\n",
    "po['final_status'] = po['final_status'].astype(int)\n",
    "\n",
    "co = commentlinks[['id', 'end_processed_valid', 'end_processed_url', 'final_valid', 'final_status', 'final_url']].copy()\n",
    "co['redirected'] = co['final_url'] != co['end_processed_url']\n",
    "co['final_status'] = co['final_status'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11573367"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('wikireddit.db')\n",
    "\n",
    "po.to_sql('postlinks', conn, if_exists='replace', index=False)\n",
    "co.to_sql('commentlinks', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11573367"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import unquote\n",
    "import wikitoolkit as wt\n",
    "import string\n",
    "import pickle\n",
    "import sqlite3\n",
    "\n",
    "# conn = sqlite3.connect('wikireddit_final_notopics.db')\n",
    "\n",
    "# po = pd.read_sql(\"SELECT * FROM postlinks\", con=conn)\n",
    "# co = pd.read_sql(\"SELECT * FROM commentlinks\", con=conn)\n",
    "\n",
    "po['final_url'] = po['final_url'].apply(unquote)\n",
    "co['final_url'] = co['final_url'].apply(unquote)\n",
    "\n",
    "po.to_sql('postlinks', conn, if_exists='replace', index=False)\n",
    "co.to_sql('commentlinks', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587395\n",
      "9972324\n"
     ]
    }
   ],
   "source": [
    "# put all unique (date, links) in one table\n",
    "\n",
    "postlinks['created_date'] = postlinks['created_at'].dt.floor('D')\n",
    "postlinks['updated_date'] = postlinks['updated_at'].dt.floor('D')\n",
    "postlinks = postlinks[postlinks['final_valid']]\n",
    "postlinks_unique = postlinks[['final_url', 'created_date', 'updated_date']].drop_duplicates().copy()\n",
    "print(len(postlinks_unique))\n",
    "\n",
    "commentlinks['created_date'] = commentlinks['created_at'].dt.floor('D')\n",
    "commentlinks['updated_date'] = commentlinks['last_modified_at'].dt.floor('D')\n",
    "commentlinks = commentlinks[commentlinks['final_valid']]\n",
    "commentlinks_unique = commentlinks[['final_url', 'created_date', 'updated_date']].drop_duplicates().copy()\n",
    "print(len(commentlinks_unique))\n",
    "\n",
    "all_links = pd.concat([postlinks_unique, commentlinks_unique], ignore_index=True).drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to get article titles from urls\n",
    "\n",
    "def url_parse(url):\n",
    "    if '?' in url:\n",
    "        query = url.split('?')[1]\n",
    "        query_dict = dict(q.split('=') for q in query.split('&') if len(q.split('=')) == 2)\n",
    "        # print(query_dict)\n",
    "        return query_dict\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "async def resolve_ids(links_df):\n",
    "\n",
    "    missing = links_df[links_df['raw_title'].isna()][['lang', 'final_url']]\n",
    "    for l in missing['lang'].unique():\n",
    "        urls = missing[missing['lang'] == l]['final_url'].unique()\n",
    "        urldicts = {u: url_parse(u) for u in urls}\n",
    "        # print(urldicts)\n",
    "        titlemaps = {}\n",
    "        revmaps = {}\n",
    "        pageidmaps = {}\n",
    "        for u in urls:\n",
    "            if urldicts[u] is not None:\n",
    "                if 'title' in urldicts[u]:\n",
    "                    titlemaps[u] = urldicts[u]['title'].replace('+', ' ')\n",
    "                elif 'curid' in urldicts[u]:\n",
    "                    pageidmaps[u] = unquote(urldicts[u]['curid']).strip(string.punctuation+string.whitespace)\n",
    "                elif ('oldid' in urldicts[u])&(urldicts[u].get('oldid', '') != 'prev'):\n",
    "                    revmaps[u] = unquote(urldicts[u]['oldid']).strip(string.punctuation+string.whitespace)\n",
    "                elif 'diff' in urldicts[u]:\n",
    "                    revmaps[u] = unquote(urldicts[u]['diff']).strip(string.punctuation+string.whitespace)\n",
    "            # print(urldicts[u])\n",
    "\n",
    "        wtsession = wt.WTSession(f'{l}.wikipedia', user_agent=my_agent)\n",
    "        pagemaps = wt.PageMaps()\n",
    "        # print(revmaps)\n",
    "        if revmaps:\n",
    "            # print(revmaps)\n",
    "            revinfo = await wt.basic_info(wtsession, revids=list(revmaps.values()), pagemaps=pagemaps, params={'prop': 'revisions', 'rvprop': 'ids'})\n",
    "        else:\n",
    "            revinfo = []\n",
    "        if pageidmaps:\n",
    "            pageidinfo = await wt.basic_info(wtsession, pageids=list(pageidmaps.values()), pagemaps=pagemaps, params={'prop': 'revisions', 'rvprop': 'ids'})\n",
    "        else:\n",
    "            pageidinfo = []\n",
    "        await wtsession.close()\n",
    "        \n",
    "        # print(pageidinfo)\n",
    "\n",
    "        revtitledict = [{z['revid']: y['title'] for z in y['revisions']} for x in revinfo for y in x]\n",
    "        # combine into single dict\n",
    "        revtitledict = {k: v for d in revtitledict for k, v in d.items()}\n",
    "        pageidtitledict = {y['pageid']: y['title'] for x in pageidinfo for y in x}\n",
    "        # combine into single dict\n",
    "        # print(pagetitledict)\n",
    "        revmaps = {k: revtitledict.get(int(v), None) for k, v in revmaps.items()}\n",
    "        pageidmaps = {k: pageidtitledict[int(v)] for k, v in pageidmaps.items()}\n",
    "\n",
    "        titledict = {**titlemaps, **revmaps, **pageidmaps}\n",
    "\n",
    "        urltitledict = {u: titledict.get(u, None) for u in urls}\n",
    "\n",
    "        links_df.loc[(links_df['lang'] == l) & (links_df['raw_title'].isna()), 'raw_title'\n",
    "                     ] = links_df.loc[(links_df['lang'] == l) & (links_df['raw_title'].isna()), 'final_url'].map(urltitledict)\n",
    "    \n",
    "    return links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77c3c7751af0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf6b475220>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf6b250200>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf6b251370>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc9040>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc9b20>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fca0f0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc9460>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf6b8a1640>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8c80>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc9460>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8c80>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc9910>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf6b8a1250>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8410>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc91c0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf6b8a3890>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc9850>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8bc0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc9ca0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf6b8a29f0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8a70>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8b90>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc9430>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8530>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8d10>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8fe0>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8110>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8b30>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8890>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc9130>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77c3c77c1220>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf6b8a0710>\n",
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x77bf69fc8680>\n"
     ]
    }
   ],
   "source": [
    "# get article titles from urls\n",
    "\n",
    "links_df = all_links.copy()\n",
    "langvars = ['zh-hans', 'zh-tw', 'zh-hk', 'zh-cn', 'zh-hant', 'zh', 'sr-ec', 'sr-el', 'zh-sg', 'zh-my', 'zh-mo', 'sr'] # lang variants\n",
    "links_df.loc[:, 'lang_subdomain'] = links_df['final_url'].str.extract(r'https://([\\w\\.-]+)\\.wikipedia\\.org')[0]\n",
    "links_df.loc[:, 'lang'] = links_df['lang_subdomain'].str.split('.').str[0]\n",
    "links_df.loc[:, 'mobile'] = links_df['lang_subdomain'].str.split('.').str[1] == 'm'\n",
    "links_df['final_url'] = links_df['final_url'].apply(unquote)\n",
    "links_df.loc[:, 'raw_title'] = links_df['final_url'].str.extract(r'https://([\\w\\.-]+)\\.wikipedia\\.org/+wiki/+(.+)'\n",
    "                                )[1].str.split('?').str[0]\n",
    "links_df.loc[:, 'raw_title'] = links_df['raw_title'].fillna(\n",
    "    links_df['final_url'].str.extract(r'https://([\\w\\.-]+)\\.wikipedia\\.org/api/rest_v1/page/mobile-html/(.+)')[1].str.split('?').str[0])\n",
    "links_df.loc[:, 'raw_title'] = links_df['raw_title'].fillna(\n",
    "    links_df['final_url'].str.extract(r'https://([\\w\\.-]+)\\.wikipedia\\.org/+w/index\\.php\\?title=([^&]+)')[1].str.split('?').str[0])\n",
    "\n",
    "# raise\n",
    "for lv in langvars:\n",
    "    links_df.loc[:, 'raw_title'] = links_df['raw_title'].fillna(\n",
    "        links_df['final_url'].str.extract(r'https://([\\w\\.-]+)\\.wikipedia\\.org/+%s/+([^/]+)' %lv)[1].str.split('?').str[0])\n",
    "\n",
    "links_df = await resolve_ids(links_df)\n",
    "links_df['raw_title'] = links_df['raw_title'].str.replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_193434/3046090501.py:1: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  links_df.to_hdf('data/links_df.h5', 'df')\n",
      "/tmp/ipykernel_193434/3046090501.py:1: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['final_url', 'lang_subdomain', 'lang', 'raw_title'], dtype='object')]\n",
      "\n",
      "  links_df.to_hdf('data/links_df.h5', 'df')\n"
     ]
    }
   ],
   "source": [
    "links_df.to_hdf('data/links_df.h5', 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('wikireddit.db')\n",
    "linkmap = links_df[['final_url', 'lang', 'mobile', 'raw_title']].drop_duplicates()\n",
    "\n",
    "linkmap.to_sql('linkarticles', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "lna = links_df[links_df['raw_title'].isna()]\n",
    "vc = lna['final_url'].str.split('/').str[3].value_counts().index\n",
    "lna['final_url'].str.split('/').str[3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lna[lna['final_url'].str.split('/').str[3]=='?wiki']['final_url'].value_counts().index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.read_hdf('data/links_df.h5', 'df')\n",
    "\n",
    "# create long table of articles and dates\n",
    "articles_long = links_df.melt(id_vars=['lang', 'raw_title'], value_vars=['created_date', 'updated_date'], \n",
    "                                       var_name='date_type', value_name='date').dropna(subset=['date'])\n",
    "articles_long = articles_long.rename(columns={'date_type': 'is_updated_date'}).reset_index(drop=True)\n",
    "articles_long['is_updated_date'] = articles_long['is_updated_date'] == 'updated_date'\n",
    "articles_long = articles_long.copy()\n",
    "\n",
    "article_dates_unique = articles_long[['lang', 'raw_title', 'date']].drop_duplicates().reset_index(drop=True)\n",
    "# article_dates_unique.to_hdf('data/article_dates_unique.h5', 'df', mode='w')\n",
    "article_dates_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save link to article df to sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect ID and redirect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dates_unique = pd.read_hdf('data/article_dates_unique.h5', 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>raw_title</th>\n",
       "      <th>date</th>\n",
       "      <th>redirected_title</th>\n",
       "      <th>pageid</th>\n",
       "      <th>wikidata_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>Unit 731</td>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>Unit 731</td>\n",
       "      <td>214659</td>\n",
       "      <td>Q378835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>Shanghaiing</td>\n",
       "      <td>2020-09-06</td>\n",
       "      <td>Shanghaiing</td>\n",
       "      <td>686244</td>\n",
       "      <td>Q307329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>List of public corporations by market capitali...</td>\n",
       "      <td>2020-04-25</td>\n",
       "      <td>List of public corporations by market capitali...</td>\n",
       "      <td>14094649</td>\n",
       "      <td>Q3679380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>Space suit</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>Space suit</td>\n",
       "      <td>39375</td>\n",
       "      <td>Q223571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>Captain Ron</td>\n",
       "      <td>2020-04-27</td>\n",
       "      <td>Captain Ron</td>\n",
       "      <td>4179081</td>\n",
       "      <td>Q1035137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13123353</th>\n",
       "      <td>nl</td>\n",
       "      <td>Zaak-Arcopar</td>\n",
       "      <td>2023-12-23</td>\n",
       "      <td>Zaak-Arcopar</td>\n",
       "      <td>3195077</td>\n",
       "      <td>Q14524656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13123354</th>\n",
       "      <td>de</td>\n",
       "      <td>Audi Q4 e-tron</td>\n",
       "      <td>2023-12-14</td>\n",
       "      <td>Audi Q4 e-tron</td>\n",
       "      <td>11690456</td>\n",
       "      <td>Q62107422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13123355</th>\n",
       "      <td>de</td>\n",
       "      <td>Audi A4 B9</td>\n",
       "      <td>2023-12-14</td>\n",
       "      <td>Audi A4 B9</td>\n",
       "      <td>8924897</td>\n",
       "      <td>Q20760348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13123356</th>\n",
       "      <td>en</td>\n",
       "      <td>Bombing of Bangkok in World War II</td>\n",
       "      <td>2023-12-23</td>\n",
       "      <td>Bombing of Bangkok in World War II</td>\n",
       "      <td>33247384</td>\n",
       "      <td>Q4940687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13123357</th>\n",
       "      <td>en</td>\n",
       "      <td>Shipborne rolling vertical landing</td>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>Shipborne rolling vertical landing</td>\n",
       "      <td>36187065</td>\n",
       "      <td>Q7392870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13057758 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lang                                          raw_title       date  \\\n",
       "0          en                                           Unit 731 2020-01-29   \n",
       "1          en                                        Shanghaiing 2020-09-06   \n",
       "2          en  List of public corporations by market capitali... 2020-04-25   \n",
       "3          en                                         Space suit 2020-07-18   \n",
       "4          en                                        Captain Ron 2020-04-27   \n",
       "...       ...                                                ...        ...   \n",
       "13123353   nl                                       Zaak-Arcopar 2023-12-23   \n",
       "13123354   de                                     Audi Q4 e-tron 2023-12-14   \n",
       "13123355   de                                         Audi A4 B9 2023-12-14   \n",
       "13123356   en                 Bombing of Bangkok in World War II 2023-12-23   \n",
       "13123357   en                 Shipborne rolling vertical landing 2023-12-30   \n",
       "\n",
       "                                           redirected_title    pageid  \\\n",
       "0                                                  Unit 731    214659   \n",
       "1                                               Shanghaiing    686244   \n",
       "2         List of public corporations by market capitali...  14094649   \n",
       "3                                                Space suit     39375   \n",
       "4                                               Captain Ron   4179081   \n",
       "...                                                     ...       ...   \n",
       "13123353                                       Zaak-Arcopar   3195077   \n",
       "13123354                                     Audi Q4 e-tron  11690456   \n",
       "13123355                                         Audi A4 B9   8924897   \n",
       "13123356                 Bombing of Bangkok in World War II  33247384   \n",
       "13123357                 Shipborne rolling vertical landing  36187065   \n",
       "\n",
       "         wikidata_id  \n",
       "0            Q378835  \n",
       "1            Q307329  \n",
       "2           Q3679380  \n",
       "3            Q223571  \n",
       "4           Q1035137  \n",
       "...              ...  \n",
       "13123353   Q14524656  \n",
       "13123354   Q62107422  \n",
       "13123355   Q20760348  \n",
       "13123356    Q4940687  \n",
       "13123357    Q7392870  \n",
       "\n",
       "[13057758 rows x 6 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_dates_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagemapsdict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in article_dates_unique['lang'].unique():\n",
    "    print()\n",
    "    print(lang)\n",
    "    if lang not in pagemapsdict:\n",
    "        pagemapsdict[lang] = wt.PageMaps()\n",
    "    lang_articles = article_dates_unique[article_dates_unique['lang'] == lang]\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    groupsize = 1000\n",
    "    ua = list(lang_articles['raw_title'].unique())\n",
    "    groups = [ua[i:i+groupsize] for i in range(0, len(ua), groupsize)]\n",
    "\n",
    "    for n, g in enumerate(groups):\n",
    "        print(f'Fixing redirects {n+1}/{len(groups)}', end='\\r')\n",
    "        ingroupsize = groupsize\n",
    "        done = 0\n",
    "        while done < len(g):\n",
    "            try:\n",
    "                await pagemapsdict[lang].fix_redirects(wtsession, titles=g[done:done+ingroupsize])\n",
    "                done += ingroupsize\n",
    "            except ValueError:\n",
    "                ingroupsize = ingroupsize//2\n",
    "                \n",
    "                print(f'\\nError, reducing group size to {ingroupsize}')\n",
    "\n",
    "    for n, g in enumerate(groups):\n",
    "        print(f'Getting wikidata ids {n+1}/{len(groups)}', end='\\r')\n",
    "        ingroupsize = groupsize\n",
    "        done = 0\n",
    "        while done < len(g):\n",
    "            try:\n",
    "                await pagemapsdict[lang].get_wikidata_ids(wtsession, titles=g[done:done+ingroupsize])\n",
    "                done += ingroupsize\n",
    "            except ValueError:\n",
    "                ingroupsize = ingroupsize//2\n",
    "                print(f'\\nError, reducing group size to {ingroupsize}')\n",
    "\n",
    "    for n, g in enumerate(groups):\n",
    "        print(f'Getting redirects {n+1}/{len(groups)}', end='\\r')\n",
    "        ingroupsize = groupsize\n",
    "        done = 0\n",
    "        while done < len(g):\n",
    "            try:\n",
    "                await pagemapsdict[lang].get_redirects(wtsession, titles=g[done:done+ingroupsize])\n",
    "                done += ingroupsize\n",
    "            except ValueError:\n",
    "                ingroupsize = ingroupsize//2\n",
    "                print(f'\\nError, reducing group size to {ingroupsize}')\n",
    "\n",
    "    await wtsession.close()\n",
    "\n",
    "# save pagemapsdict\n",
    "with open('data/langpagemaps.pkl', 'wb') as f:\n",
    "    pickle.dump(pagemapsdict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert redirects table to save in db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/langpagemaps.pkl', 'rb') as f:\n",
    "    pagemapsdict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageidsdf = []\n",
    "for lang, pm in pagemapsdict.items():\n",
    "    df = pd.Series(pm.id_map).reset_index(name='pageid').rename(columns={'index': 'title'})\n",
    "    wikidata_ids = pd.Series(pm.wikidata_id_map).reset_index(name='wikidata_id').rename(columns={'index': 'title'})\n",
    "    df['lang'] = lang\n",
    "    df = df.merge(wikidata_ids, on='title', how='left')\n",
    "    pageidsdf.append(df[['lang', 'title', 'pageid', 'wikidata_id']])\n",
    "\n",
    "pageidsdf = pd.concat(pageidsdf, ignore_index=True)\n",
    "pageidsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_redirects = []\n",
    "for lang, pm in pagemapsdict.items():\n",
    "    norm_df = pd.Series(pm.norm_map).reset_index(name='norm_title').rename(columns={'index': 'raw_title'})\n",
    "    titles_redirect_df = pd.Series(pm.titles_redirect_map).reset_index(name='redirected_title').rename(columns={'index': 'norm_title'})\n",
    "\n",
    "    redirects_df = norm_df.merge(titles_redirect_df, on='norm_title', how='outer')\n",
    "    redirects_df['raw_title'] = redirects_df['raw_title'].fillna(redirects_df['norm_title'])\n",
    "    redirects_df['lang'] = lang\n",
    "\n",
    "    resolved_redirects.append(redirects_df[['lang', 'raw_title', 'norm_title', 'redirected_title']])\n",
    "resolved_redirects = pd.concat(resolved_redirects, ignore_index=True)\n",
    "resolved_redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_redirects = []\n",
    "for lang, pm in pagemapsdict.items():\n",
    "    df = pd.DataFrame([(k, y) for k, v in pm.collected_title_redirects.items() for y in v],\n",
    "                columns=['canonical_title', 'other_title'])\n",
    "    df['lang'] = lang\n",
    "    collected_redirects.append(df[['lang', 'canonical_title', 'other_title']])\n",
    "collected_redirects = pd.concat(collected_redirects, ignore_index=True)\n",
    "collected_redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pagemaps data to sql tables\n",
    "\n",
    "conn = sqlite3.connect('wikireddit.db')\n",
    "pageidsdf.to_sql('wiki_ids', conn, if_exists='replace', index=False)\n",
    "resolved_redirects.to_sql('resolved_redirects', conn, if_exists='replace', index=False)\n",
    "collected_redirects.to_sql('collected_redirects', conn, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get storage size of database\n",
    "import os\n",
    "mb_size = os.path.getsize('wikireddit.db') / 1024 / 1024\n",
    "print(f\"Database size: {mb_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the lang link date tables to get canonical titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in pagemapsdict:\n",
    "    print(lang)\n",
    "    article_dates_unique.loc[article_dates_unique['lang'] == lang, 'redirected_title'] = article_dates_unique['raw_title'].map(pagemapsdict[lang].norm_map).fillna(article_dates_unique['raw_title'])   \n",
    "    article_dates_unique.loc[article_dates_unique['lang'] == lang, 'redirected_title'] = (\n",
    "        article_dates_unique.loc[article_dates_unique['lang'] == lang, 'redirected_title']\n",
    "        .map(pagemapsdict[lang].titles_redirect_map)\n",
    "        .fillna(article_dates_unique.loc[article_dates_unique['lang'] == lang, 'redirected_title'])\n",
    "    )\n",
    "    article_dates_unique.loc[article_dates_unique['lang'] == lang, 'pageid'] = article_dates_unique['redirected_title'].map(pagemapsdict[lang].id_map)\n",
    "    article_dates_unique.loc[article_dates_unique['lang'] == lang, 'wikidata_id'] = article_dates_unique['redirected_title'].map(pagemapsdict[lang].wikidata_id_map)\n",
    "\n",
    "article_dates_unique = article_dates_unique.drop_duplicates(subset=['lang', 'date', 'redirected_title']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  bug fix - unicode parsing pecularity of api?\n",
    "\n",
    "article_dates_unique.loc[article_dates_unique['raw_title']=='Augustów roundup', 'redirected_title'] = 'Augustów roundup'\n",
    "article_dates_unique.loc[article_dates_unique['raw_title']=='Augustów roundup', 'pageid'] = 6002747\n",
    "article_dates_unique.loc[article_dates_unique['raw_title']=='Augustów roundup', 'wikidata_id'] = 'Q2612443'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "\n",
    "display(article_dates_unique[article_dates_unique['pageid'].isna()]['redirected_title'].value_counts())\n",
    "article_dates_unique[article_dates_unique['pageid']==-1]['redirected_title'].value_counts().head(50) # -1 is a placeholder for missing pageids - these are all invalid / special pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "article_dates_unique = article_dates_unique.dropna(subset=['pageid'])\n",
    "article_dates_unique = article_dates_unique[article_dates_unique['pageid'] != -1]\n",
    "article_dates_unique['pageid'] = article_dates_unique['pageid'].astype(int)\n",
    "\n",
    "article_dates_unique.to_hdf('data/article_dates_unique.h5', 'df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get (overlapping) date ranges for each article\n",
    "\n",
    "langs = article_dates_unique['lang'].unique()\n",
    "\n",
    "ranges_dfs = []\n",
    "for lang in langs:\n",
    "    print(lang)\n",
    "\n",
    "    lang_article_dates_unique = article_dates_unique[article_dates_unique['lang'] == lang]\n",
    "    lang_article_dates_unique = lang_article_dates_unique.sort_values(['redirected_title', 'date'])\n",
    "    lang_article_dates_unique['start_date'] = lang_article_dates_unique['date'] - pd.DateOffset(days=10)\n",
    "    lang_article_dates_unique['end_date'] = lang_article_dates_unique['date'] + pd.DateOffset(days=11)\n",
    "    \n",
    "    # get overlapping date ranges with same article\n",
    "    title = None\n",
    "    last_date = None\n",
    "    range_dfs = []\n",
    "    l_range_df = []\n",
    "    n = 0\n",
    "    for i, row in lang_article_dates_unique.iterrows():\n",
    "        if n % 100000 == 0:\n",
    "            print(n/len(lang_article_dates_unique), end='\\r')\n",
    "        if row['redirected_title'] == title:\n",
    "            if row['start_date'] <= last_date:\n",
    "                last_date = row['end_date']\n",
    "            else:\n",
    "                l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "                start_date = row['start_date']\n",
    "                last_date = row['end_date']  \n",
    "        else:\n",
    "            if title is not None:\n",
    "                l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "            title = row['redirected_title']\n",
    "            start_date = row['start_date']\n",
    "            last_date = row['end_date']\n",
    "        n+=1\n",
    "    l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "\n",
    "    l_range_df = pd.DataFrame(l_range_df)\n",
    "    l_range_df['lang'] = lang\n",
    "    ranges_dfs.append(l_range_df[['lang', 'title', 'start_date', 'end_date']])\n",
    "\n",
    "ranges_df = pd.concat(ranges_dfs, ignore_index=True)\n",
    "ranges_df.to_hdf('data/ranges_df.h5', 'df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also get lang article dates for raw, originally posted titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/langpagemaps.pkl', 'rb') as f:\n",
    "    pagemapsdict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in pagemapsdict:\n",
    "    print(lang)\n",
    "    article_dates_unique.loc[article_dates_unique['lang'] == lang, 'norm_raw_title'] = article_dates_unique['raw_title'].map(pagemapsdict[lang].norm_map).fillna(article_dates_unique['raw_title']) \n",
    "article_dates_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get (overlapping) date ranges for each article - with normalised RAW TITLES - don't bother collecting for those we've already got\n",
    "\n",
    "diffarts = article_dates_unique[article_dates_unique['norm_raw_title'] != article_dates_unique['redirected_title']]\n",
    "\n",
    "langs = article_dates_unique['lang'].unique()\n",
    "\n",
    "raw_ranges_dfs = []\n",
    "for lang in langs:\n",
    "    print(lang)\n",
    "\n",
    "    lang_article_dates_unique = diffarts[diffarts['lang'] == lang]\n",
    "    lang_article_dates_unique = lang_article_dates_unique.sort_values(['norm_raw_title', 'date'])\n",
    "    lang_article_dates_unique['start_date'] = lang_article_dates_unique['date'] - pd.DateOffset(days=10)\n",
    "    lang_article_dates_unique['end_date'] = lang_article_dates_unique['date'] + pd.DateOffset(days=11)\n",
    "    \n",
    "    # get overlapping date ranges with same article\n",
    "    title = None\n",
    "    last_date = None\n",
    "    raw_range_dfs = []\n",
    "    l_range_df = []\n",
    "    n = 0\n",
    "    for i, row in lang_article_dates_unique.iterrows():\n",
    "        if n % 100000 == 0:\n",
    "            print(n/len(lang_article_dates_unique), end='\\r')\n",
    "        if row['norm_raw_title'] == title:\n",
    "            if row['start_date'] <= last_date:\n",
    "                last_date = row['end_date']\n",
    "            else:\n",
    "                l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "                start_date = row['start_date']\n",
    "                last_date = row['end_date']  \n",
    "        else:\n",
    "            if title is not None:\n",
    "                l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "            title = row['norm_raw_title']\n",
    "            start_date = row['start_date']\n",
    "            last_date = row['end_date']\n",
    "        n+=1\n",
    "    l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "\n",
    "    l_range_df = pd.DataFrame(l_range_df)\n",
    "    l_range_df['lang'] = lang\n",
    "    raw_ranges_dfs.append(l_range_df[['lang', 'title', 'start_date', 'end_date']])\n",
    "\n",
    "raw_ranges_df = pd.concat(raw_ranges_dfs, ignore_index=True)\n",
    "raw_ranges_df.to_hdf('data/raw_ranges_df.h5', 'df', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
