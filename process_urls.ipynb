{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import unquote\n",
    "import wikitoolkit as wt\n",
    "import string\n",
    "import pickle\n",
    "import sqlite3\n",
    "\n",
    "my_agent = 'wikireddit <p.gildersleve@exeter.ac.uk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create full link tables\n",
    "\n",
    "bodylinks = pd.read_hdf('data/bodylinks.h5', 'df')\n",
    "titlelinks = pd.read_hdf('data/titlelinks.h5', 'df')\n",
    "bodylinks['in_title'] = False\n",
    "titlelinks['in_title'] = True\n",
    "commentlinks = pd.read_hdf('data/commentlinks.h5', 'df')\n",
    "\n",
    "posts = pd.read_hdf('data/posts.h5', 'df')\n",
    "comments = pd.concat([pd.read_hdf(f'data/comments_{x}.h5') for x in range(1,5)]).reset_index(drop=True)\n",
    "\n",
    "postlinks = pd.concat([bodylinks, titlelinks], ignore_index=True)\n",
    "postlinks = posts.merge(postlinks, on='id', how='left').dropna(subset=['final_url'])\n",
    "commentlinks = comments.merge(commentlinks, on='id', how='left').dropna(subset=['final_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all unique (date, links) in one table\n",
    "\n",
    "postlinks['created_date'] = postlinks['created_at'].dt.floor('D')\n",
    "postlinks['updated_date'] = postlinks['updated_at'].dt.floor('D')\n",
    "postlinks = postlinks[postlinks['final_valid']]\n",
    "postlinks_unique = postlinks[['final_url', 'created_date', 'updated_date']].drop_duplicates().copy()\n",
    "print(len(postlinks_unique))\n",
    "\n",
    "commentlinks['created_date'] = commentlinks['created_at'].dt.floor('D')\n",
    "commentlinks['updated_date'] = commentlinks['last_modified_at'].dt.floor('D')\n",
    "commentlinks = commentlinks[commentlinks['final_valid']]\n",
    "commentlinks_unique = commentlinks[['final_url', 'created_date', 'updated_date']].drop_duplicates().copy()\n",
    "print(len(commentlinks_unique))\n",
    "\n",
    "all_links = pd.concat([postlinks_unique, commentlinks_unique], ignore_index=True).drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to get article titles from urls\n",
    "\n",
    "def url_parse(url):\n",
    "    if '?' in url:\n",
    "        query = url.split('?')[1]\n",
    "        query_dict = dict(q.split('=') for q in query.split('&') if len(q.split('=')) == 2)\n",
    "        # print(query_dict)\n",
    "        return query_dict\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "async def resolve_ids(links_df):\n",
    "\n",
    "    missing = links_df[links_df['raw_title'].isna()][['lang', 'final_url']]\n",
    "    for l in missing['lang'].unique():\n",
    "        urls = missing[missing['lang'] == l]['final_url'].unique()\n",
    "        urldicts = {u: url_parse(u) for u in urls}\n",
    "        # print(urldicts)\n",
    "        titlemaps = {}\n",
    "        revmaps = {}\n",
    "        pageidmaps = {}\n",
    "        for u in urls:\n",
    "            if urldicts[u] is not None:\n",
    "                if 'title' in urldicts[u]:\n",
    "                    titlemaps[u] = urldicts[u]['title'].replace('+', ' ')\n",
    "                elif 'curid' in urldicts[u]:\n",
    "                    pageidmaps[u] = unquote(urldicts[u]['curid']).strip(string.punctuation+string.whitespace)\n",
    "                elif ('oldid' in urldicts[u])&(urldicts[u].get('oldid', '') != 'prev'):\n",
    "                    revmaps[u] = unquote(urldicts[u]['oldid']).strip(string.punctuation+string.whitespace)\n",
    "                elif 'diff' in urldicts[u]:\n",
    "                    revmaps[u] = unquote(urldicts[u]['diff']).strip(string.punctuation+string.whitespace)\n",
    "            # print(urldicts[u])\n",
    "\n",
    "        wtsession = wt.WTSession(f'{l}.wikipedia', user_agent=my_agent)\n",
    "        pagemaps = wt.PageMaps()\n",
    "        # print(revmaps)\n",
    "        if revmaps:\n",
    "            # print(revmaps)\n",
    "            revinfo = await wt.basic_info(wtsession, revids=list(revmaps.values()), pagemaps=pagemaps, params={'prop': 'revisions', 'rvprop': 'ids'})\n",
    "        else:\n",
    "            revinfo = []\n",
    "        if pageidmaps:\n",
    "            pageidinfo = await wt.basic_info(wtsession, pageids=list(pageidmaps.values()), pagemaps=pagemaps, params={'prop': 'revisions', 'rvprop': 'ids'})\n",
    "        else:\n",
    "            pageidinfo = []\n",
    "        await wtsession.close()\n",
    "        \n",
    "        # print(pageidinfo)\n",
    "\n",
    "        revtitledict = [{z['revid']: y['title'] for z in y['revisions']} for x in revinfo for y in x]\n",
    "        # combine into single dict\n",
    "        revtitledict = {k: v for d in revtitledict for k, v in d.items()}\n",
    "        pageidtitledict = {y['pageid']: y['title'] for x in pageidinfo for y in x}\n",
    "        # combine into single dict\n",
    "        # print(pagetitledict)\n",
    "        revmaps = {k: revtitledict.get(int(v), None) for k, v in revmaps.items()}\n",
    "        pageidmaps = {k: pageidtitledict[int(v)] for k, v in pageidmaps.items()}\n",
    "\n",
    "        titledict = {**titlemaps, **revmaps, **pageidmaps}\n",
    "\n",
    "        urltitledict = {u: titledict.get(u, None) for u in urls}\n",
    "\n",
    "        links_df.loc[(links_df['lang'] == l) & (links_df['raw_title'].isna()), 'raw_title'\n",
    "                     ] = links_df.loc[(links_df['lang'] == l) & (links_df['raw_title'].isna()), 'final_url'].map(urltitledict)\n",
    "    \n",
    "    return links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get article titles from urls\n",
    "\n",
    "links_df = all_links.copy()\n",
    "langvars = ['zh-hans', 'zh-tw', 'zh-hk', 'zh-cn', 'zh-hant', 'zh', 'sr-ec', 'sr-el', 'zh-sg', 'zh-my', 'zh-mo', 'sr'] # lang variants\n",
    "links_df.loc[:, 'lang_subdomain'] = links_df['final_url'].str.extract(r'https://([\\w\\.-]+)\\.wikipedia\\.org')[0]\n",
    "links_df.loc[:, 'lang'] = links_df['lang_subdomain'].str.split('.').str[0]\n",
    "links_df.loc[:, 'mobile'] = links_df['lang_subdomain'].str.split('.').str[1] == 'm'\n",
    "links_df['final_url'] = links_df['final_url'].apply(unquote)\n",
    "links_df.loc[:, 'raw_title'] = links_df['final_url'].str.extract(r'https://([\\w\\.-]+)\\.wikipedia\\.org/+wiki/+(.+)'\n",
    "                                )[1].str.split('?').str[0]\n",
    "links_df.loc[:, 'raw_title'] = links_df['raw_title'].fillna(\n",
    "    links_df['final_url'].str.extract(r'https://([\\w\\.-]+)\\.wikipedia\\.org/api/rest_v1/page/mobile-html/(.+)')[1].str.split('?').str[0])\n",
    "links_df.loc[:, 'raw_title'] = links_df['raw_title'].fillna(\n",
    "    links_df['final_url'].str.extract(r'https://([\\w\\.-]+)\\.wikipedia\\.org/+w/index\\.php\\?title=([^&]+)')[1].str.split('?').str[0])\n",
    "\n",
    "# raise\n",
    "for lv in langvars:\n",
    "    links_df.loc[:, 'raw_title'] = links_df['raw_title'].fillna(\n",
    "        links_df['final_url'].str.extract(r'https://([\\w\\.-]+)\\.wikipedia\\.org/+%s/+([^/]+)' %lv)[1].str.split('?').str[0])\n",
    "\n",
    "links_df = await resolve_ids(links_df)\n",
    "links_df['raw_title'] = links_df['raw_title'].str.replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "lna = links_df[links_df['raw_title'].isna()]\n",
    "vc = lna['final_url'].str.split('/').str[3].value_counts().index\n",
    "lna['final_url'].str.split('/').str[3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lna[lna['final_url'].str.split('/').str[3]=='?wiki']['final_url'].value_counts().index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create long table of articles and dates\n",
    "articles_long = links_df.melt(id_vars=['lang', 'raw_title'], value_vars=['created_date', 'updated_date'], \n",
    "                                       var_name='date_type', value_name='date').dropna(subset=['date'])\n",
    "articles_long = articles_long.rename(columns={'date_type': 'is_updated_date'}).reset_index(drop=True)\n",
    "articles_long['is_updated_date'] = articles_long['is_updated_date'] == 'updated_date'\n",
    "articles_long = articles_long.copy()\n",
    "\n",
    "article_dates_unique = articles_long[['lang', 'raw_title', 'date']].drop_duplicates().reset_index(drop=True)\n",
    "article_dates_unique.to_hdf('data/article_dates_unique.h5', 'df', mode='w')\n",
    "article_dates_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect ID and redirect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_dates_unique = pd.read_hdf('data/article_dates_unique.h5', 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagemapsdict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in article_dates_unique['lang'].unique():\n",
    "    print()\n",
    "    print(lang)\n",
    "    if lang not in pagemapsdict:\n",
    "        pagemapsdict[lang] = wt.PageMaps()\n",
    "    lang_articles = article_dates_unique[article_dates_unique['lang'] == lang]\n",
    "    wtsession = wt.WTSession(f'{lang}.wikipedia', user_agent=my_agent)\n",
    "    groupsize = 1000\n",
    "    ua = list(lang_articles['raw_title'].unique())\n",
    "    groups = [ua[i:i+groupsize] for i in range(0, len(ua), groupsize)]\n",
    "\n",
    "    for n, g in enumerate(groups):\n",
    "        print(f'Fixing redirects {n+1}/{len(groups)}', end='\\r')\n",
    "        ingroupsize = groupsize\n",
    "        done = 0\n",
    "        while done < len(g):\n",
    "            try:\n",
    "                await pagemapsdict[lang].fix_redirects(wtsession, titles=g[done:done+ingroupsize])\n",
    "                done += ingroupsize\n",
    "            except ValueError:\n",
    "                ingroupsize = ingroupsize//2\n",
    "                \n",
    "                print(f'\\nError, reducing group size to {ingroupsize}')\n",
    "\n",
    "    for n, g in enumerate(groups):\n",
    "        print(f'Getting wikidata ids {n+1}/{len(groups)}', end='\\r')\n",
    "        ingroupsize = groupsize\n",
    "        done = 0\n",
    "        while done < len(g):\n",
    "            try:\n",
    "                await pagemapsdict[lang].get_wikidata_ids(wtsession, titles=g[done:done+ingroupsize])\n",
    "                done += ingroupsize\n",
    "            except ValueError:\n",
    "                ingroupsize = ingroupsize//2\n",
    "                print(f'\\nError, reducing group size to {ingroupsize}')\n",
    "\n",
    "    for n, g in enumerate(groups):\n",
    "        print(f'Getting redirects {n+1}/{len(groups)}', end='\\r')\n",
    "        ingroupsize = groupsize\n",
    "        done = 0\n",
    "        while done < len(g):\n",
    "            try:\n",
    "                await pagemapsdict[lang].get_redirects(wtsession, titles=g[done:done+ingroupsize])\n",
    "                done += ingroupsize\n",
    "            except ValueError:\n",
    "                ingroupsize = ingroupsize//2\n",
    "                print(f'\\nError, reducing group size to {ingroupsize}')\n",
    "\n",
    "    await wtsession.close()\n",
    "\n",
    "# save pagemapsdict\n",
    "with open('data/langpagemaps.pkl', 'wb') as f:\n",
    "    pickle.dump(pagemapsdict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert redirects table to save in db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/langpagemaps.pkl', 'rb') as f:\n",
    "    pagemapsdict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageidsdf = []\n",
    "for lang, pm in pagemapsdict.items():\n",
    "    df = pd.Series(pm.id_map).reset_index(name='pageid').rename(columns={'index': 'title'})\n",
    "    wikidata_ids = pd.Series(pm.wikidata_id_map).reset_index(name='wikidata_id').rename(columns={'index': 'title'})\n",
    "    df['lang'] = lang\n",
    "    df = df.merge(wikidata_ids, on='title', how='left')\n",
    "    pageidsdf.append(df[['lang', 'title', 'pageid', 'wikidata_id']])\n",
    "\n",
    "pageidsdf = pd.concat(pageidsdf, ignore_index=True)\n",
    "pageidsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_redirects = []\n",
    "for lang, pm in pagemapsdict.items():\n",
    "    norm_df = pd.Series(pm.norm_map).reset_index(name='norm_title').rename(columns={'index': 'raw_title'})\n",
    "    titles_redirect_df = pd.Series(pm.titles_redirect_map).reset_index(name='redirected_title').rename(columns={'index': 'norm_title'})\n",
    "\n",
    "    redirects_df = norm_df.merge(titles_redirect_df, on='norm_title', how='outer')\n",
    "    redirects_df['raw_title'] = redirects_df['raw_title'].fillna(redirects_df['norm_title'])\n",
    "    redirects_df['lang'] = lang\n",
    "\n",
    "    resolved_redirects.append(redirects_df[['lang', 'raw_title', 'norm_title', 'redirected_title']])\n",
    "resolved_redirects = pd.concat(resolved_redirects, ignore_index=True)\n",
    "resolved_redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_redirects = []\n",
    "for lang, pm in pagemapsdict.items():\n",
    "    df = pd.DataFrame([(k, y) for k, v in pm.collected_title_redirects.items() for y in v],\n",
    "                columns=['canonical_title', 'other_title'])\n",
    "    df['lang'] = lang\n",
    "    collected_redirects.append(df[['lang', 'canonical_title', 'other_title']])\n",
    "collected_redirects = pd.concat(collected_redirects, ignore_index=True)\n",
    "collected_redirects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pagemaps data to sql tables\n",
    "\n",
    "conn = sqlite3.connect('wikireddit.db')\n",
    "pageidsdf.to_sql('wiki_ids', conn, if_exists='replace', index=False)\n",
    "resolved_redirects.to_sql('resolved_redirects', conn, if_exists='replace', index=False)\n",
    "collected_redirects.to_sql('collected_redirects', conn, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get storage size of database\n",
    "import os\n",
    "mb_size = os.path.getsize('wikireddit.db') / 1024 / 1024\n",
    "print(f\"Database size: {mb_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the lang link date tables to get canonical titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in pagemapsdict:\n",
    "    print(lang)\n",
    "    article_dates_unique.loc[article_dates_unique['lang'] == lang, 'redirected_title'] = article_dates_unique['raw_title'].map(pagemapsdict[lang].norm_map).fillna(article_dates_unique['raw_title'])   \n",
    "    article_dates_unique.loc[article_dates_unique['lang'] == lang, 'redirected_title'] = (\n",
    "        article_dates_unique.loc[article_dates_unique['lang'] == lang, 'redirected_title']\n",
    "        .map(pagemapsdict[lang].titles_redirect_map)\n",
    "        .fillna(article_dates_unique.loc[article_dates_unique['lang'] == lang, 'redirected_title'])\n",
    "    )\n",
    "    article_dates_unique.loc[article_dates_unique['lang'] == lang, 'pageid'] = article_dates_unique['redirected_title'].map(pagemapsdict[lang].id_map)\n",
    "    article_dates_unique.loc[article_dates_unique['lang'] == lang, 'wikidata_id'] = article_dates_unique['redirected_title'].map(pagemapsdict[lang].wikidata_id_map)\n",
    "\n",
    "article_dates_unique = article_dates_unique.drop_duplicates(subset=['lang', 'date', 'redirected_title']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  bug fix - unicode parsing pecularity of api?\n",
    "\n",
    "article_dates_unique.loc[article_dates_unique['raw_title']=='Augusto패w roundup', 'redirected_title'] = 'August칩w roundup'\n",
    "article_dates_unique.loc[article_dates_unique['raw_title']=='Augusto패w roundup', 'pageid'] = 6002747\n",
    "article_dates_unique.loc[article_dates_unique['raw_title']=='Augusto패w roundup', 'wikidata_id'] = 'Q2612443'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "\n",
    "display(article_dates_unique[article_dates_unique['pageid'].isna()]['redirected_title'].value_counts())\n",
    "article_dates_unique[article_dates_unique['pageid']==-1]['redirected_title'].value_counts().head(50) # -1 is a placeholder for missing pageids - these are all invalid / special pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "article_dates_unique = article_dates_unique.dropna(subset=['pageid'])\n",
    "article_dates_unique = article_dates_unique[article_dates_unique['pageid'] != -1]\n",
    "article_dates_unique['pageid'] = article_dates_unique['pageid'].astype(int)\n",
    "\n",
    "article_dates_unique.to_hdf('data/article_dates_unique.h5', 'df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get (overlapping) date ranges for each article\n",
    "\n",
    "langs = article_dates_unique['lang'].unique()\n",
    "\n",
    "ranges_dfs = []\n",
    "for lang in langs:\n",
    "    print(lang)\n",
    "\n",
    "    lang_article_dates_unique = article_dates_unique[article_dates_unique['lang'] == lang]\n",
    "    lang_article_dates_unique = lang_article_dates_unique.sort_values(['redirected_title', 'date'])\n",
    "    lang_article_dates_unique['start_date'] = lang_article_dates_unique['date'] - pd.DateOffset(days=10)\n",
    "    lang_article_dates_unique['end_date'] = lang_article_dates_unique['date'] + pd.DateOffset(days=11)\n",
    "    \n",
    "    # get overlapping date ranges with same article\n",
    "    title = None\n",
    "    last_date = None\n",
    "    range_dfs = []\n",
    "    l_range_df = []\n",
    "    n = 0\n",
    "    for i, row in lang_article_dates_unique.iterrows():\n",
    "        if n % 100000 == 0:\n",
    "            print(n/len(lang_article_dates_unique), end='\\r')\n",
    "        if row['redirected_title'] == title:\n",
    "            if row['start_date'] <= last_date:\n",
    "                last_date = row['end_date']\n",
    "            else:\n",
    "                l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "                start_date = row['start_date']\n",
    "                last_date = row['end_date']  \n",
    "        else:\n",
    "            if title is not None:\n",
    "                l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "            title = row['redirected_title']\n",
    "            start_date = row['start_date']\n",
    "            last_date = row['end_date']\n",
    "        n+=1\n",
    "    l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "\n",
    "    l_range_df = pd.DataFrame(l_range_df)\n",
    "    l_range_df['lang'] = lang\n",
    "    ranges_dfs.append(l_range_df[['lang', 'title', 'start_date', 'end_date']])\n",
    "\n",
    "ranges_df = pd.concat(ranges_dfs, ignore_index=True)\n",
    "ranges_df.to_hdf('data/ranges_df.h5', 'df', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also get lang article dates for raw, originally posted titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/langpagemaps.pkl', 'rb') as f:\n",
    "    pagemapsdict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in pagemapsdict:\n",
    "    print(lang)\n",
    "    article_dates_unique.loc[article_dates_unique['lang'] == lang, 'norm_raw_title'] = article_dates_unique['raw_title'].map(pagemapsdict[lang].norm_map).fillna(article_dates_unique['raw_title']) \n",
    "article_dates_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get (overlapping) date ranges for each article - with normalised RAW TITLES - don't bother collecting for those we've already got\n",
    "\n",
    "diffarts = article_dates_unique[article_dates_unique['norm_raw_title'] != article_dates_unique['redirected_title']]\n",
    "\n",
    "langs = article_dates_unique['lang'].unique()\n",
    "\n",
    "raw_ranges_dfs = []\n",
    "for lang in langs:\n",
    "    print(lang)\n",
    "\n",
    "    lang_article_dates_unique = diffarts[diffarts['lang'] == lang]\n",
    "    lang_article_dates_unique = lang_article_dates_unique.sort_values(['norm_raw_title', 'date'])\n",
    "    lang_article_dates_unique['start_date'] = lang_article_dates_unique['date'] - pd.DateOffset(days=10)\n",
    "    lang_article_dates_unique['end_date'] = lang_article_dates_unique['date'] + pd.DateOffset(days=11)\n",
    "    \n",
    "    # get overlapping date ranges with same article\n",
    "    title = None\n",
    "    last_date = None\n",
    "    raw_range_dfs = []\n",
    "    l_range_df = []\n",
    "    n = 0\n",
    "    for i, row in lang_article_dates_unique.iterrows():\n",
    "        if n % 100000 == 0:\n",
    "            print(n/len(lang_article_dates_unique), end='\\r')\n",
    "        if row['norm_raw_title'] == title:\n",
    "            if row['start_date'] <= last_date:\n",
    "                last_date = row['end_date']\n",
    "            else:\n",
    "                l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "                start_date = row['start_date']\n",
    "                last_date = row['end_date']  \n",
    "        else:\n",
    "            if title is not None:\n",
    "                l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "            title = row['norm_raw_title']\n",
    "            start_date = row['start_date']\n",
    "            last_date = row['end_date']\n",
    "        n+=1\n",
    "    l_range_df.append({'title': title, 'start_date': start_date, 'end_date': last_date})\n",
    "\n",
    "    l_range_df = pd.DataFrame(l_range_df)\n",
    "    l_range_df['lang'] = lang\n",
    "    raw_ranges_dfs.append(l_range_df[['lang', 'title', 'start_date', 'end_date']])\n",
    "\n",
    "raw_ranges_df = pd.concat(raw_ranges_dfs, ignore_index=True)\n",
    "raw_ranges_df.to_hdf('data/raw_ranges_df.h5', 'df', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
