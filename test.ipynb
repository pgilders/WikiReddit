{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import syft as sy\n",
    "\n",
    "sy.requires(\"==0.9.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFR_URL = \"https://reddit-for-researchers.snooguts.net/login\"\n",
    "# guest_client = sy.login_as_guest(url=RFR_URL)\n",
    "# guest_client.forgot_password(email=\"prgildersleve@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we recommend that you source these values using environment variables\n",
    "URL = \"https://reddit-for-researchers.snooguts.net\"\n",
    "EMAIL = \n",
    "PASSWORD = \n",
    "\n",
    "# you can provide a \"password\" keyword argument, but if you don't...\n",
    "# the browser will prompt you for input\n",
    "client = sy.login(\n",
    "    url=URL,\n",
    "    email=EMAIL,\n",
    "    password=PASSWORD\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.custom_api.api_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.refresh()\n",
    "client.api.services.reddit.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query=\"\"\"\n",
    "  SELECT subreddit_id, AVG(score) AS average_score\n",
    "  FROM comments\n",
    "  GROUP BY subreddit_id\n",
    "  LIMIT 500\n",
    "\"\"\"\n",
    "\n",
    "client.api.services.reddit.query.mock(sql_query=sql_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query=\"\"\"\n",
    "  SELECT *\n",
    "  FROM comments\n",
    "  WHERE body LIKE '%wikipedia%'\n",
    "  AND created_at >=< '2021-01-01'\n",
    "  AND created_at < '2022-01-01'\n",
    "\"\"\"\n",
    "\n",
    "df = client.api.services.reddit.query.mock(sql_query=sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query=\"\"\"\n",
    "  SELECT *\n",
    "  FROM comments\n",
    "  WHERE post_id\n",
    "  IN (\n",
    "  SELECT id\n",
    "  FROM posts \n",
    "  WHERE (body LIKE '%wikipedia%' OR title LIKE '%wikipedia%')\n",
    "  )\n",
    "\"\"\"\n",
    "client.api.services.reddit.query.mock(sql_query=sql_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query=\"\"\"\n",
    "  SELECT *\n",
    "  FROM comments \n",
    "  WHERE body LIKE '%facebook%'\n",
    "  AND created_at >= '2023-01-01'\n",
    "  AND created_at < '2024-01-01'\n",
    "\"\"\"\n",
    "\n",
    "client.api.services.reddit.query.mock(sql_query=sql_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query=\"\"\"\n",
    "  SELECT *\n",
    "  FROM comments \n",
    "  WHERE body LIKE '%wikipedia%'\n",
    "  AND created_at >= '2020-01-01'\n",
    "  AND created_at < '2021-01-01'\n",
    "\"\"\"\n",
    "\n",
    "client.api.services.reddit.submit_query(\n",
    "    func_name=\"pg_wiki_comments_2020\",\n",
    "    sql_query=sql_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_dict = {}\n",
    "for year in range(2020, 2024):\n",
    "    for month in range(1, 13):\n",
    "        start_date = f\"{year}-{month:02d}-01\"\n",
    "        if month == 12:\n",
    "            end_date = f\"{year + 1}-01-01\"\n",
    "        else:\n",
    "            end_date = f\"{year}-{month + 1:02d}-01\"\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM comments\n",
    "        WHERE body LIKE '%wikipedia%'\n",
    "        AND created_at >= '{start_date}'\n",
    "        AND created_at < '{end_date}'\n",
    "        \"\"\"\n",
    "        key = f\"pg_wiki_comments_{year}{month:02d}\"\n",
    "        queries_dict[key] = sql_query\n",
    "\n",
    "queries_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit all queries in the dictionary\n",
    "for key, sql_query in queries_dict.items():\n",
    "    print(key)\n",
    "    if '2020' in key:\n",
    "        continue\n",
    "    client.api.services.reddit.submit_query(\n",
    "        func_name=key,\n",
    "        sql_query=sql_query\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.api.services.reddit.query.mock(sql_query=sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query=\"\"\"\n",
    "  SELECT *\n",
    "  FROM comments\n",
    "  WHERE post_id\n",
    "  IN (\n",
    "  SELECT id\n",
    "  FROM posts \n",
    "  WHERE (body LIKE '%wikipedia%' OR title LIKE '%wikipedia%')\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "client.api.services.reddit.submit_query(\n",
    "    func_name=\"pg_wiki_replies\",\n",
    "    sql_query=sql_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query=\"\"\"\n",
    "  SELECT *\n",
    "  FROM posts \n",
    "  WHERE (body LIKE '%wikipedia%' OR title LIKE '%wikipedia%')\n",
    "  AND created_at >= '2021-01-01'\n",
    "  AND created_at < '2022-01-01'\n",
    "\"\"\"\n",
    "\n",
    "client.api.services.reddit.submit_query(\n",
    "    func_name=\"pg_wiki_2021\",\n",
    "    sql_query=sql_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refresh()\n",
    "client.requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "y_id = {2020: '4515465e7f39480cb163f8c880d0b9dd', 2021: '8828789e73774282a4b9842f86032615',\n",
    " 2022: 'ad435289e53d4393ba6556cf69a5a032', 2023: '27922f61db4644338f0c2e917d6a7a38'}\n",
    "\n",
    "wikidf = []\n",
    "for y, id in y_id.items():\n",
    "    while True:\n",
    "        print(y)\n",
    "        try:\n",
    "            request = client.api.services.request.get_by_uid(uid=sy.UID(id))\n",
    "            job = request.code(blocking=False)\n",
    "            results = job.wait()\n",
    "            df = results.get()\n",
    "            wikidf.append(df)\n",
    "            break\n",
    "        except AttributeError as ex:\n",
    "            print(ex)\n",
    "            time.sleep(10)\n",
    "            client = sy.login(\n",
    "                url=URL,\n",
    "                email=EMAIL,\n",
    "                password=PASSWORD\n",
    "            )\n",
    "\n",
    "wikidf = pd.concat(wikidf, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:   \n",
    "    try:\n",
    "        request = client.api.services.request.get_by_uid(uid=sy.UID('a4a5e43a1aea4c9e87f857d40e59e1bb'))\n",
    "        job = request.code(blocking=False)\n",
    "        results = job.wait()\n",
    "        df = results.get()\n",
    "        break\n",
    "    except AttributeError as ex:\n",
    "        print(ex)\n",
    "        time.sleep(10)\n",
    "        client = sy.login(\n",
    "            url=URL,\n",
    "            email=EMAIL,\n",
    "            password=PASSWORD\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = client.api.services.request.get_by_uid(uid=sy.UID('d02becf533fc4085911783f6c48d1f9e'))\n",
    "job = request.code(blocking=False)\n",
    "results = job.wait()\n",
    "df = results.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = client.api.services.request.get_by_uid(uid=sy.UID('2bf9dd73b0d34659a08dec5185f5694c'))\n",
    "job = request.code(blocking=False)\n",
    "results = job.wait()\n",
    "df = results.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['score', 'gildings', 'num_comments']:\n",
    "    wikidf[c] = wikidf[c].astype(np.int64)\n",
    "for c in ['nsfw', 'self', 'video', 'locked', 'spoiler', 'sticky']:\n",
    "    wikidf[c] = wikidf[c].astype(np.bool_)\n",
    "for c in ['created_at', 'updated_at']:\n",
    "    wikidf[c] = pd.to_datetime(wikidf[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf.to_hdf('wikidf.h5', key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wikidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf['created_at'].hist(bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Regex pattern to identify Wikipedia links\n",
    "pattern = r'([a-z]{0,1,2,3}\\.)?wikipedia\\.org/wiki/[^\\s]+'\n",
    "\n",
    "# Example usage\n",
    "text = \"Check out this Wikipedia link: https://en.wikipedia.org/wiki/Artificial_intelligence and this one: https://fr.wikipedia.org/wiki/Intelligence_artificielle\"\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = r'wikipedia\\.org/wiki/[^\\s]+'\n",
    "wikidf['body'].str.findall(pattern).explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wikidf.loc[4, 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Check out this Wikipedia link: [fjgsjf](https://en.wikipedia.org/wiki/Artificial_intelligence) and this one: https://fr.wikipedia.org/wiki/Intelligence_artificielle\"\n",
    "pattern = r'\\[([^\\]]+)\\]\\(https?://[a-z]{2,3}\\.wikipedia\\.org/wiki/[^\\s)]+\\)'\n",
    "matches = re.findall(pattern, text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\S*wikipedia.org\\S*'\n",
    "uls = wikidf['body'].str.findall(pattern).explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(uls.index[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "\n",
    "t = '[Houseparty](https://en.wikipedia.org/wiki/Houseparty_(app)),'\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "urls = url_extractor.find_urls(t)\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls =  \"\"\"GROUPS](https://en.wikipedia.org/wiki/Warez_scene_)**\n",
    "24h](https://en.wikipedia.org/wiki/Special:RecentChangesLinked?hidebots=1&hideminor=1&hidenewpages=1&hidecategorization=1&hideWikibase=1&hidelog=1&target=User%3AGuarapiranga%2Fsandbox%2F1&limit=1000&days=7&enhanced=1&urlversion=2)\n",
    "page)](https://wikipedia.org/wiki/Special:Random)\n",
    "[https://en.wikipedia.org/wiki/Race\\_and\\_ethnicity\\_in\\_the\\_United\\_States](https://en.wikipedia.org/wiki/Race_and_ethnicity_in_the_United_States)\n",
    "contemporary](https://en.wikipedia.org/wiki/Soft_adult_contemporary)[Bell\n",
    "Radio](https://en.wikipedia.org/wiki/Bell_Media_Radio)[French](https://en.wikipedia.org/wiki/French_language)\n",
    "Media](https://en.wikipedia.org/wiki/Rogers_Media)FM\n",
    "Falls](https://en.wikipedia.org/wiki/Smiths_Falls,_Ontario)FM\n",
    "Corporation](https://en.wikipedia.org/wiki/Canadian_Broadcasting_Corporation)FM\n",
    "Radio](https://en.wikipedia.org/wiki/Bell_Media_Radio)FM\n",
    "Corporation](https://en.wikipedia.org/wiki/Canadian_Broadcasting_Corporation)[French](https://en.wikipedia.org/wiki/French_language)FM\n",
    "Media](https://en.wikipedia.org/wiki/Rogers_Media)licensed\n",
    "Group](https://en.wikipedia.org/wiki/Stingray_Group)FM\n",
    "Media](https://en.wikipedia.org/wiki/RNC_Media)[French](https://en.wikipedia.org/wiki/French_language)\n",
    "Entertainment](https://en.wikipedia.org/wiki/Corus_Entertainment)FM\n",
    "[https://en.wikipedia.org/wiki/List\\_of\\_suicide\\_crisis\\_lines](https://en.wikipedia.org/wiki/List_of_suicide_crisis_lines)\n",
    "[Houseparty](https://en.wikipedia.org/wiki/Houseparty_(app)),\n",
    "needed*](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed)\\]\n",
    "App](https://en.wikipedia.org/wiki/Cash_App)\n",
    "https://en.wikipedia.org/wiki/Truth#Mathematics\n",
    "https://en.m.wikipedia.org/wiki/Demographics_of_Ontario\n",
    "speech](https://en.wikipedia.org/wiki/Hate_speech)\n",
    "Teahouse](https://en.wikipedia.org/wiki/Wikipedia:Teahouse)\n",
    "Contents](https://en.wikipedia.org/wiki/Help:Contents)\n",
    "[Wikipedia.org](http://www.wikipedia.org)\n",
    "[ISBN](https://en.wikipedia.org/wiki/ISBN_(identifier))\n",
    "Wikipedia](http://en.wikipedia.org/wiki/List_of_Major_League_Soccer_seasons)\n",
    "Country](https://en.wikipedia.org/wiki/Pure_Country_(radio_network))\n",
    "OBJKECT[B](https://en.wikipedia.org/wiki/The_Shadow_of_the_Torturer)UILDING\n",
    "music](https://en.wikipedia.org/wiki/Country_music)[Bell\n",
    "106.9[CKQB-FM](https://en.wikipedia.org/wiki/CKQB-FM)Jump\n",
    "radio](https://en.wikipedia.org/wiki/Contemporary_hit_radio)[Corus\n",
    "93.9[CKKL-FM](https://en.wikipedia.org/wiki/CKKL-FM)[Pure\n",
    "100.3[CJMJ-FM](https://en.wikipedia.org/wiki/CJMJ-FM)Majic\n",
    "radio](https://en.wikipedia.org/wiki/Community_radio)Radio\n",
    "contemporary](https://en.wikipedia.org/wiki/Hot_adult_contemporary)[Rogers\n",
    "Ottawa](https://en.wikipedia.org/wiki/University_of_Ottawa)FM\n",
    "FM](https://en.wikipedia.org/wiki/Rouge_FM)[soft\n",
    "radio](https://en.wikipedia.org/wiki/Campus_radio)[University\n",
    "89.1[CHUO-FM](https://en.wikipedia.org/wiki/CHUO-FM)CHUO\n",
    "rock](https://en.wikipedia.org/wiki/Modern_rock)[Stingray\n",
    "hits](https://en.wikipedia.org/wiki/Adult_hits)[Rogers\n",
    "88.5[CILV-FM](https://en.wikipedia.org/wiki/CILV-FM)Live\n",
    "radio](https://en.wikipedia.org/wiki/Contemporary_hit_radio)[Stingray\n",
    "105.3[CISS-FM](https://en.wikipedia.org/wiki/CISS-FM)Kiss\n",
    "radio](https://en.wikipedia.org/wiki/Talk_radio)[Cogeco](https://en.wikipedia.org/wiki/Cogeco)[French](https://en.wikipedia.org/wiki/French_language)\n",
    "104.7[CKOF-FM](https://en.wikipedia.org/wiki/CKOF-FM)104,7[Talk\n",
    "radio](https://en.wikipedia.org/wiki/Contemporary_hit_radio)[Bell\n",
    "104.1[CKTF-FM](https://en.wikipedia.org/wiki/CKTF-FM)[Énergie](https://en.wikipedia.org/wiki/%C3%89nergie)[contemporary\n",
    "Music](https://en.wikipedia.org/wiki/CBC_Music)[public](https://en.wikipedia.org/wiki/Public_broadcasting)\n",
    "FM](https://en.wikipedia.org/wiki/Rythme_FM)[soft\n",
    "89.9[CIHT-FM](https://en.wikipedia.org/wiki/CIHT-FM)Hot\n",
    "Première](https://en.wikipedia.org/wiki/Ici_Radio-Canada_Premi%C3%A8re)[news](https://en.wikipedia.org/wiki/News)/[talk](https://en.wikipedia.org/wiki/Talk_radio)[Canadian\n",
    "94.9[CIMF-FM](https://en.wikipedia.org/wiki/CIMF-FM)[Rouge\n",
    "90.7[CBOF-FM](https://en.wikipedia.org/wiki/CBOF-FM)[Ici\n",
    "92.3[CJET-FM](https://en.wikipedia.org/wiki/CJET-FM)[Jack\n",
    "92.7[CJVN-FM](https://en.wikipedia.org/wiki/CJVN-FM)[Christian\n",
    "radio](https://en.wikipedia.org/wiki/Christian_radio)Fiston\n",
    "Kalambay[French](https://en.wikipedia.org/wiki/French_language)\n",
    "Musique](https://en.wikipedia.org/wiki/Ici_Musique)[public](https://en.wikipedia.org/wiki/Public_broadcasting)\n",
    "93.1[CKCU-FM](https://en.wikipedia.org/wiki/CKCU-FM)CKCU\n",
    "radio](https://en.wikipedia.org/wiki/Campus_radio)[Carleton\n",
    "One](https://en.wikipedia.org/wiki/CBC_Radio_One)[news](https://en.wikipedia.org/wiki/News)/[talk](https://en.wikipedia.org/wiki/Talk_radio)[Canadian\n",
    "91.5[CBO-FM](https://en.wikipedia.org/wiki/CBO-FM)[CBC\n",
    "94.5[CJFO-FM](https://en.wikipedia.org/wiki/CJFO-FM)[community\n",
    "d'Ottawa[French](https://en.wikipedia.org/wiki/French_language)\n",
    "107.9[CKDJ-FM](https://en.wikipedia.org/wiki/CKDJ-FM)CKDJ\n",
    "rock](https://en.wikipedia.org/wiki/Mainstream_rock)[Rogers\n",
    "106.1[CHEZ-FM](https://en.wikipedia.org/wiki/CHEZ-FM)Chez\n",
    "FM](https://en.wikipedia.org/wiki/Jack_FM)[adult\n",
    "103.3[CBOQ-FM](https://en.wikipedia.org/wiki/CBOQ-FM)[CBC\n",
    "97.1[CHLX-FM](https://en.wikipedia.org/wiki/CHLX-FM)[Rythme\n",
    "University](https://en.wikipedia.org/wiki/Carleton_University)FM\n",
    "96.5[CFTX-FM](https://en.wikipedia.org/wiki/CFTX-FM)POP\n",
    "Communications](https://en.wikipedia.org/wiki/Evanov_Communications)FM\n",
    "98.5[CJWL-FM](https://en.wikipedia.org/wiki/CJWL-FM)The\n",
    "International](https://en.wikipedia.org/wiki/CHIN_Radio/TV_International)FM\n",
    "[community](https://en.wikipedia.org/wiki/Community_radio)[CHIN\n",
    "99.1[CHRI-FM](https://en.wikipedia.org/wiki/CHRI-FM)CHRI\n",
    "97.9[CJLL-FM](https://en.wikipedia.org/wiki/CJLL-FM)CHIN\n",
    "95.7[CFPO-FM](https://en.wikipedia.org/wiki/CFPO-FM)Elmnt\n",
    "contemporary](https://en.wikipedia.org/wiki/Adult_contemporary)[Aboriginal\n",
    "Network](https://en.wikipedia.org/wiki/Aboriginal_Peoples_Television_Network)FM\n",
    "[Kemptville](https://en.wikipedia.org/wiki/Kemptville,_Ontario)FM\n",
    "music](https://en.wikipedia.org/wiki/Christian_music)Christian\n",
    "99.7[CJOT-FM](https://en.wikipedia.org/wiki/CJOT-FM)[boom\n",
    "99.7](https://en.wikipedia.org/wiki/Boom_FM)[Classic\n",
    "standards](https://en.wikipedia.org/wiki/Pop_standards)[Evanov\n",
    "Group](https://en.wikipedia.org/wiki/Vista_Broadcast_Group)licensed\n",
    "music](https://en.wikipedia.org/wiki/Country_music)[Rogers\n",
    "hits](https://en.wikipedia.org/wiki/Classic_hits)[RNC\n",
    "contemporary](https://en.wikipedia.org/wiki/Soft_adult_contemporary)[RNC\n",
    "102.5[CBOX-FM](https://en.wikipedia.org/wiki/CBOX-FM)[Ici\n",
    "Rock](https://en.wikipedia.org/wiki/Mainstream_Rock)Torres\n",
    "101.7[CIDG-FM](https://en.wikipedia.org/wiki/CIDG-FM)Rebel\n",
    "contemporary](https://en.wikipedia.org/wiki/Adult_contemporary)[Vista\n",
    "97.5[CKKV-FM](https://en.wikipedia.org/wiki/CKKV-FM)The\n",
    "101.1[CKBY-FM](https://en.wikipedia.org/wiki/CKBY-FM)Country\n",
    "Hits](https://en.wikipedia.org/wiki/Classic_Hits)[Corus\n",
    "College](https://en.wikipedia.org/wiki/Algonquin_(College))\n",
    "en.wikipedia.org/wiki/CBOQ-FM\n",
    "wikipedia.org/wiki/CBOQ-(FM)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Define the regex pattern to extract Wikipedia URLs\n",
    "pattern = r'(https?://)?((?:\\w+\\.)?wikipedia\\.org/?\\S*)'\n",
    "\n",
    "# Example regex pattern to match URLs with or without https, and handle parentheses\n",
    "pattern = r'(\\(?https?://)?((?:\\w+\\.)?wikipedia\\.org/?[\\S]*)\\)?'\n",
    "\n",
    "# Find all matches in the urls string\n",
    "wikipedia_urls = re.findall(pattern, urls)\n",
    "wikipedia_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sample text containing Markdown-style links\n",
    "text = \"\"\"\n",
    "Check out [OpenAI](https://www.openai.com) and [Python](https://www.python.org) for more information.\n",
    "Also, [Link with [nested] brackets](https://example.com/page(yes)) and [Link with escaped \\\\] bracket](https://example.com).\n",
    "\"\"\"\n",
    "\n",
    "# Convert Markdown to HTML\n",
    "soup = BeautifulSoup(markdown.markdown(text), 'html.parser')\n",
    "\n",
    "# Find all links\n",
    "for a in soup.find_all('a'):\n",
    "    displayed_text = a.text\n",
    "    url = a['href']\n",
    "    print(f\"Displayed text: {displayed_text}, URL: {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(text):\n",
    "    # Convert Markdown to HTML\n",
    "    soup = BeautifulSoup(markdown.markdown(text), 'html.parser')\n",
    "    # Find all links\n",
    "    links = [a['href'] for a in soup.find_all('a')]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "def extract_links(text):\n",
    "    # Convert Markdown to HTML\n",
    "    soup = BeautifulSoup(markdown.markdown(text), 'html.parser')\n",
    "    # Find all links\n",
    "    links = [a.get(\"href\") for a in soup.find_all('a')]\n",
    "    # Normalize URLs\n",
    "    llinks = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            if link:\n",
    "                ll = urllib.parse.urlparse(link).geturl()\n",
    "                if not urllib.parse.urlparse(ll).scheme:\n",
    "                    ll = 'https://' + ll\n",
    "                if urllib.parse.urlparse(ll).netloc[-13:] == 'wikipedia.org':\n",
    "                    llinks.append(ll)\n",
    "        except Exception as ex:\n",
    "            print(link, ex)\n",
    "    \n",
    "    # links = [urllib.parse.urlparse(link) for link in links if link]\n",
    "    # links = [l.geturl() for l in llinks if l.netloc[-13:] == 'wikipedia.org']\n",
    "    return llinks\n",
    "\n",
    "def extract_links2(text):\n",
    "    # Convert Markdown to HTML\n",
    "    soup = BeautifulSoup(markdown.markdown(text), 'html.parser')\n",
    "    # Find all links\n",
    "    for a in soup.find_all('a'):\n",
    "        a.replace_with('')\n",
    "\n",
    "    pattern = r'\\S*.wikipedia\\.org\\S*'\n",
    "    matches = re.findall(pattern, soup.text)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in soup.find_all('a'):\n",
    "    a.replace_with('')\n",
    "\n",
    "pattern = r'\\S*.wikipedia\\.org\\S*'\n",
    "matches = re.findall(pattern, soup.text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf['body'].iloc[:100000].fillna('').apply(extract_links2).explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = '[https://en.wikipedia.org/wiki/WarGames_match](https://en.wikipedia.org/wiki/WarGames_match)'\n",
    "ll = urllib.parse.urlparse(l).geturl()\n",
    "# if not urllib.parse.urlparse(ll).scheme:\n",
    "#     ll = 'https://' + ll\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.parse.urlparse('en.wikipedia.org').geturl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf['wikilinks'] = wikidf['body'].fillna('').apply(extract_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidf['wikilinks'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(markdown.markdown(t), 'html.parser')\n",
    "# Find all links\n",
    "links = [a.get(\"href\") for a in soup.find_all('a')]\n",
    "llinks = = []\n",
    "for link in links:\n",
    "    try:\n",
    "        if link:\n",
    "            llinks.append(urllib.parse.urlparse(link).geturl())\n",
    "    except:\n",
    "        print(link)\n",
    "links = [urllib.parse.urlparse(link) for link in links[71:72] if link]\n",
    "# links = [l.geturl() for l in links if l.netloc[-13:] == 'wikipedia.org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')[71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wikidf['body'].iloc[1804]\n",
    "soup = BeautifulSoup(markdown.markdown(text), 'html.parser')\n",
    "# Find all links\n",
    "links = [a.get(\"href\") for a in soup.find_all('a')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[1000:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.parse.urlparse('https://en.wikipedia.org/wiki/Tupolev_Tu-85')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
