{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to add to the data:\n",
    "\n",
    "# posts\n",
    "# total wikipedia mentions\n",
    "# total mentions in URLs\n",
    "# total replies\n",
    "# total reply score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new tables - post URLs, comment urls\n",
    "\n",
    "# columns: post_id, created_at, updated_at, raw url, parsed url, domain, parsed title, processed title, article IDs???, \n",
    "# columns: comment_id, post_id, parent_id, created_at, last_modfied_at, raw url, parsed url, domain, article, redirecttitel, article IDs???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "ru\n",
      "pt\n",
      "it\n",
      "ro\n",
      "nl\n",
      "es\n",
      "fr\n",
      "no\n",
      "de\n",
      "simple\n",
      "eu\n",
      "ja\n",
      "zh\n",
      "tr\n",
      "hi\n",
      "sl\n",
      "ar\n",
      "cs\n",
      "csb\n",
      "mk\n",
      "szl\n",
      "hr\n",
      "uk\n",
      "sr\n",
      "sk\n",
      "bg\n",
      "cu\n",
      "dsb\n",
      "bs\n",
      "hsb\n",
      "sh\n",
      "pl\n",
      "rue\n",
      "be\n",
      "ml\n",
      "el\n",
      "fa\n",
      "sw\n",
      "ko\n",
      "id\n",
      "www\n",
      "hu\n",
      "fi\n",
      "vi\n",
      "eo\n",
      "th\n",
      "sv\n",
      "ta\n",
      "lt\n",
      "sco\n",
      "he\n",
      "ga\n",
      "zh-classical\n",
      "hy\n",
      "da\n",
      "ur\n",
      "ast\n",
      "ca\n",
      "als\n",
      "et\n",
      "nan\n",
      "tl\n",
      "arz\n",
      "ka\n",
      "az\n",
      "sq\n",
      "is\n",
      "la\n",
      "ms\n",
      "got\n",
      "fy\n",
      "stq\n",
      "lb\n",
      "sa\n",
      "rmy\n",
      "sd\n",
      "bar\n",
      "af\n",
      "nds\n",
      "vo\n",
      "ang\n",
      "yi\n",
      "pcd\n",
      "cy\n",
      "kw\n",
      "lv\n",
      "ltg\n",
      "pap\n",
      "so\n",
      "vec\n",
      "qu\n",
      "nap\n",
      "si\n",
      "my\n",
      "nds-nl\n",
      "zh-yue\n",
      "nostalgia\n",
      "ie\n",
      "tk\n",
      "bn\n",
      "nah\n",
      "lmo\n",
      "nn\n",
      "ku\n",
      "mr\n",
      "ba\n",
      "kk\n",
      "zh-min-nan\n",
      "gl\n",
      "avk\n",
      "vep\n",
      "ary\n",
      "ia\n",
      "hif\n",
      "ps\n",
      "skr\n",
      "pnb\n",
      "ks\n",
      "bpy\n",
      "pa\n",
      "or\n",
      "te\n",
      "kn\n",
      "gu\n",
      "jam\n",
      "ceb\n",
      "mn\n",
      "shi\n",
      "kab\n",
      "azb\n",
      "uz\n",
      "ky\n",
      "mni\n",
      "ne\n",
      "wuu\n",
      "nv\n",
      "pam\n",
      "cbk-zam\n",
      "bh\n",
      "tg\n",
      "am\n",
      "be-tarask\n",
      "ty\n",
      "vls\n",
      "co\n",
      "br\n",
      "frp\n",
      "nrm\n",
      "gcr\n",
      "oc\n",
      "mwl\n",
      "wa\n",
      "li\n",
      "pfl\n",
      "gd\n",
      "rm\n",
      "fur\n",
      "pdc\n",
      "tpi\n",
      "ksh\n",
      "fo\n",
      "os\n",
      "an\n",
      "pi\n",
      "se\n",
      "bat-smg\n",
      "gom\n",
      "olo\n",
      "gn\n",
      "mg\n",
      "mt\n",
      "arc\n",
      "test\n",
      "war\n",
      "roa-rup\n",
      "crh\n",
      "ace\n",
      "tyv\n",
      "gv\n",
      "eml\n",
      "scn\n",
      "pnt\n",
      "zea\n",
      "lad\n",
      "tt\n",
      "sc\n",
      "jbo\n",
      "km\n",
      "gan\n",
      "ha\n",
      "frr\n",
      "ckb\n",
      "pih\n",
      "lfn\n",
      "yo\n",
      "bo\n",
      "min\n",
      "nov\n",
      "io\n",
      "ht\n",
      "tn\n",
      "pms\n",
      "kv\n",
      "ab\n",
      "aa\n",
      "bcl\n",
      "thankyou\n",
      "ug\n",
      "sat\n",
      "nqo\n",
      "kl\n",
      "xh\n",
      "jv\n",
      "om\n",
      "zu\n",
      "bi\n",
      "ff\n",
      "haw\n",
      "lij\n",
      "mai\n",
      "ext\n",
      "wo\n",
      "tay\n",
      "trv\n",
      "cdo\n",
      "hak\n",
      "dv\n",
      "ss\n",
      "sm\n",
      "rw\n",
      "cv\n",
      "chy\n",
      "mi\n",
      "cr\n",
      "xmf\n",
      "su\n",
      "bug\n",
      "hyw\n",
      "bjn\n",
      "roa-tara\n",
      "kaa\n",
      "diq\n",
      "atj\n",
      "ban\n",
      "fiu-vro\n",
      "na\n",
      "as\n",
      "ady\n",
      "bxr\n",
      "to\n",
      "chr\n",
      "glk\n",
      "mdf\n",
      "ay\n",
      "ce\n",
      "mrj\n",
      "lbe\n",
      "udm\n",
      "myv\n",
      "mhr\n",
      "krc\n",
      "koi\n",
      "szy\n",
      "gag\n",
      "lez\n",
      "sah\n",
      "za\n",
      "smn\n",
      "gur\n",
      "pcm\n",
      "lo\n",
      "sn\n",
      "mus\n",
      "donate\n",
      "lld\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "article_dates_unique = pd.read_hdf('data/article_dates_unique.h5', 'df')\n",
    "for lang in article_dates_unique['lang'].unique():\n",
    "    print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from unicodedata import category\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Assume 'posts' DataFrame is already loaded, containing a 'body' column.\n",
    "# posts = pd.read_hdf('data/posts.h5').reset_index(drop=True)\n",
    "\n",
    "HEADERS = {'user-agent': 'wikireddit p.gildersleve@exeter.ac.uk'}\n",
    "MAX_CONCURRENCY = 100\n",
    "\n",
    "if os.path.exists('.temp/rd_url_cache_p.pkl'):\n",
    "    with open('.temp/rd_url_cache_p.pkl', 'rb') as f:\n",
    "        RD_URL_CACHE_P = pickle.load(f)\n",
    "else:\n",
    "    RD_URL_CACHE_P = {True: {}, False: {}}\n",
    "    \n",
    "if os.path.exists('.temp/rd_url_cache_tp.pkl'):\n",
    "    with open('.temp/rd_url_cache_tp.pkl', 'rb') as f:\n",
    "        RD_URL_CACHE_TP = pickle.load(f)\n",
    "else:\n",
    "    RD_URL_CACHE_TP = {}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def clean_wikipedia_domain(link):\n",
    "    if 'wikipedia.org' not in link:\n",
    "        return None\n",
    "    try:\n",
    "        parsed = urlparse(link)\n",
    "    except ValueError as ex:\n",
    "        print(ex)\n",
    "        return None\n",
    "    domain = parsed.netloc.lower()\n",
    "    while domain and category(domain[-1])[0]=='P':\n",
    "        domain = domain[:-1]\n",
    "    # get subdomain\n",
    "    # subdomain = domain.split('wikipedia.org')[0]\n",
    "    # if (subdomain not in subdomains) and subdomain:\n",
    "    #     return None\n",
    "    # Domain must be exactly 'wikipedia.org' or end with '.wikipedia.org'\n",
    "    if domain == 'wikipedia.org' or domain.endswith('.wikipedia.org'):\n",
    "        return urlunparse(('https', domain, parsed.path, parsed.params, parsed.query, parsed.fragment))\n",
    "    else:\n",
    "        # print(link, 'not wikipedia')\n",
    "        return None\n",
    "\n",
    "URL_REGEX = re.compile(\n",
    "    r'(https?://[^\\s<>\\[\\]{}|]+(?:wikipedia\\.org)[^\\s<>\\[\\]{}|]*)|((?:[a-z0-9-]+\\.)*wikipedia\\.org[^\\s<>\\[\\]{}|]*)',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_plain_links(text):\n",
    "    plain_links = []\n",
    "    for match in URL_REGEX.finditer(text):\n",
    "        candidate = match.group(1) if match.group(1) else match.group(2)\n",
    "        plain_links.append(candidate.strip())\n",
    "    return plain_links\n",
    "\n",
    "def normalize_links(links):\n",
    "    normalized = []\n",
    "    for link in links:\n",
    "        if not re.match(r'^https?://', link, re.IGNORECASE):\n",
    "            link = 'https://' + link\n",
    "        normalized.append(link)\n",
    "    return normalized\n",
    "\n",
    "def filter_wikipedia_links(links):\n",
    "    wiki_links = set()\n",
    "    for link in links:\n",
    "        cleanlink = clean_wikipedia_domain(link)\n",
    "        if cleanlink:\n",
    "            wiki_links.add(cleanlink)\n",
    "    return wiki_links\n",
    "\n",
    "def extract_links_from_text(text):\n",
    "    # Convert Markdown to HTML and extract markdown-parsed URLs\n",
    "    html = markdown.markdown(text, extensions=['extra'])\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    a_tags = soup.find_all('a', href=True)\n",
    "    markdown_links = [a['href'].strip() for a in a_tags]\n",
    "\n",
    "    # Remove these <a> tags to avoid double counting\n",
    "    for a_tag in a_tags:\n",
    "        a_tag.decompose()\n",
    "    cleaned_text = soup.get_text()\n",
    "\n",
    "    # Extract plain links\n",
    "    plain_links = extract_plain_links(cleaned_text)\n",
    "\n",
    "    # Combine, normalize, and deduplicate\n",
    "    all_links = set(normalize_links(markdown_links + plain_links))\n",
    "\n",
    "    # Filter to Wikipedia links\n",
    "    wiki_links = filter_wikipedia_links(all_links)\n",
    "    return list(wiki_links)\n",
    "\n",
    "def reextract_links(text):\n",
    "    plain_links = extract_plain_links(text)\n",
    "    all_links = set(normalize_links(plain_links))\n",
    "    wiki_links = filter_wikipedia_links(all_links)\n",
    "    return list(wiki_links)\n",
    "\n",
    "# --- Async Validation Functions ---\n",
    "async def async_validate_link(session, url, timeout=5, retries=10, allow_redirects=False):\n",
    "    try:\n",
    "        async with session.head(url, allow_redirects=allow_redirects, timeout=timeout) as r:\n",
    "            if 200 <= r.status < 400:\n",
    "                return True, r.status, str(r.url)\n",
    "            elif r.status == 429 and retries > 0:\n",
    "                await asyncio.sleep(1)  # Wait for 1 second before retrying\n",
    "                return await async_validate_link(session, url, timeout, retries - 1, allow_redirects=allow_redirects)\n",
    "            else:\n",
    "                async with session.get(url, allow_redirects=allow_redirects, timeout=timeout) as r2:\n",
    "                    if 200 <= r2.status < 400:\n",
    "                        return True, r2.status, str(r2.url)\n",
    "                    else:\n",
    "                        return False, r2.status, str(r2.url)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    return False, -1, None\n",
    "\n",
    "async def async_validate_url_with_punctuation(session, url, timeout=5, retries=10, allow_redirects=False):\n",
    "    is_valid, status, processed_url = await async_validate_link(session, url, timeout, retries, allow_redirects)\n",
    "    if is_valid:\n",
    "        return is_valid, status, processed_url\n",
    "\n",
    "    # Try removing trailing punctuation\n",
    "    while url and category(url[-1])[0]=='P':\n",
    "        url = url[:-1]\n",
    "        is_valid, status, processed_url = await async_validate_link(session, url, timeout, retries, allow_redirects)\n",
    "        if is_valid:\n",
    "            return is_valid, status, processed_url\n",
    "\n",
    "    return False, status, processed_url\n",
    "\n",
    "async def async_validate_url_with_textpunctuation(session, url, timeout=5):\n",
    "    is_valid, status, processed_url = await async_validate_link(session, url, timeout)\n",
    "    if is_valid:\n",
    "        return is_valid, status, processed_url\n",
    "\n",
    "    # Remove trailing alphanumeric and punctuation until punctuation is hit\n",
    "    original_processed_url = processed_url\n",
    "    # print('av', url)\n",
    "    url = url.rstrip('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789:\\'\"_')\n",
    "    if url[-5:] != 'wiki/' and url[-5:] != '.org/':\n",
    "        is_valid, status, processed_url = await async_validate_url_with_punctuation(session, url, timeout=5)\n",
    "        if is_valid:\n",
    "            return is_valid, status, processed_url\n",
    "\n",
    "    return False, status, original_processed_url\n",
    "\n",
    "async def validate_urls_main(urls, retries=10, allow_redirects=False, max_concurrency=MAX_CONCURRENCY):\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "    async with aiohttp.ClientSession(headers=HEADERS) as session:\n",
    "        async def validate(url):\n",
    "            if url in RD_URL_CACHE_P[allow_redirects]:\n",
    "                return url, RD_URL_CACHE_P[allow_redirects][url]  # Return cached result\n",
    "            \n",
    "            # ṬODO, add str vaidatopm here for domain\n",
    "\n",
    "            async with semaphore:\n",
    "                result = await async_validate_url_with_punctuation(session, url,\n",
    "                                                                      retries=retries,\n",
    "                                                                      allow_redirects=allow_redirects)\n",
    "                if result[1] != 429:\n",
    "                    RD_URL_CACHE_P[allow_redirects][url] = result  # Cache result\n",
    "                return url, result\n",
    "        tasks = [validate(u) for u in urls]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "\n",
    "async def validate_urls_main2(urls, max_concurrency=MAX_CONCURRENCY):\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "    async with aiohttp.ClientSession(headers=HEADERS) as session:\n",
    "        async def validate(url):\n",
    "            if url in RD_URL_CACHE_TP:\n",
    "                return url, RD_URL_CACHE_TP[url]  # Return cached result\n",
    "            async with semaphore:\n",
    "                result = await async_validate_url_with_textpunctuation(session, url)\n",
    "                if result[1] != 429:\n",
    "                    RD_URL_CACHE_TP[url] = result  # Cache result\n",
    "                return url, result\n",
    "        tasks = [validate(u) for u in urls]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "\n",
    "# --- Processing ---\n",
    "\n",
    "async def get_links_df(posts_df, column='body'):\n",
    "\n",
    "    # print(len(posts_df))\n",
    "    posts_df['extracted_links'] = posts_df[column].dropna().apply(extract_links_from_text)\n",
    "    # print(posts_df['extracted_links'].iloc[0])\n",
    "    # print(posts_df['extracted_links'].iloc[-1])\n",
    "    links_df = posts_df.explode('extracted_links').rename(columns={'extracted_links':'extracted_url'}).reset_index(drop=True)\n",
    "    links_df = links_df[~links_df['extracted_url'].isna()].drop_duplicates().copy()\n",
    "    # print(len(links_df))\n",
    "    # print('egl', links_df)\n",
    "\n",
    "    return await process_links(links_df)\n",
    "\n",
    "async def process_links(links_df, retry_count=0, max_retries=5, backoff_factor=3):\n",
    "    chop_index = list(links_df.columns).index('extracted_url') + 1\n",
    "    # 1) Initial validation\n",
    "    urls = links_df['extracted_url'].unique()\n",
    "    # print(urls)\n",
    "    results = await validate_urls_main(urls)\n",
    "    # print(len(results))\n",
    "    df_results = pd.DataFrame([[x[0], *x[1]] for x in results],\n",
    "                              columns=['extracted_url', 'valid_1', 'status_1', 'processed_url_1']).drop_duplicates('extracted_url')\n",
    "    # print(len(df_results))\n",
    "    # links_df.to_hdf('data/links_df_test.h5', key='df', mode='w')\n",
    "    # df_results.to_hdf('data/df_results_test.h5', key='df', mode='w')\n",
    "    links_df = links_df.merge(df_results, on='extracted_url', how='left').drop_duplicates()\n",
    "    # links_df.to_hdf('data/links_df_test1.h5', key='df', mode='w')\n",
    "    # print(len(links_df))\n",
    "    # 2) Reextract for invalid URLs\n",
    "    invalid_urls = links_df.loc[links_df['valid_1']==False, 'extracted_url'].unique()\n",
    "\n",
    "    if len(invalid_urls) > 0:\n",
    "        # Create a mapping DataFrame of extracted_url -> reextracted_url\n",
    "        invalid_urls_series = pd.Series(invalid_urls, name='extracted_url')\n",
    "        reextracted_list = []\n",
    "        for u in invalid_urls_series:\n",
    "            reex_urls = reextract_links(u)\n",
    "            for rurl in reex_urls:\n",
    "                reextracted_list.append((u, rurl))\n",
    "\n",
    "        if reextracted_list:\n",
    "            reextracted_df = pd.DataFrame(reextracted_list, columns=['extracted_url', 'reextracted_url']).drop_duplicates('extracted_url')\n",
    "            # reextracted_df.to_hdf('data/reextracted_df_test.h5', key='df', mode='w')\n",
    "            # print('re', reextracted_df)\n",
    "            # Validate reextracted URLs\n",
    "            re_urls = reextracted_df['reextracted_url'].unique()\n",
    "            # print('ru', re_urls)\n",
    "            results2 = await validate_urls_main(re_urls)\n",
    "            df_results2 = pd.DataFrame([[x[0], *x[1]] for x in results2],\n",
    "                                       columns=['reextracted_url', 'valid_2', 'status_2', 'processed_url_2']).drop_duplicates('reextracted_url')\n",
    "            # print('r2', df_results2)\n",
    "            # Merge second attempt results into reextracted_df\n",
    "            # df_results2.to_hdf('data/df_results2_test.h5', key='df', mode='w')\n",
    "            reextracted_df = reextracted_df.merge(df_results2, on='reextracted_url', how='left').drop_duplicates()\n",
    "            # print('re2', reextracted_df)\n",
    "            # Now merge reextracted results back to links_df using 'extracted_url'\n",
    "            # prin t('l0', links_df)\n",
    "            # reextracted_df.to_hdf('data/reextracted_df_test2.h5', key='df', mode='w')\n",
    "            links_df = links_df.merge(reextracted_df, on='extracted_url', how='left', suffixes=('', '_second')).drop_duplicates()\n",
    "            # print('lr', links_df)\n",
    "            # Check still invalid after reextraction\n",
    "            # Consider invalid if original attempt and reattempt still fail\n",
    "            # links_df.to_hdf('data/links_df_test2.h5', key='df', mode='w')\n",
    "            # print('ldf', len(links_df))\n",
    "            # print('re', len(re_urls))\n",
    "            # print('r2', len(results2))\n",
    "            # print('rdf', len(reextracted_df))\n",
    "            still_invalid_df = links_df.loc[\n",
    "                (links_df['valid_1']==False) &\n",
    "                ((links_df['valid_2'].isna()) | (links_df['valid_2']==False))\n",
    "            ][['reextracted_url']].drop_duplicates().copy()\n",
    "            # print('si', still_invalid_df)\n",
    "            if len(still_invalid_df) > 0:\n",
    "                # Validate still invalid URLs with textpunctuation\n",
    "                # We need to look up the reextracted URLs again for these still invalid URLs\n",
    "                # or possibly re-run the logic if reextraction is required again.\n",
    "                # For now, let's assume we can directly use 'reextracted_df' to get them.\n",
    "\n",
    "                # Filter reextracted_df for those matching still_invalid original URLs\n",
    "                re_urls_third = still_invalid_df['reextracted_url'].unique()\n",
    "                # print('r3', re_urls_third)\n",
    "                results3 = await validate_urls_main2(re_urls_third)\n",
    "                df_results3 = pd.DataFrame([[x[0], *x[1]] for x in results3],\n",
    "                                           columns=['reextracted_url', 'valid_3', 'status_3', 'processed_url_3']).drop_duplicates('reextracted_url')\n",
    "\n",
    "                # Merge third attempt results\n",
    "                still_invalid_df = still_invalid_df.merge(df_results3, on='reextracted_url', how='left').drop_duplicates()\n",
    "                \n",
    "                # Merge back into links_df by original_url\n",
    "                links_df = links_df.merge(still_invalid_df, on='reextracted_url', how='left', suffixes=('', '_third')).drop_duplicates()\n",
    "\n",
    "    # Now links_df contains:\n",
    "    # - original extracted_url from the posts\n",
    "    # - initial validation results (valid, status, processed_url)\n",
    "    # - second attempt validation results (valid_re, status_re, processed_url_re) via reextraction\n",
    "    # - third attempt validation results (valid_third, status_third, processed_url_third)\n",
    "\n",
    "    # You can now decide on final validity and merge results back into posts if needed.\n",
    "\n",
    "    # add cols to links_df, if not present\n",
    "    cols = ['extracted_url', 'valid_1', 'status_1',\n",
    "       'processed_url_1', 'reextracted_url', 'valid_2', 'status_2', 'processed_url_2',\n",
    "       'valid_3', 'status_3', 'processed_url_3']\n",
    "    for col in cols:\n",
    "        if col not in links_df.columns:\n",
    "            links_df[col] = None\n",
    "\n",
    "    # get final validity, prefer the last valid url\n",
    "    if len(links_df) > 0:\n",
    "        links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
    "            links_df['valid_2'].fillna(\n",
    "                links_df['valid_1']\n",
    "            )\n",
    "        )\n",
    "\n",
    "        links_df['end_processed_status'] = links_df['status_3'].fillna(\n",
    "            links_df['status_2'].fillna(\n",
    "                links_df['status_1']\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        links_df['end_processed_url'] = links_df['processed_url_3'].fillna(\n",
    "            links_df['processed_url_2'].fillna(\n",
    "                links_df['processed_url_1']\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # print('l', links_df)\n",
    "        # get redirects for 3xx errors\n",
    "        error3xxs = links_df[(links_df['status_1']>=300)&(links_df['status_1']<400)|\n",
    "                             (links_df['status_2']>=300)&(links_df['status_2']<400)|\n",
    "                             (links_df['status_3']>=300)&(links_df['status_3']<400)\n",
    "                             ][['end_processed_url']].drop_duplicates().copy()\n",
    "\n",
    "        rd_results = await validate_urls_main(error3xxs['end_processed_url'], retries=100000000, allow_redirects=True)\n",
    "        rd_df = pd.DataFrame([[x[0], *x[1]] for x in rd_results],\n",
    "                             columns=['end_processed_url', 'valid_rd', 'status_rd', 'redirected_url']\n",
    "                             ).drop_duplicates('end_processed_url')\n",
    "        # print('rd', rd_df)\n",
    "                             \n",
    "        links_df = links_df.merge(rd_df, on='end_processed_url', how='left').drop_duplicates()\n",
    "\n",
    "        #  rerun for 429 errors\n",
    "        error429s = links_df[(links_df['status_1']==429)|\n",
    "                                (links_df['status_2']==429)|\n",
    "                                (links_df['status_3']==429)|\n",
    "                                (links_df['status_rd']==429)\n",
    "                                ][links_df.columns[:chop_index]].drop_duplicates().copy()\n",
    "        links_df = links_df[(links_df['status_1']!=429)&\n",
    "                            (links_df['status_2']!=429)&\n",
    "                            (links_df['status_3']!=429)&\n",
    "                            (links_df['status_rd']!=429)\n",
    "                            ].drop_duplicates()\n",
    "\n",
    "        if len(error429s) > 0:\n",
    "            if retry_count < max_retries:\n",
    "                delay = backoff_factor ** retry_count\n",
    "                print(f\"429 errors: {len(error429s)}. Retrying after {delay} seconds...\")\n",
    "                await asyncio.sleep(delay)\n",
    "                # Retry recursively with increased retry_count\n",
    "                new_links = await process_links(error429s.copy(), retry_count=retry_count + 1, max_retries=max_retries, backoff_factor=backoff_factor)\n",
    "                links_df = pd.concat([links_df, new_links]).drop_duplicates()\n",
    "            else:\n",
    "                print(f\"Max retries reached for {len(error429s)} URLs. Skipping...\")\n",
    "                error429s_extra = links_df[(links_df['status_1']==429)|\n",
    "                                (links_df['status_2']==429)|\n",
    "                                (links_df['status_3']==429)|\n",
    "                                (links_df['status_rd']==429)\n",
    "                                ][links_df.columns[:chop_index]].drop_duplicates().copy()\n",
    "                error429s = pd.concat([error429s, error429s_extra])\n",
    "                links_df = links_df[(links_df['status_1']!=429)&\n",
    "                            (links_df['status_2']!=429)&\n",
    "                            (links_df['status_3']!=429)&\n",
    "                            (links_df['status_rd']!=429)\n",
    "                            ].drop_duplicates()\n",
    "                # print(f\"429 errors: {len(l1429)} -> {len(error429s)}\")\n",
    "                # save 429s\n",
    "                if len(error429s) > 0:\n",
    "                    print(\"Saving 429s\")\n",
    "                    error429s.to_hdf('data/429s.h5', key='df', mode='a')\n",
    "\n",
    "\n",
    "        links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n",
    "        links_df['final_status'] = links_df['status_rd'].fillna(links_df['end_processed_status'])\n",
    "        links_df['final_url'] = links_df['redirected_url'].fillna(links_df['end_processed_url'])\n",
    "\n",
    "        print(\"Saving cache. Total size: \", len(RD_URL_CACHE_P[True]) + len(RD_URL_CACHE_P[False])\n",
    "                + len(RD_URL_CACHE_TP))\n",
    "        with open('.temp/rd_url_cache_p.pkl', 'wb') as f:\n",
    "            pickle.dump(RD_URL_CACHE_P, f)\n",
    "        with open('.temp/rd_url_cache_tp.pkl', 'wb') as f:\n",
    "            pickle.dump(RD_URL_CACHE_TP, f)\n",
    "\n",
    "        return links_df.drop_duplicates().reset_index(drop=True)\n",
    "    else:\n",
    "        return links_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  0\n"
     ]
    }
   ],
   "source": [
    "# RD_URL_CACHE_P = {True: {}, False: {}}\n",
    "# RD_URL_CACHE_TP = {}\n",
    "\n",
    "# print(\"Saving cache. Total size: \", len(RD_URL_CACHE_P[True]) + len(RD_URL_CACHE_P[False])\n",
    "#         + len(RD_URL_CACHE_TP))\n",
    "# with open('.temp/rd_url_cache_p.pkl', 'wb') as f:\n",
    "#     pickle.dump(RD_URL_CACHE_P, f)\n",
    "# with open('.temp/rd_url_cache_tp.pkl', 'wb') as f:\n",
    "#     pickle.dump(RD_URL_CACHE_TP, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_805388/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_805388/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 errors: 1. Retrying after 1 seconds...\n",
      "429 errors: 1. Retrying after 3 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_805388/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n",
      "/tmp/ipykernel_805388/2514332107.py:381: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_status'] = links_df['status_rd'].fillna(links_df['end_processed_status'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  668243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_805388/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n",
      "/tmp/ipykernel_805388/2514332107.py:381: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_status'] = links_df['status_rd'].fillna(links_df['end_processed_status'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  668243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_805388/2514332107.py:359: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  links_df = pd.concat([links_df, new_links]).drop_duplicates()\n",
      "/tmp/ipykernel_805388/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  668243\n"
     ]
    }
   ],
   "source": [
    "# For example:\n",
    "posts = pd.read_hdf('data/posts.h5').reset_index(drop=True)\n",
    "ptest = posts.iloc[:1000].copy()  # limit for testing\n",
    "\n",
    "# Run the async function\n",
    "links_df = await get_links_df(ptest, column='body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_1 valid_2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>url</th>\n",
       "      <th>author_id</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>...</th>\n",
       "      <th>processed_url_3</th>\n",
       "      <th>end_processed_valid</th>\n",
       "      <th>end_processed_status</th>\n",
       "      <th>end_processed_url</th>\n",
       "      <th>valid_rd</th>\n",
       "      <th>status_rd</th>\n",
       "      <th>redirected_url</th>\n",
       "      <th>final_valid</th>\n",
       "      <th>final_status</th>\n",
       "      <th>final_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, subreddit_id, title, body, url, author_id, nsfw, score, upvote_ratio, distinguished, self, video, locked, spoiler, sticky, flair_text, language_code, gildings, thumbnail, crosspost_parent_id, permalink, num_comments, created_at, updated_at, extracted_url, valid_1, status_1, processed_url_1, reextracted_url, valid_2, status_2, processed_url_2, valid_3, status_3, processed_url_3, end_processed_valid, end_processed_status, end_processed_url, valid_rd, status_rd, redirected_url, final_valid, final_status, final_url]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_1 valid_3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>url</th>\n",
       "      <th>author_id</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>...</th>\n",
       "      <th>processed_url_3</th>\n",
       "      <th>end_processed_valid</th>\n",
       "      <th>end_processed_status</th>\n",
       "      <th>end_processed_url</th>\n",
       "      <th>valid_rd</th>\n",
       "      <th>status_rd</th>\n",
       "      <th>redirected_url</th>\n",
       "      <th>final_valid</th>\n",
       "      <th>final_status</th>\n",
       "      <th>final_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, subreddit_id, title, body, url, author_id, nsfw, score, upvote_ratio, distinguished, self, video, locked, spoiler, sticky, flair_text, language_code, gildings, thumbnail, crosspost_parent_id, permalink, num_comments, created_at, updated_at, extracted_url, valid_1, status_1, processed_url_1, reextracted_url, valid_2, status_2, processed_url_2, valid_3, status_3, processed_url_3, end_processed_valid, end_processed_status, end_processed_url, valid_rd, status_rd, redirected_url, final_valid, final_status, final_url]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_2 valid_3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>url</th>\n",
       "      <th>author_id</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>...</th>\n",
       "      <th>processed_url_3</th>\n",
       "      <th>end_processed_valid</th>\n",
       "      <th>end_processed_status</th>\n",
       "      <th>end_processed_url</th>\n",
       "      <th>valid_rd</th>\n",
       "      <th>status_rd</th>\n",
       "      <th>redirected_url</th>\n",
       "      <th>final_valid</th>\n",
       "      <th>final_status</th>\n",
       "      <th>final_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, subreddit_id, title, body, url, author_id, nsfw, score, upvote_ratio, distinguished, self, video, locked, spoiler, sticky, flair_text, language_code, gildings, thumbnail, crosspost_parent_id, permalink, num_comments, created_at, updated_at, extracted_url, valid_1, status_1, processed_url_1, reextracted_url, valid_2, status_2, processed_url_2, valid_3, status_3, processed_url_3, end_processed_valid, end_processed_status, end_processed_url, valid_rd, status_rd, redirected_url, final_valid, final_status, final_url]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for n, x in enumerate(['valid_1', 'valid_2', 'valid_3']):\n",
    "    for m, y in enumerate(['valid_1', 'valid_2', 'valid_3']):\n",
    "        if m > n:\n",
    "            print(x, y)\n",
    "            display(links_df[links_df[x] & links_df[y]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_hdf('data/posts.h5').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 33589\n",
      "File data/titlelinks_batches.h5 does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100797\n",
      "Saving...\n",
      "Batch 2 33589\n",
      "'No object named /batch_2 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2031526577.py:21: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100797\n",
      "Saving...\n",
      "Batch 3 33589\n",
      "'No object named /batch_3 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2031526577.py:21: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100797\n",
      "Saving...\n",
      "Batch 4 33589\n",
      "'No object named /batch_4 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2031526577.py:21: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100797\n",
      "Saving...\n",
      "Batch 5 33589\n",
      "'No object named /batch_5 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2031526577.py:21: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100797\n",
      "Saving...\n",
      "Batch 6 33589\n",
      "'No object named /batch_6 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2031526577.py:21: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100797\n",
      "Saving...\n",
      "Batch 7 33589\n",
      "'No object named /batch_7 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2031526577.py:21: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100797\n",
      "Saving...\n",
      "Batch 8 33589\n",
      "'No object named /batch_8 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2031526577.py:21: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100797\n",
      "Saving...\n",
      "Batch 9 33589\n",
      "'No object named /batch_9 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2031526577.py:21: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100797\n",
      "Saving...\n",
      "Batch 10 33589\n",
      "'No object named /batch_10 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2031526577.py:21: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100797\n",
      "Saving...\n",
      "Batch 11 33589\n",
      "'No object named /batch_11 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2031526577.py:21: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2031526577.py:27: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  titlelinks.to_hdf('data/titlelinks.h5', key='df', mode='w')\n"
     ]
    }
   ],
   "source": [
    "#  run on titles\n",
    "\n",
    "outcols = ['id', 'extracted_url', 'valid_1', 'status_1',\n",
    "       'processed_url_1', 'reextracted_url', 'valid_2', 'status_2',\n",
    "       'processed_url_2', 'valid_3', 'status_3', 'processed_url_3',\n",
    "       'end_processed_valid', 'end_processed_url',\n",
    "       'valid_rd', 'status_rd', 'redirected_url', 'final_valid',\n",
    "       'final_status', 'final_url']\n",
    "\n",
    "size = len(posts)//10\n",
    "titlelinks = []\n",
    "for batch in range(0, len(posts), size):\n",
    "    print(f\"Batch {batch//size + 1}\", size)\n",
    "    try:\n",
    "        titlelinks.append(pd.read_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}'))\n",
    "    except (FileNotFoundError, KeyError) as ex:\n",
    "        print(ex)\n",
    "        links_df = await get_links_df(posts.iloc[batch:batch+size].copy(), column='title')\n",
    "        if len(links_df) > 0:\n",
    "            print('Saving...')\n",
    "            links_df[outcols].to_hdf(f'data/titlelinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
    "            titlelinks.append(links_df[outcols])\n",
    "\n",
    "titlelinks = pd.concat(titlelinks).reset_index(drop=True)\n",
    "if os.path.exists('data/titlelinks.h5'):\n",
    "    os.remove('data/titlelinks.h5')\n",
    "titlelinks.to_hdf('data/titlelinks.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem IDs t3_udac4z, t3_ud2feb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_hdf('data/posts.h5').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 33589\n",
      "File data/bodylinks_batches.h5 does not exist\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "'Google have some books archived for free' does not appear to be an IPv4 or IPv6 address\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "'Cisco' does not appear to be an IPv4 or IPv6 address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100801\n",
      "saving\n",
      "Batch 2 33589\n",
      "'No object named /batch_2 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Horse Sacrifice' does not appear to be an IPv4 or IPv6 address\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100809\n",
      "saving\n",
      "Batch 3 33589\n",
      "'No object named /batch_3 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100816\n",
      "saving\n",
      "Batch 4 33589\n",
      "'No object named /batch_4 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid IPv6 URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100823\n",
      "saving\n",
      "Batch 5 33589\n",
      "'No object named /batch_5 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid IPv6 URL\n",
      "'Bulgarian Bus Crash' does not appear to be an IPv4 or IPv6 address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100830\n",
      "saving\n",
      "Batch 6 33589\n",
      "'No object named /batch_6 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100834\n",
      "saving\n",
      "Batch 7 33589\n",
      "'No object named /batch_7 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100837\n",
      "saving\n",
      "Batch 8 33589\n",
      "'No object named /batch_8 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100843\n",
      "saving\n",
      "Batch 9 33589\n",
      "'No object named /batch_9 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100846\n",
      "saving\n",
      "Batch 10 33589\n",
      "'No object named /batch_10 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  3100850\n",
      "saving\n",
      "Batch 11 33589\n",
      "'No object named /batch_11 in the file'\n",
      "Saving cache. Total size:  3100850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_1226366/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n",
      "/tmp/ipykernel_1226366/2514332107.py:381: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_status'] = links_df['status_rd'].fillna(links_df['end_processed_status'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226366/2336260079.py:20: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'status_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
      "/tmp/ipykernel_1226366/2336260079.py:23: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  bodylinks = pd.concat(bodylinks).reset_index(drop=True)\n",
      "/tmp/ipykernel_1226366/2336260079.py:26: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block3_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  bodylinks.to_hdf('data/bodylinks.h5', key='df', mode='w')\n"
     ]
    }
   ],
   "source": [
    "# run on bodies\n",
    "outcols = ['id', 'extracted_url', 'valid_1', 'status_1',\n",
    "       'processed_url_1', 'reextracted_url', 'valid_2', 'status_2',\n",
    "       'processed_url_2', 'valid_3', 'status_3', 'processed_url_3',\n",
    "       'end_processed_valid', 'end_processed_url',\n",
    "       'valid_rd', 'status_rd', 'redirected_url', 'final_valid',\n",
    "       'final_status', 'final_url']\n",
    "size = len(posts)//10\n",
    "\n",
    "bodylinks = []\n",
    "for batch in range(0, len(posts), size):\n",
    "   print(f\"Batch {batch//size + 1}\", size)\n",
    "   try:\n",
    "      bodylinks.append(pd.read_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}'))\n",
    "   except (FileNotFoundError, KeyError) as ex:\n",
    "      print(ex)\n",
    "      links_df = await get_links_df(posts.iloc[batch:batch+size].copy(), column='body')\n",
    "      if len(links_df) > 0:\n",
    "         print('saving')\n",
    "         links_df[outcols].to_hdf(f'data/bodylinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
    "         bodylinks.append(links_df[outcols])\n",
    "\n",
    "bodylinks = pd.concat(bodylinks).reset_index(drop=True)\n",
    "if os.path.exists('data/bodylinks.h5'):\n",
    "   os.remove('data/bodylinks.h5')\n",
    "bodylinks.to_hdf('data/bodylinks.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.concat([pd.read_hdf(f'data/comments_{x}.h5') for x in range(1,5)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 102643\n",
      "Batch 2 102643\n",
      "Batch 3 102643\n",
      "Batch 4 102643\n",
      "Batch 5 102643\n",
      "Batch 6 102643\n",
      "Batch 7 102643\n",
      "Batch 8 102643\n",
      "Batch 9 102643\n",
      "Batch 10 102643\n",
      "Batch 11 102643\n",
      "Batch 12 102643\n",
      "Batch 13 102643\n",
      "Batch 14 102643\n",
      "Batch 15 102643\n",
      "Batch 16 102643\n",
      "Batch 17 102643\n",
      "Batch 18 102643\n",
      "Batch 19 102643\n",
      "Batch 20 102643\n",
      "Batch 21 102643\n",
      "Batch 22 102643\n",
      "Batch 23 102643\n",
      "Batch 24 102643\n",
      "Batch 25 102643\n",
      "'No object named /batch_25 in the file'\n",
      "Invalid IPv6 URL\n",
      "\n",
      "label empty or too long\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "\n",
      "\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "label empty or too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 errors: 2665. Retrying after 1 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 errors: 184. Retrying after 3 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1416190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1416190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1416190\n",
      "saving\n",
      "Batch 26 102643\n",
      "'No object named /batch_26 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/3146937327.py:27: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/commentlinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Cannot connect to host ww.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "\n",
      "\n",
      "\n",
      "Cannot connect to host ww.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "label empty or too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 errors: 2908. Retrying after 1 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 errors: 104. Retrying after 3 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1451645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1451645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1451645\n",
      "saving\n",
      "Batch 27 102643\n",
      "'No object named /batch_27 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/3146937327.py:27: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/commentlinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Paddy Cap' does not appear to be an IPv4 or IPv6 address\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host em.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "\n",
      "label empty or too long\n",
      "\n",
      "Cannot connect to host imageen.m.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "\n",
      "Cannot connect to host em.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host em.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host em.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "Cannot connect to host imageen.m.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host imageen.m.wikipedia.:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host imageen.m.wikipedia:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "429 errors: 2688. Retrying after 1 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "429 errors: 111. Retrying after 3 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 errors: 32. Retrying after 9 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:314: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['status_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:313: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_status'] = links_df['status_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1483090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:359: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  links_df = pd.concat([links_df, new_links]).drop_duplicates()\n",
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1483090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1483090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1483090\n",
      "saving\n",
      "Batch 28 102643\n",
      "'No object named /batch_28 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/3146937327.py:27: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/commentlinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cannot connect to host e.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host diafr.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "label empty or too long\n",
      "Cannot connect to host n.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "\n",
      "\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host n.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host e.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host e.wikipedia.:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host e.wikipedia:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host diafr.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host diafr.wikipedia.:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host diafr.wikipedia:443 ssl:default [Name or service not known]\n",
      "label empty or too long\n",
      "Cannot connect to host n.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host en.www.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host n.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host n.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "Cannot connect to host n.wikipedia.org:443 ssl:default [Name or service not known]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "429 errors: 2512. Retrying after 1 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "429 errors: 101. Retrying after 3 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 errors: 23. Retrying after 9 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1516321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1516321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1516321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:380: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['final_valid'] = links_df['valid_rd'].fillna(links_df['end_processed_valid'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cache. Total size:  1516321\n",
      "saving\n",
      "Batch 29 102643\n",
      "'No object named /batch_29 in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/3146937327.py:27: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['id', 'extracted_url', 'processed_url_1', 'reextracted_url', 'valid_2',\n",
      "       'processed_url_2', 'valid_3', 'processed_url_3', 'end_processed_url',\n",
      "       'valid_rd', 'redirected_url', 'final_url'],\n",
      "      dtype='object')]\n",
      "\n",
      "  links_df[outcols].to_hdf(f'data/commentlinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "Invalid IPv6 URL\n",
      "'this guy' does not appear to be an IPv4 or IPv6 address\n",
      "\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "label empty or too long\n",
      "Cannot connect to host www.en.wikipedia.org:443 ssl:default [Name or service not known]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_996941/2514332107.py:308: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['valid_2'].fillna(\n",
      "/tmp/ipykernel_996941/2514332107.py:307: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  links_df['end_processed_valid'] = links_df['valid_3'].fillna(\n"
     ]
    }
   ],
   "source": [
    "# run on comments\n",
    "outcols = ['id', 'extracted_url', 'valid_1', 'status_1',\n",
    "         'processed_url_1', 'reextracted_url', 'valid_2', 'status_2',\n",
    "         'processed_url_2', 'valid_3', 'status_3', 'processed_url_3',\n",
    "         'end_processed_valid', 'end_processed_url',\n",
    "         'valid_rd', 'status_rd', 'redirected_url', 'final_valid',\n",
    "         'final_status', 'final_url']\n",
    "size = len(comments)//100\n",
    "\n",
    "commentlinks = []\n",
    "for batch in range(0, len(comments), size):\n",
    "    print(f\"Batch {batch//size + 1}\", size)\n",
    "    try:\n",
    "        commentlinks.append(pd.read_hdf(f'data/commentlinks_batches.h5', key=f'/batch_{batch//size + 1}'))\n",
    "    except (FileNotFoundError, KeyError) as ex:\n",
    "        print(ex)\n",
    "        while True:\n",
    "            try:\n",
    "                links_df = await get_links_df(comments.iloc[batch:batch+size].copy(), column='body')\n",
    "                MAX_CONCURRENCY = min(200, int(MAX_CONCURRENCY*(2**0.5)))\n",
    "                break\n",
    "            except MemoryError as ex:\n",
    "                print(ex)\n",
    "                MAX_CONCURRENCY = MAX_CONCURRENCY//2\n",
    "        if len(links_df) > 0:\n",
    "            print('saving')\n",
    "            links_df[outcols].to_hdf(f'data/commentlinks_batches.h5', key=f'/batch_{batch//size + 1}', mode='a')\n",
    "            commentlinks.append(links_df[outcols])\n",
    "\n",
    "commentlinks = pd.concat(commentlinks).reset_index(drop=True)\n",
    "if os.path.exists('data/commentlinks.h5'):\n",
    "    os.remove('data/commentlinks.h5')\n",
    "commentlinks.to_hdf('data/commentlinks.h5', key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentlinks = pd.concat([pd.read_hdf(f'data/commentlinks_batches.h5', key=f'/batch_{x}') for x in range(1,5)]).reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
